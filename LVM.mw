''LVM'' (Logical Volume Manager) is a software which uses physical devices abstract as PVs (Physical Volumes) in storage pools called VG (Volume Group). Whereas physical volumes could be a partition, whole SATA hard drives grouped as JBOD ('''J'''ust a '''B'''unch '''O'''f '''D'''isks), RAID systems, iSCSI, Fibre Channel, eSATA etc.

== Installation ==
First we need to enable the ''device mapper'' in our kernel:
{{Kernel||<pre>
Device Drivers  --->
   Multiple devices driver support (RAID and LVM)  --->
       <*> Device mapper support
           <M> Crypt target support
           <M> Snapshot target
           <M> Mirror target
       <M> Multipath target
           <M> I/O Path Selector based on the number of in-flight I/Os
           <M> I/O Path Selector based on the service time</pre>}}
{{Note|You probably don't need everything enabled, but some of the options are needed for [[#LVM2_Snapshots]], [[#LVM2_MIRROR]], [[#LVM2_Stripeset]] and encryption.}}
{{Package|sys-fs/lvm2}} is available on Gentoo Linux and supports the following useflags:
{{USEflag
|package=sys-fs/lvm2
|clvm
|cman
|lvm1+yes++
|readline
|selinux++no+
|static+yes++
|static-libs+yes++
}}
You can install LVM2 using the following command:
{{Emerge|lvm2}}
The package comes with a start-stop-daemon, which we start and add to the boot runlevel:
{{RootCmd|/etc/init.d/lvm start && rc-update add lvm boot}}
The configuration files are located in
{{Code|Configuration Files|<pre>
/etc/lvm/lvm.conf</pre>}}

== Usage ==
LVM organizes storage in three different levels as follows:
* hard drives, partitions, RAID systems or other means of storage are initialized as PV (Physical Volume)
* Physical Volumes (PV) are grouped together in Volume Groups (VG)
* Logical Volumes (LV) are managed in Volume Groups (VG)

=== PV (Physical Volume) ===
Physical Volumes are the actual hardware or storage system LVM builds up upon.

==== Partitioning ====
The partition type for ''LVM'' is ''8e'' (Linux LVM):
{{RootCmd|fdisk /dev/sdX}}
In fdisk, you can create MBR partitions using the '''n''' key and then change the partition type with the '''t''' key to ''8e''. We will end up with one primary partition /dev/sdX1 of partition type ''8e'' (Linux LVM).
{{Note|this step is not needed, since LVM can initialize whole hard drives as PV. Actual not using partitioning avoids LVM to be restricted to certain limits of MBR or GPT tables}}

==== Create PV ====
The following command creates a Physical Volume (PV) on the two first primary partitions of /dev/sdX and /dev/sdY:
{{RootCmd|pvcreate /dev/sd[X-Y]1}}

==== List PV ====
The folloing command lists all active Physical Volumes (PV) in the system:
{{RootCmd|pvdisplay}}
You can scan for PV in the system, to troubleshoot not properly initialized or lost storage devices:
{{RootCmd|pvscan}}

==== Remove PV ====
LVM automatically distributed the data onto all available PV, if not told otherwise. To make sure there is no data left on our device before we remove it, use the following command:
{{RootCmd|pvmove -v /dev/sdX1}}
This might take a long time and once finished, there should be no data left on /dev/sdX1. We first remove the PV from our Volume Group (VG) and then the actual PV:
{{RootCmd|vgreduce vg0 /dev/sdX1 && pvremove /dev/sdX1}}
{{Note|If a whole hard drives was once initialized as PV, then you have to remove it before it can be properly partitioned again. That is because PV have no valid MBR table}}

=== VG (Volume Group) ===
Volume Groups (VG) consist of one or more Physical Volumes (PV) and show up as /dev/<VG name>/ in the device file system.

==== Create VG ====
The following command creates a Volume Group (VG) named ''vg0'' on two previously initialized Physical Volumes (PV) named /dev/sdX1 and /dev/sdY1:
{{RootCmd|vgcreate vg0 /dev/sd[X-Y]1}}

==== List VG ====
The folloing command lists all active Volume Groups (VG) in the system:
{{RootCmd|vgdisplay}}
You can scan for VG in the system, to troubleshoot not properly created or lost VGs:
{{RootCmd|vgscan}}

==== Extend VG ====
With the following command, we extend the exisiting Volume Group (VG) vg0 onto the Physical Volume (PV) /dev/sdZ1:
{{RootCmd|vgextend vg0 /dev/sdZ1}}

==== Reduce VG ====
Before we can remove a Physical Volume (PV), we need to make sure that LVM has no data left on the device. To move all data off that PV and distribute it onto the other available, use the following command:
{{RootCmd|pvmove -v /dev/sdX1}}
This might take a while and once finished, we can remove the PV from our VG:
{{RootCmd|vgreduce vg0 /dev/sdX1}}

==== Remove VG ====
Before we can remove a Volume Group (VG), we have to remove all existing Snapshots, all Logical Volumes (LV) and all Physical Volumes (PV) but one. The following command removes the VG named vg0:
{{RootCmd|vgremove vg0}}

=== LV (Logical Volume) ===
Logical Volumes (LV) are created and managed in Volume Groups (VG), once created they show up as /dev/<VG name>/<LV name> and can be used like normal partitions.

==== Create LV ====
With the following command, we create a Logical Volume (LV) named lvol1 in Volume Group (VG) vg0 with a size of 150MB:
{{RootCmd|lvcreate -L 150M -n lvol1 vg0}}
There are other useful options to set the size of a new LV like:
* -l 100%FREE = maximum size of the LV within the VG
* -l 50%VG = 50% size of the whole VG

==== List LV ====
The folloing command lists all Logical Volumes (LV) in the system:
{{RootCmd|lvdisplay}}
You can scan for LV in the system, to troubleshoot not properly created or lost LVs:
{{RootCmd|lvscan}}

==== Extend LV ====
With the following command, we can extend the Logical Volume (LV) named lvol1 in Volume Group (VG) vg0 to 500MB:
{{RootCmd|lvextend -L500M /dev/vg0/lvol1}}
{{Note|use -L+350M to increase the current size of a LV by 350MB}}
Once the LV is extended, we need to grow the file system as well (in this example we used ext4 and the LV is mounted to /mnt/data):
{{Note|Some file systems do support online-resizing, like ext4 otherwise you have to umount the file system first}}
{{RootCmd|resize2fs /mnt/data 500M}}

==== Reduce LV ====
Before we can reduce the size of our Logical Volume (LV) without corrupting existing data, we have to shrink the file system on it. In this example we used ext4, the LV needs to be unmounted to shrink the file system:
{{RootCmd|umount /mnt/data
|e2fsck -f /dev/vg0/lvol1
|resize2fs /dev/vg0/lvol1 150M}}
Now we are ready to reduce the size of our LV:
{{RootCmd|lvreduce -L150M /dev/vg0/lvol1}}
{{Note|use -L-350M to reduce the current size of a LV by 350MB}}

==== LV Permissions ====
Logical Volumes (LV) can be set to be ''read only'' storage devices.
{{RootCmd|lvchange -p r /dev/vg0/lvol1}}
The LV needs to be remounted for the changes to take affect:
{{RootCmd|mount -o remount /dev/vg0/lvol1}}
To set the LV to be read/write again:
{{RootCmd|lvchange -p rw /dev/vg0/lvol1 && mount -o remount /dev/vg0/lvol1}}

==== Remove LV ====
Before we remove a Logical Volume (LV) we should unmount and deactivate, so no further write activity can take place:
{{RootCmd|umount /dev/vg0/lvol1 && lvchange -a n /dev/vg0/lvol1}}
The following command removes the LV named lvol1 from VG named vg0:
{{RootCmd|lvremove /dev/vg0/lvol1}}

== Examples ==
We can create some scenarios using loopback devices, so no real storage devices are used.

=== Preparation ===
First we need to make sure the loopback module is loaded. If you want to play around with partitions, use the following option:
{{RootCmd|modprobe -r loop && modprobe loop max_part{{=}}63}}
{{Note|you cannot reload the module, if it is built into the kernel}}
Now we need to either tell LVM to not use ''udev'' to scan for devices or change the filters in /etc/lvm/lvm.conf. In this case we just temporarely do not use ''udev'':
{{File|/etc/lvm/lvm.conf||<pre>
<nowiki>obtain_device_list_from_udev = 0</nowiki></pre>}}
{{Important|this is for testing only, you want to change the setting back when dealing with real devices since it is much faster}}
We create some image files, that will become our storage devices (uses ~6GB of real hard drive space):
{{RootCmd|mkdir /var/lib/lvm_img
|dd if{{=}}/dev/zero of{{=}}/var/lib/lvm_img/lvm0.img bs{{=}}1024 count{{=}}2097152
|dd if{{=}}/dev/zero of{{=}}/var/lib/lvm_img/lvm1.img bs{{=}}1024 count{{=}}2097152
|dd if{{=}}/dev/zero of{{=}}/var/lib/lvm_img/lvm2.img bs{{=}}1024 count{{=}}2097152}}
Check which loopback devices are available:
{{RootCmd|losetup -a}}
We assume all loopback devices are available and create our hard drives:
{{RootCmd|losetup /dev/loop0 /var/lib/lvm_img/lvm0.img
|losetup /dev/loop1 /var/lib/lvm_img/lvm1.img
|losetup /dev/loop2 /var/lib/lvm_img/lvm2.img}}
Now we can use /dev/loop[0-2] as we would use any other hard drive in the system.
{{Note|On the next reboot, all the loopback devices will be released and the folder /var/lib/lvm_img can be deleted}}

=== Two Hard Drives ===
In this example, we will initialize two hard drive as PV and then create the VG ''vg0'':
{{RootCmd|pvcreate /dev/loop[0-1]
|vgcreate vg0 /dev/loop[0-1]}}
Now lets create the LV ''lvol1'' in our VG ''vg0'' and take the maximum space available:
{{RootCmd|lvcreate -l 100%FREE -n lvol1 vg0}}
Create the file system and mount it to /mnt/data:
{{RootCmd|mkfs.ext4 /dev/vg0/lvol1
|mount /dev/vg0/lvol1 /mnt/data}}
Now we have the capacity of 2GB from each hard drive available in /mnt/data as one 4GB device.
{{Note|The same applies to RAID systems, if you want to create one VG use /dev/md[X-Y] instead}}

=== /etc/fstab ===
Here is an example of an entry in /etc/fstab (using ext4):
{{File|/etc/fstab||<pre>
/dev/vg0/lvol1  /mnt/data  ext4  noatime  0 2</pre>}}

=== LVM2 MIRROR ===
We use two hard drives and create our LV ''lvol1'' like in the first example. This time we use 40% of the size of our VG ''vg0'', because we need some space in the VG for the MIRROR and log files:
{{RootCmd|pvcreate /dev/loop[0-1]
|vgcreate vg0 /dev/loop[0-1]
|lvcreate -l 40%VG -n lvol1 vg0
|mkfs.ext4 /dev/vg0/lvol1
|mount /dev/vg0/lvol1 /mnt/data}}
To create our copy of /dev/vg0/lvol1 on the PV /dev/loop1, use the following command:
{{RootCmd|lvconvert -m1 /dev/vg0/lvol1 --corelog /dev/loop1}}
LVM will now ensure that a full copy (MIRROR) of /dev/vg0/lvol1 exists on /dev/loop1 and is not distributed between other PVs.
{{Note|this is very I/O intensive, --corelog writes the log files for the conversion into memory}}
To remove the MIRROR:
{{RootCmd|lvconvert -m0 /dev/vg0/lvol1}}
If one half of the MIRROR fails, the other one will be automatically converted into a not mirrored LV (loose the ''mirror'' atribute). LVM is different from Linux RAID1 that it doesn't read/write from both mirrored images, there is no performace increase.

=== LVM2 Snapshots ===
A snapshot is an LV as copy of another LV, which takes in all the changes that were made in the original LV to show the content of that LV in a different state. We once again use our two hard drives and create LV ''lvol1'' this time with 60% of VG ''vg0'':
{{RootCmd|pvcreate /dev/loop[0-1]
|vgcreate vg0 /dev/loop[0-1]
|lvcreate -l 60%VG -n lvol1 vg0
|mkfs.ext4 /dev/vg0/lvol1
|mount /dev/vg0/lvol1 /mnt/data}}
Now we create a snapshot of ''lvol1'' named ''08092011_lvol1'' and give it 10% of VG ''vg0'':
{{RootCmd|lvcreate -l 10%VG -s -n 08092011_lvol1 /dev/vg0/lvol1}}
{{Important|if a snapshot exceeds it's maximum size, it disappears}}
Mount our snapshot somewhere else:
{{RootCmd|mkdir /mnt/08092011_data
|mount /dev/vg0/08092011_lvol1 /mnt/08092011_data}}
We could now access data in ''lvol1'' from a previous state.<br/>
LVM2 snapshots are writeable LV, we could use them to let a project go on into two different directions:
{{RootCmd|lvcreate -l 10%VG -s -n project1_lvol1 /dev/vg0/lvol1
|lvcreate -l 10%VG -s -n project2_lvol1 /dev/vg0/lvol1
|mkdir /mnt/project1 /mnt/project2
|mount /dev/vg0/project1_lvol1 /mnt/project1
|mount /dev/vg0/project2_lvol1 /mnt/project2}}
Now we have three different versions of LV ''lvol1'', the original and two snapshots which can be used parallel and changes are written to the snapshots.
{{Note|the original LV ''lvol1'' cannot be reduced in size or removed if snapshots of it exist. Snapshots can be increased in size without growing the file system on them, but they cannot exceed the size of the original LV}}

=== LVM2 Rollback Snapshots ===
To rollback the logical volume to the version of the snapshot, use the following command:
{{RootCmd|lvconvert --merge /dev/vg0/08092011_lvol1}}
This might take a couple of minutes, depending on the size of the volume.
{{Important|the snapshot will disappear and this is not revertable}}

=== LVM2 Stripeset ===
The STRIPSET is the same as RAID0, data is written to several devices at the same time to increase performance. In LVM2 it is possible to distribute LV over several PV for the same effect. We create three PV and then VG ''vg0'':
{{RootCmd|pvcreate /dev/loop[0-2]
|vgcreate vg0 /dev/loop[0-2]}}
VG ''vg0'' consists of three different hard drives and now we can create our LV and spread it over them:
{{RootCmd|lvcreate -i 3 -l 20%VG -n lvm_stripe vg0}}
The option -i 3 indicates that we want to spread it over 3 PV in out VG vg0:
{{RootCmd|pvscan
|output=<pre>Logging initialised at Thu Sep  8 22:19:27 2011
    Set umask from 0022 to 0077
    Wiping cache of LVM-capable devices
    Wiping internal VG cache
    Walking through all physical volumes
  PV /dev/loop0   VG vg0   lvm2 [2.00 GiB / 1.60 GiB free]
  PV /dev/loop1   VG vg0   lvm2 [2.00 GiB / 1.60 GiB free]
  PV /dev/loop2   VG vg0   lvm2 [2.00 GiB / 1.60 GiB free]
  Total: 3 [5.99 GiB] / in use: 3 [5.99 GiB] / in no VG: 0 [0   ]
    Wiping internal VG cache
</pre>}}
On each PV 400MB got reserved for our LV ''lvm_stripe'' in VG ''vg0''

=== LVM2 RAID5 ===
LVM2 can use its internal mechanism to create stripesets with parity in a similar way as RAID5 does, but in this case you need at least 3 different PV
{{RootCmd|pvcreate /dev/loop[0-2]
|vgcreate vg0 /dev/loop[0-2]}}
VG ''vg0'' consists of three different hard drives and now we can create our LV and spread it over them:
{{RootCmd|lvcreate --type raid5 -l 20%VG -i 2 -I 64 -n lvm_raid5 vg0}}
The option -i 2 indicates that we want create 2 stripes + 1 parity stripe (so we need at least 3 devices)
{{RootCmd|pvscan
|output=<pre>Logging initialised at Thu Sep  8 22:19:27 2011
    Set umask from 0022 to 0077
    Wiping cache of LVM-capable devices
    Wiping internal VG cache
    Walking through all physical volumes  
  PV /dev/loop0   VG vg0      lvm2 [2,00 GiB / 1,39 GiB free]
  PV /dev/loop1   VG vg0      lvm2 [2,00 GiB / 1,39 GiB free]
  PV /dev/loop2   VG vg0      lvm2 [2,00 GiB / 1,39 GiB free]
  Total: 3 [5.99 GiB] / in use: 3 [5.99 GiB] / in no VG: 0 [0   ]
    Wiping internal VG cache
</pre>}}
On each PV 600MB got reserved for our LV ''lvm_raid5'' in VG ''vg0''

== Troubleshooting ==
LVM has only MIRROR and snapshots to provide some level of redundancy. However there are certain situations where one might be able to restore lost PV or LV.

=== vgcfgrestore ===
In /etc/lvm/archive and /etc/lvm/backup are files which contain logs about metadata changes in LVM. To see what states of the VG are available to be restored:
{{RootCmd|vgcfgrestore --list vg0
|output=<pre>File:         /etc/lvm/archive/vg0_00002-923283887.vg
  VG name:      vg0
  Description:  Created *before* executing 'lvremove /dev/vg0/lvol1'
  Backup Time:  Sat Sep 10 20:02:05 2011

  File:         /etc/lvm/backup/vg0
  VG name:      vg0
  Description:  Created *after* executing 'lvremove /dev/vg0/lvol1'
  Backup Time:  Sat Sep 10 20:02:05 2011
</pre>}}
In this example we removed the LV ''lvol1'' by accident and want it back in our VG ''vg0'':
{{RootCmd|vgcfgrestore -f /etc/archive/vg0_00002-923283887.vg vg0}}
{{Important|This is not the data in the LV itself that will be restored, just the LV is recreated as it was distributed on the PV before and you have a good chance that the files are still on the PV if they weren't overwritten yet}}

=== Replace PV ===
We want to replace a PV and then restore the metadata to a new one, so that we reach the same state as before the device stopped working. To display all PV in a VG (even lost ones) use the following command:
{{RootCmd|vgdisplay --partial --verbose}}
In this example I let /dev/loop1 (unknown device) fail:
{{RootCmd|vgdisplay --partial --verbose
|output=<pre>--- Physical volumes ---
  PV Name               unknown device
  PV UUID               3B0yFN-zKDY-yICo-fT0M-nJwS-3wZf-UEjvd1
  PV Status             allocatable
  Total PE / Free PE    511 / 0

  PV Name               /dev/loop2
  PV UUID               4AuFVX-PWnX-qpWl-fpoS-euCV-SISW-IX6ceF
  PV Status             allocatable
  Total PE / Free PE    511 / 511
</pre>}}
Using the UUID, we can tell LVM to restore new hardware and be implemented within the VG as the old one was.
{{RootCmd|pvcreate --restorefile /etc/archive/<METADATA-FILE> --uuid <UUID> /dev/loop3}}
Then we restore the VG to the state before the PV failed:
{{RootCmd|vgcfgrestore -f /etc/archive/<METADATA-FILE> vg0}}
Now you can replay your file backup if you haven't already restored the PV itself.

=== Deactivate LV ===
You can deactivat a LV with the following command:
{{RootCmd|umount /dev/vg0/lvol1
|lvchange -a n /dev/vg0/lvol1}}
You will not be able to mount the LV anywhere before it got reactivated:
{{RootCmd|lvchange -a y /dev/vg0/lvol1}}

== Links ==
* [http://sourceware.org/lvm2/ LVM2 sourceware.org]
* [http://tldp.org/HOWTO/LVM-HOWTO/ LVM tldp.org]
* [http://sources.redhat.com/lvm2/wiki/ LVM2 Wiki redhat.com]

[[Category:Hardware]]
