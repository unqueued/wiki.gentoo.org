<languages />

{{Metadata|abstract=С помощью LVM администраторы могут создавать метаустройства, предоставляющие уровень абстракции между файловой системой и нижним уровнем — физическими устройствами хранения.}}

{{InfoBox stack
|{{InfoBox wikipedia|Logical Volume Manager (Linux)|header=true}}
}}

С помощью '''LVM''' ('''L'''ogical '''V'''olume '''M'''anager) администраторы могут создавать метаустройства, предоставляющие уровень абстракции между файловой системой и нижним уровнем — физическими устройствами хранения. Такие метаустройства (на которых размещены файловые системы) называются ''логическими томами'' (''logical volumes''), они используют память набора устройств хранения — ''группы томов'' (''volume groups''). Группа томов включает один или несколько ''физических томов'' (''physical volumes''), которые, собственно, и хранят данные.

Физические тома могут быть разделами, целыми жесткими дисками SATA, сгруппированными как JBOD ('''J'''ust a '''B'''unch '''O'''f '''D'''isks - просто пачка дисков), системы RAID, iSCSI, Fibre Channel, eSATA и др.

== Установка ==

LVM регулируется как драйверами уровня ядра, так и пользовательскими приложениями для управления настройкой LVM.

=== Ядро ===

Активируйте следующие параметры ядра:

{{KernelBox|<pre>
Device Drivers  --->
   Multiple devices driver support (RAID and LVM)  --->
       <*> Device mapper support
           <*> Crypt target support
           <*> Snapshot target
           <*> Mirror target
       <*> Multipath target
           <*> I/O Path Selector based on the number of in-flight I/Os
           <*> I/O Path Selector based on the service time
</pre>}}

{{Note/ru|Не требуется включать абсолютно все; некоторые из параметров требуются только для [[#LVM2_Snapshots_and_LVM2_Thin_Snapshots|Снимков и тонких снимков LVM2]], [[#LVM2_Mirrors|Зеркалирования LVM2]], [[#LVM2_RAID_0.2FStripeset|RAID0 в LVM2]] и шифрования.}}

=== Программы ===

Установите {{Package|sys-fs/lvm2}}:

{{USEflag|package=sys-fs/lvm2
|clvm+++Позволяет пользователям собрать кластерную lvm2
|cman+++Поддержка cman для кластерной lvm
|lvm1+yes++Позволяет пользователям собрать lvm2 с поддержкой lvm1
|readline+yes++Добавляет поддержку libreadline, библиотеки GNU для редактирования строк, которую почти все хотят использовать
|selinux++no+!!только для внутреннего использования!! Поддержка Security Enchanted Linux (SELinux), этот флаг должен включаться профилем selinux, иначе произойдет поломка
|static+++!!не устанавливайте его в процессе начальной установки!! Включает статическую линковку для бинарных файлов вместо динамической
|static-libs+++Сборка статических библиотек
|thin+yes++Поддержка тонких томов
|udev+yes++Включить интеграцию sys-fs/udev (определение устройств, поддержка устройств хранения данных, управление питанием, и т.д.)
}}

{{Emerge|lvm2}}

== Конфигурация ==

Настройка LVM выполняется на нескольких уровнях:
# Управление логическими томами, физическими томами и группами томов с помощью утилит управления
# Тонкая настройка подсистемы LVM с помощью файла конфигурации
# Управление службами на уровне дистрибутива
# Настройка с помощью initramfs

Управление логическими и физическими томами, наряду с группами томов, рассматривается в главе [[#Usage|Usage]].

=== Файл конфигурации LVM ===

LVM обладает обширным файлом конфигурации, располагающимся в {{Path|/etc/lvm/lvm.conf}}. Большинству пользователей не требуется изменять настройки в этом файле для того, чтобы начать использовать LVM.

=== Управление службами ===

Gentoo предоставляет службу LVM для автоматического обнаружения и активирования групп томов и логических томов.

Эта служба может регулироваться системой init.

==== openrc ====

Для запуска LVM вручную:

{{RootCmd|/etc/init.d/lvm start}}

Для запуска LVM во время загрузки:

{{RootCmd|rc-update add lvm boot}}

==== systemd ====

Для запуска lvm вручную:

{{RootCmd|systemctl start lvm2-monitor.service}}

Для запуска LVM во время загрузки:

{{RootCmd|systemctl enable lvm.service}}

=== Использование LVM в initramfs ===

Большинство загрузчиков не способны загрузиться непосредственно с LVM. С этим не могут справиться ни GRUB legacy ни LILO. Grub 2 может загрузиться с "линейного" логического тома LVM (linear logical volume), "зеркального" логического тома (mirrored logical volume) и, возможно, некоторых видов логических томов RAID. На данный момент ни один загрузчик не поддерживает "тонкие" логические тома (thin logical volumes). 

По этой причине рекомендуется использовать загрузочный раздел без LVM и монтировать корневой раздел LVM из initramfs. Такая initramfs может быть сгенерирована автоматически с помощью [[Genkernel|genkernel]], {{Package|sys-kernel/genkernel-next}} и [[dracut]]:

* '''genkernel''' может загружаться с разделов любого типа, кроме тонких разделов (так как он не компилирует и не копирует пакет {{Package|thin-provisioning-tools}} с исходного хоста), и, может быть, RAID10 (поддержка RAID10 требует LVM2 2.02.98, но but genkernel собирает версию 2.02.89, однако если существуют статические бинарные файлы, он может их скопировать) 
* '''genkernel-next''' может загружаться с разделов любого типа, но требует достаточно новый {{Package|app-misc/pax-utils}}, либо получающиеся бинарные файлы для тонких разделов не будут работать (Смотри {{Bug|482504}}) 
* '''dracut''' должен загружаться с разделов любого типа, но он включает поддержку тонких разделов в initramfs только если корневой раздел хоста является тонким.

==== Genkernel/Genkernel-next ====

Установите либо {{Package|sys-kernel/genkernel}}, либо {{Package|sys-kernel/genkernel-next}}. Можно включить USE флаг static для пакета {{Package|sys-fs/lvm2}}, чтобы genkernel использовал бинарные файлы, найденные на системе (в противном случае он будет собирать свою собственную копию). Следующий пример соберет только initramfs (не все ядро) и добавит поддержку LVM.

{{RootCmd|genkernel --lvm initramfs}}

Man-страница genkernel описывает другие опции, использование которых зависит от требований системы.

Initrd требует параметров, которые сообщают ей, как запустить LVM. Их можно передать также, как и остальные параметры ядра. Например:

{{FileBox|filename=/etc/default/grub|title=Добавляем dolvm как параметр ядра при загрузке|lang=bash|1=
GRUB_CMDLINE_LINUX="dolvm"
}}

==== Dracut ====

Пакет {{Package|sys-kernel/dracut}} был портирован из проекта RedHat и является похожим инструментом для генерирования initramfs. Так как он в настоящее время находится в ~arch для тестирования, пользователям необходимо будет [[Knowledge_Base:Accepting_a_keyword_for_a_single_package|разрешить его]] (с использованием {{Path|/etc/portage/package.accept_keywords}}) для установки.  Перед тем, как это сделать, переменная <code>DRACUT_MODULES="lvm"</code> должна быть добавлена в {{Path|/etc/portage/make.conf}}.  Могут использоваться и другие модули, обратитесь к статье [[Dracut]]. Обычно следующие команды сгенерируют пригодную для использования initramfs по умолчанию.

{{RootCmd|dracut -a lvm}}

Initrd требует параметров, которые сообщают ей, как запустить LVM. Их можно передать также, как и остальные параметры ядра. Например:

{{FileBox|filename=/etc/default/grub|title=Добавляем dolvm как параметр ядра при загрузке|lang=bash|1=
GRUB_CMDLINE_LINUX="rd.lvm.vg=vol00"
}}

Для полного списка опций по LVM в dracut, просмотрите раздел в [https://www.kernel.org/pub/linux/utils/boot/dracut/dracut.html#_lvm руководстве по Dracut].

== Использование ==

LVM организует хранение на трех различных уровнях, описанных ниже:
* Жесткие диски, разделы, системы RAID, либо другие источники хранения инициализируются как Физические Тома (PV)
* Физические тома (PV) группируются вместе в Группы Томов (VG)
* Логические Тома (LV) создаются в Группах Томов (VG)

=== PV (Физический Том) ===
Физические тома это действительное железо, либо система хранения, для которой и собран LVM

==== Разбивка ====

{{Note/ru|Использование отдельных разделов для предоставления пространства группам томов необходимо только если вы не хотите использовать весь диск целиком как одну группу томов LVM. Если можно использовать целый диск, то пропустите этот шаг и инициализируйте весь жесткий диск как физический том.}}

Тип раздела для ''LVM'' это ''8e'' (Linux LVM).

Например, чтобы установить тип с помощью <code>fdisk</code> для раздела на {{Path|/dev/sda}}:

{{RootCmd|fdisk /dev/sda}}

В <code>fdisk</code>, создайте разделы, используя клавишу {{Key|n}}, а затем измените тип раздела с использованием клавиши {{Key|t}} на ''8e''.

==== Создание PV ====

Физические тома можно создать и проинициализировать с помощью команды <code>pvcreate</code>.

Например, следующая команда создает физический том на первом главном разделе {{Path|/dev/sda}} и {{Path|/dev/sdb}}:

{{RootCmd|pvcreate /dev/sd[ab]1}}

==== Получение списка PV ====

С помощью команды <code>pvdisplay</code> можно получить обзор всех активных физических томов на системе.

{{RootCmd|pvdisplay|output=<pre>
 --- Physical volume ---
  PV Name               /dev/sda1
  VG Name               volgrp
  PV Size               160.01 GiB / not usable 2.31 MiB
  Allocatable           yes 
  PE Size               4.00 MiB
  Total PE              40962
  Free PE               4098
  Allocated PE          36864
  PV UUID               3WHAz3-dh4r-RJ0E-5o6T-9Dbs-4xLe-inVwcV
  
 --- Physical volume ---
  PV Name               /dev/sdb1
  VG Name               volgrp
  PV Size               160.01 GiB / not usable 2.31 MiB
  Allocatable           yes 
  PE Size               4.00 MiB
  Total PE              40962
  Free PE               40962
  Allocated PE          0
  PV UUID               b031x0-6rej-BcBu-bE2C-eCXG-jObu-0Boo0x
</pre>}}

Если необходимо отобразить больше физических томов, то <code>pvscan</code> может определить неактивные физические тома и активировать их.

{{RootCmd|pvscan|output=<pre>
  PV /dev/sda1  VG volgrp        lvm2 [160.01 GiB / 16.01 GiB free]
  PV /dev/sdb1  VG volgrp        lvm2 [160.01 GiB / 160.01 GiB free]
  Total: 2 [320.02 GB] / in use: 2 [320.02 GiB] / in no VG: 0 [0]
</pre>}}

==== Удаление PV ====

LVM автоматически распределяет данные на все доступные физические тома (если не указано иное), но линейно. Если запрошенный логический том (внутри группы томов) меньше, чем объем свободного пространства на одном физическом томе, то все пространство для логического тома будет выделено на этом (одном) физическом томе, и оно будет непрерывно. Это сделано из соображений производительности.

Если физический том должен быть удален из группы томов, то вначале нужно переместить с физического тома данные. С помощью команды <code>pvmove</code> все данные на физическом томе перемещаются на другие физические тома в той же группе томов.

{{RootCmd|pvmove -v /dev/sda1}}

Такая операция может занять достаточно долгое время, в зависимости от объема данных, которые необходимо перенести. Как только она будет закончена, на устройстве не останется данных. Проверьте с помощью команды <code>pvdisplay</code>, что физический том больше не используется никаким логическим томом.

Следующий шаг это удаление физического тома из группы томов с использованием <code>vgreduce</code>, после чего устройство можно ''перестать'' считать физическим томом с использованием <code>pvremove</code>:

{{RootCmd|vgreduce vg0 /dev/sda1 && pvremove /dev/sda1}}

=== VG (Группа Томов) ===

Группа томов (VG) это некоторое число физических томов, которые отображаются как {{Path|/dev/ИМЯ_VG}} в файловой системе устройств. Имя группы томов выбирается администратором.

==== Создание VG ====

Следующая команда создает группу томов под названием ''vg0'', которой назначены два физических тома {{Path|/dev/sda1}} и {{Path|/dev/sdb1}}.

{{RootCmd|vgcreate vg0 /dev/sd[ab]1}}

==== Получение списка VG ====

Чтобы получить список всех активных групп томов, используйте команду <code>vgdisplay</code>:

{{RootCmd|vgdisplay|output=<pre>
  --- Volume group ---
  VG Name               vg0
  System ID             
  Format                lvm2
  Metadata Areas        1
  Metadata Sequence No  8
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                6
  Open LV               6
  Max PV                0
  Cur PV                1
  Act PV                1
  VG Size               320.02 GiB
  PE Size               4.00 MiB
  Total PE              81924
  Alloc PE / Size       36864 / 144.00 GiB
  Free  PE / Size       45056 /176.01 GiB
  VG UUID               mFPXj3-DdPi-7YJ5-9WKy-KA5Y-Vd4S-Lycxq3
</pre>}}

Если группы томов отсутствуют, используйте команду <code>vgscan</code>, чтобы найти группы томов:

{{RootCmd|vgscan|output=<pre>
  Reading all physical volumes.  This may take a while...
  Found volume group "vg0" using metadata type lvm2
</pre>}}

==== Расширение VG ====

Группы томов используются для группировки физических томов, что позволяет администраторам использовать пул ресурсов хранения, на котором можно создавать файловые системы. Когда в группе томов недостаточно ресурсов для хранения, необходимо расширить группу томов, добавив к ней дополнительные физические тома.

В следующем примере мы расширяем группу томов''vg0'' физическим томом {{Path|/dev/sdc1}}:

{{RootCmd|vgextend vg0 /dev/sdc1}}

Помните, что сначала физический том должен быть проинициализирован как таковой!

==== Уменьшение VG ====

Если физические тома нужно удалить из группы томов, то данные, которые все еще используются на физическом томе нужно перенести в другие физические тома в той же группе томов. Как мы видели выше, это делается с помощью команды <code>pvmove</code>, после чего физический том можно удалить из группы томов, используя <code>vgreduce</code>:

{{RootCmd|pvmove -v /dev/sdc1
|vgreduce vg0 /dev/sdc1}}

==== Удаление VG ====

Если группа томов больше не нужна (или, другими словами, пул хранения, который она представляет, больше не используется, и физические тома в ней нужно освободить для других целей), то группа томов может быть удалена с помощью <code>vgremove</code>. Это работает только если для группы томов не заведено ни одного логического тома, и все кроме физические тома, кроме последнего, были уже удалены из пула.

{{RootCmd|vgremove vg0}}

=== LV (Логический Том) ===

Логические тома это финальные мета-устройства, которые обычно используются системой для создания файловых систем. Они создаются и с ними можно работать только внутри групп томов, и они называются {{Path|/dev/ИМЯ_VG/ИМЯ_LV}}. Как и в случае с группами томов, имя для логического тома придумывает администратор.

==== Создание LV ====

Чтобы создать логический том, используется команда <code>lvcreate</code>. Параметрами к данной команде нужно указать необходимый размер логического тома (который не может быть больше объема свободного места в группе томов), группу томов, в которой нужно выделить пространство, и имя логического тома для создания.

В следующем примере создается логический том с названием ''lvol1'' в группе томов с названием ''vg0'' с размером 150 Мб:

{{RootCmd|lvcreate -L 150M -n lvol1 vg0}}

Существует возможность сказать <code>lvcreate</code>, что нужно использовать все свободное пространство внутри группы томов. Это делается с использованием параметра ''-l'', который выбирает число ''экстентов'', а не (человекочитаемый) размер. Логические тома разбиты на ''логические экстенты'', которые представляют собой куски данных внутри группы томов. Все экстенты в группе томов имеют одинаковый размер. С помощью параметра ''-l'' команду <code>lvcreate</code> можно попросить выделить все свободные экстенты:

{{RootCmd|lvcreate -l 100%FREE -n lvol1 vg0}}

Кроме ''FREE'' может использоваться ключ ''VG'', который означает весь размер группы томов.

==== Получение списка LV ====

Чтобы получить список всех логических томов, используйте команду <code>lvdisplay</code>:

{{RootCmd|lvdisplay}}

Если логические тома отсутствуют, то можно использовать команду <code>lvscan</code>, чтобы просканировать все существующие группы томов в поиске логических томов

{{RootCmd|lvscan}}

==== Расширение LV ====

Когда логический том нужно расширить, можно использовать команду <code>lvextend</code> с целью увеличения выделенного для логического тома места.

Например, чтобы расширить логический том ''lvol1'' до 500 Мб:

{{RootCmd|lvextend -L500M /dev/vg0/lvol1}}

Также возможно указать размер, который нужно прибавить, а не конечный размер:

{{RootCmd|lvextend -L+350MB /dev/vg0/lvol1}}

Расширение группы томов не дает мгновенного увеличения пространства для конечных пользователей. Для этого необходимо также увеличить объем файловой системы, созданной в данной группе томов. Не все файловые системы поддерживают увеличение пространства без размонтирования, поэтому проверьте документацию по интересующей вас файловой системе для более детальной информации.

Например, чтобы изменить размер файловой системы ext4, чтобы она стала размером 500 Мб:

{{RootCmd|resize2fs /mnt/data 500M}}

==== Уменьшение LV ====

Если необходимо уменьшить размер логического тома, сначала необходимо уменьшить размер самой файловой системы. Не все файловые системы поддерживают уменьшение объема без размонтирования.

Например, ext4 не поддерживает уменьшение размера без размонтирования, поэтому сначала файловую систему нужно размонтировать. Также рекомендуется сначала проверить файловую систему, чтобы удостовериться, что нет никаких несоответствий.

{{RootCmd|umount /mnt/data
|e2fsck -f /dev/vg0/lvol1
|resize2fs /dev/vg0/lvol1 150M}}

С уменьшенной файловой системой теперь можно уменьшить и логический том:

{{RootCmd|lvreduce -L150M /dev/vg0/lvol1}}

==== Разрешения на LV ====

LVM поддерживает разрешения на логические тома.

Например, логический том можно установить в состояние ''только для чтения'' с использованием команды <code>lvchange</code>:

{{RootCmd|lvchange -p r /dev/vg0/lvol1
|mount -o remount /dev/vg0/lvol1}}

Требуется перемонтирование, так как данное изменение не применяется мгновенно.

Чтобы пометить логический том как снова доступный для записи, используйте бит разрешений ''rw'':

{{RootCmd|lvchange -p rw /dev/vg0/lvol1 && mount -o remount /dev/vg0/lvol1}}

==== Удаление LV ====

Перед удалением логического тома, удостоверьтесь, что он больше не смонтирован:

{{RootCmd|umount /dev/vg0/lvol1}}

Деактивируйте логический том, чтобы больше не происходило никаких операций записи:

{{RootCmd|lvchange -a n /dev/vg0/lvol1}}

После размонтирования и деактивации тома, теперь можно его удалить, освободив выделенные для него экстенты, чтобы их могли использовать другие логические тома в группе томов.

{{RootCmd|lvremove /dev/vg0/lvol1}}

== Возможности ==

LVM имеет несколько интересных возможностей для администраторов по данным, включая (но не только)
* тонкие тома (перебор места)
* поддержка снимков
* типы томов с различными методами выделения пространства

=== Тонкие тома ===

Новые версии LVM2 (2.02.89) поддерживают ''тонкие'' тома. Тонкие тома для блочных устройств это то же самое, что разреженные файлы для файловых систем. Поэтому тонкий логический том внутри пула может быть ''перегружен'': его размер может быть больше чем выделенный размер - он может быть даже больше чем весь пул. Как и с разреженными файлами, экстенты выделяются тогда, когда блочное устройство заполняется данными. Если файловая система поддерживает функцию ''discard'', то экстенты будут освобождаться при удалении файлов, что приведет к уменьшению занимаемого пулом места.

Внутри LVM такой тонкий пул - это специальный логический том, который также может содержать в себе логические тома.

==== Создание тонкого пула ====

{{Warning/ru|Если случится переполнение в метаданных тонкого пула, то пул будет поврежден. '''LVM не может восстановиться после такого'''.}} 

{{Note/ru|Если резервы тонкого пула исчерпаны, то любой процесс, который будет просить тонкий пул выделить еще (несуществующие) экстенты, будет подвешен в состоянии ''убиваемый сон'', пока либо тонкий пул не будет увеличен в размере, либо процесс не получит сигнал SIGKILL.}}

У каждого тонкого пула есть связанные с ним метаданные, которые добавляются к размеру тонкого пула. LVM подсчитывает размер метаданных на основании размера тонкого пула как минимальное из значений ''число_чанков_пула * 64 байта'' или 2Миб, смотря что больше. Администратор может выбрать и другой размер метаданных.

To create a thin pool, add the ''--type thin-pool --thinpool thin_pool'' parameters to <code>lvcreate</code>:

{{RootCmd|lvcreate -L 150M --type thin-pool --thinpool thin_pool vg0}}

The above example creates a thin pool called ''thin_pool'' with a total size of 150 MB. This is the real allocated size for the thin pool (and thus the total amount of actual storage that can be used).

To explicitly ask for a certain metadata size, use the ''--metadatasize'' parameter:

{{RootCmd|lvcreate -L 150M --metadatasize 2M --type thin-pool --thinpool thin_pool vg0}}

Due to the metadata that is added to the thin pool, the intuitive way of using all available size in a volume group for a logical volume does not work (see LVM bug [https://bugzilla.redhat.com/show_bug.cgi?id=812726|812726]):

{{RootCmd|lvcreate -l 100%FREE --type thin-pool --thinpool thin_pool vg0|output=<pre>
Insufficient suitable allocatable extents for logical volume thin_pool: 549 more required
</pre>}}

Note the thin pool does not have an associated device node like other LV's.

==== Создание тонкого логического тома ====

A ''thin logical volume'' is a logical volume inside the thin pool (which itself is a logical volume). As thin logical volumes are ''sparse'', a virtual size instead of a physical size is specified using the ''-V'' parameter:

{{RootCmd|lvcreate -T vg0/thin_pool -V 300M -n lvol1}}

In this example, the (thin) logical volume ''lvol1'' is exposed as a 300MB-sized device, even though the underlying pool only holds 150MB of real allocated storage.

It is also possible to create both the thin pool as well as the logical volume inside the thin pool in one command:

{{RootCmd|lvcreate -T vg0/thin_pool -V 300M -L150M -n lvol1}}

==== Получение списка тонких пулов и тонких логических томов ====

Thin pools and thin logical volumes are special types of logical volumes, and as such as displayed through the <code>lvdisplay</code> command. The <code>lvscan</code> command will also detect these logical volumes.

==== Расширение тонкого пула ====

{{Warning|As of LVM2 2.02.89, the metadata size of the thin pool cannot be expanded, it is fixed at creation}}

The thin pool is expanded like a non-thin logical volume using <code>lvextend</code>. For instance:

{{RootCmd|lvextend -L500M vg0/thin_pool}}

==== Расширение тонкого логического тома ====

A thin logical volume is expanded just like a regular one:

{{RootCmd|lvextend -L1G vg0/lvol1}}

Note that the <code>lvextend</code> command uses the ''-L'' option (or ''-l'' if extent counts are used) and not a "virtual size" option as was used during the creation. 

==== Уменьшение тонкого пула ====

Currently, LVM cannot reduce the size of the thin pool. See LVM bug [https://bugzilla.redhat.com/show_bug.cgi?id=812731|812731].

==== Уменьшение тонкого логического тома ====

Thin logical volumes are reduced just like regular logical volumes.

Например:
{{RootCmd|lvreduce -L300M vg0/lvol1l}}

Note that the <code>lvreduce</code> command uses the ''-L'' option (or ''-l'' if extent counts are used) and not a "virtual size" option as was used during the creation.

==== Удаление тонких пулов ====

Thin pools cannot be removed until all the thin logical volumes inside it are removed.

When a thin pool no longer services any thin logical volume, it can be removed through the <code>lvremove</code> command:

{{RootCmd|lvremove vg0/thin_pool}}

=== Снимки LVM2 и тонкие снимки ===

A snapshot is a logical volume that acts as copy of another logical volume. It displays the state of the original logical volume at the time of snapshot creation.

{{Warning|Since the logical snapshot volume also gets the same filesystem ''LABEL'' and ''UUID'', be sure that your {{Path|/etc/fstab}} file or initramfs '''does not''' contain entries for these filesystems using the <code>LABEL{{=}}</code> or <code>UUID{{=}}</code> syntax. Otherwise you might end up with the snapshot being mounted instead of the (intended) original logical volume.}}

==== Создание логического тома - снимка ====

A snapshot logical volume is created using the ''-s'' option to <code>lvcreate</code>. Snapshot logical volumes are still given allocated storage as LVM "registers" all changes made to the original logical volume and stores these changes in the allocated storage for the snapshot. When querying the snapshot state, LVM will start from the original logical volume and then check all changes registered, "undoing" the changes before showing the result to the user.

A snapshot logical volume henceforth "growths" at the rate that changes are made on the original logical volume. When the allocated storage for the snapshot is completely used, then the snapshot will be removed automatically from the system.

{{RootCmd|lvcreate -l 10%VG -s -n 20140412_lvol1 /dev/vg0/lvol1}}

The above example creates a snapshot logical volume called ''20140412_lvol1'', based on the logical volume ''lvol1'' in volume group ''vg0''. It uses 10% of the space (extents actually) allocated to the volume group.

==== Accessing a snapshot logical volume ====

Snapshot logical volumes can be mounted like regular logical volumes. They are even not restricted to read-only operations - it is possible to modify snapshots and thus use it for things such as testing changes before doing these on a "production" file system.

As long as snapshot logical volumes exist, the regular/original logical volume cannot be reduced in size or removed.

==== LVM thin snapshots ====

{{Note|A thin snapshot can only be taken on a thin pool for a thin logical volume. The thin device mapper target supports thin snapshots of read-only non-thin logical volumes, but the LVM2 tooling does not support this. However, it is possible to create a regular (non-thin) snapshot logical volume of a thin logical volume.}}

To create a thin snapshot, the <code>lvcreate</code> command is used with the <code>-s</code> option. No size declaration needs to be passed on:

{{RootCmd|lvcreate -s -n 20140413_lvol1 /dev/vg0/lvol1}}

Thin logical volume snapshots have the same size as their original thin logical volume, and use a physical allocation of 0 just like all other thin logical volumes. 

{{Important|If ''-l'' or ''-L'' is specified, a snapshot will still be created, but the resulting snapshot will be a regular snapshot, not a thin snapshot.}}

It is also possible to take snapshots of snapshots:

{{RootCmd|lvcreate -s -n 1_20140413_lvol1 /dev/vg0/20140413_lvol1}}

Thin snapshots have several advantages over regular snapshots. First, thin snapshots are independent of their original logical volume once created. The original logical volume can be shrunk or deleted without affecting the snapshot. Second, thin snapshots can be efficiently created recursively (snapshots of snapshots) without the "chaining" overhead of regular recursive LVM snapshots.

==== Rolling back to snapshot state ====

To rollback the logical volume to the version of the snapshot, use the following command:

{{RootCmd|lvconvert --merge /dev/vg0/20140413_lvol1}}

This might take a couple of minutes, depending on the size of the volume.

{{Important|The snapshot will disappear and this change is not revertible}}

==== Rolling back thin snapshots ====

For thin volumes, <code>lvconvert --merge</code> does not work. Instead, delete the original logical volume and rename the snapshot:

{{RootCmd|umount /dev/vg0/lvol1
|lvremove /dev/vg0/lvol1
|lvrename vg0/20140413_lvol1 lvol1}}

=== Different storage allocation methods ===

LVM supports different allocation methods for storage:
* linear volumes (which is the default)
* mirrored volumes (in a more-or-less active/standby setup)
* striping (RAID0)
* mirrored volumes (RAID1 - which is more an active/active setup)
* striping with parity (RAID4 and RAID5)
* striping with double parity (RAID6)
* striping and mirroring (RAID10)

==== Linear volumes ====

Linear volumes are the most common kind of LVM volumes. LVM will attempt to allocate the logical volume to be as physically contiguous as possible. If there is a physical volume large enough to hold the entire logical volume, then LVM will allocate it there, otherwise it will split it up into as few pieces as possible.

The commands introduced earlier on to create volume groups and logical volumes create linear volumes.

Because linear volumes have no special requirements, they are the easiest to manipulate and can be resized and relocated at will. If a logical volume is allocated across multiple physical volumes, and any of the physical volumes become unavailable, then that logical volume cannot be started anymore and will be unusable.

==== Mirrored volumes ====

LVM supports ''mirrored'' volumes, which provide fault tolerance in the event of drive failure. Unlike RAID1, there is no performance benefit - all reads and writes are delivered to a single side of the mirror.

To keep track of the mirror state, LVM requires a ''log'' to be kept. It is recommended (and often even mandatory) to position this log on a physical volume that does not contain any of the mirrored logical volumes. There are three kind of logs that can be used for mirrors:

# '''Disk''' is the default log type. All changes made are logged into extra metadata extents, which LVM manages. If a device fails, then the changes are kept in the log until the mirror can be restored again.
# '''Mirror''' logs are '''disk''' logs that are themselves mirrored. 
# '''Core''' mirror logs record the state of the mirror in memory only. LVM will have to rebuild the mirror every time it is activated. This type is useful for temporary mirrors.

To create a logical volume with a single mirror, pass the ''-m 1'' argument (to select standard mirroring) with optionally ''--mirrorlog'' to select a particular log type:

{{RootCmd|lvcreate -m 1 --mirrorlog mirror -l 40%VG --nosync -n lvol1 vg0}}

The ''-m 1'' tells LVM to create one (additional) mirror, so requiring 2 physical volumes. The ''--nosync'' option is an optimization - without it LVM will try synchronize the mirror by copying empty sectors from one logical volume to another.

It is possible to create a mirror of an existing logical volume:

{{RootCmd|lvconvert -m 1 -b vg0/lvol1}}

The ''-b'' option does the conversion in the background as this can take quite a while.

To remove a mirror, set the number of mirrors (back) to 0:

{{RootCmd|lvconvert -m0 vg0/lvol1}}

If part of the mirror is unavailable (usually because the disk containing the physical volume has failed), the volume group will need to be brought up in degraded mode:

{{RootCmd|vgchange -ay --partial vg0}}

On the first write, LVM will notice the mirror is broken. The default policy ("remove") is to automatically reduce/break the mirror according to the number of pieces available. A 3-way mirror with a missing physical volume will be reduced to 2-way mirror; a 2-way mirror will be reduced to a regular linear volume. If the failure is only transient, and the missing physical volume returns after LVM has broken the mirror, the mirrored logical volume will need to be recreated on it. 

To recover the mirror, the failed physical volume needs to be removed from the volume group, and a replacement physical volume needs to be added (or if the volume group has a free physical volume, it can be created on that one). Then the mirror can be recreated with <code>lvconvert</code> at which point the old physical volume can be removed from the volume group:

{{RootCmd|vgextend vg0 /dev/sdc1
|lvconvert -b -m 1 --mirrorlog disk vg0/lvol1
|vgreduce --removemissing vg0}}

It is possible to have LVM recreate the mirror with free extents on a different physical volume if one side fails. To accomplish that, set <code>mirror_image_fault_policy</code> to ''allocate'' in {{Path|lvm.conf}}.

==== Thin mirrors ====

It is not (yet) possible to create a mirrored thin pool or thin volume. It is possible to create a mirrored thin pool by creating a normal mirrored logical volume and then converting the logical volume to a thin pool with <code>lvconvert</code>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will merge them into a single logical volume.

{{Warning|LVM 2.02.98 or above is required for this to work properly. Prior versions are either not capable or will segfault and corrupt the volume group. Also, conversion of a mirror into a thin pool '''destroys''' all existing data in the mirror!}}

{{RootCmd|lvcreate -m 1 --mirrorlog mirrored -l40%VG -n thin_pool vg0
|lvcreate -m 1 --mirrorlog mirrored -L4MB -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg0/thin_meta}}

==== Striping (RAID0) ====

Instead of a linear volume, where multiple contiguous physical volumes are appended, it possible to create a ''striped'' or ''RAID0'' volume for better performance. This will alternate storage allocations across the available physical volumes.

To create a striped volume over three physical volumes:

{{RootCmd|lvcreate -i 3 -l 20%VG -n lvol1_stripe vg0|output=<pre>
Using default stripesize 64.00 KiB
</pre>}}

The -i option indicates over how many physical volumes the striping should be done.

It is possible to mirror a stripe set. The -i and -m options can be combined to create a striped mirror:

{{RootCmd|lvcreate -i 2 -m 1 -l 10%VG vg0}}

This creates a 2 physical volume stripe set and mirrors it on 2 different physical volumes, for a total of 4 physical volumes. An existing stripe set can be mirrored with <code>lvconvert</code>.

A thin pool can be striped like any other logical volume. All the thin volumes created from the pool inherit that settings - do not specify it manually when creating a thin volume.

It is not possible to stripe an existing volume, nor reshape the stripes across more/less physical volumes, nor to convert to a different RAID level/linear volume. A stripe set can be mirrored. It is possible to extend a stripe set across additional physical volumes, but they must be added in multiples of the original stripe set (which will effectively linearly append a new stripe set).

==== Mirroring (RAID1) ====

Unlike RAID0, which is striping, RAID1 is mirroring, but implemented differently than the original LVM mirror. Under RAID1, reads are spread out across physical volumes, improving performance. RAID1 mirror failures do not cause I/O to block because LVM does not need to break it on write.

Any place where an LVM mirror could be used, a RAID1 mirror can be used in its place. It is possible to have LVM create RAID1 mirrors instead of regular mirrors implicitly by setting ''mirror_segtype_default'' to ''raid1'' in {{Path|lvm.conf}}.

To create a logical volume with a single mirror:

{{RootCmd|lvcreate -m 1 --type raid1 -l 40%VG --nosync -n lvm_raid1 vg0}}

Note the difference for creating a mirror: There is no ''mirrorlog'' specified, because RAID1 logical volumes do not have an explicit mirror log - it built-in to the logical volume.

It is possible to convert an existing logical volume to RAID1:

{{RootCmd|lvconvert -m 1 --type raid1 -b vg0/lvol1}}

To remove a RAID1 mirror, set the number of mirrors to 0:

{{RootCmd|lvconvert -m0 vg0/lvm_raid1}}

If part of the RAID1 is unavailable (usually because the disk containing the physical volume has failed), the volume group will need to be brought up in degraded mode:

{{RootCmd|vgchange -ay --partial vg0}}

Unlike an LVM mirror, writing does NOT break the mirroring. If the failure is only transient, and the missing physical volume returns, LVM will resync the mirror by copying cover the out-of-date segments instead of the entire logical volume. If the failure is permanent, then the failed physical volume needs to be removed from the volume group, and a replacement physical volume needs to be added (or if the volume group has a free physical volume, it can be created on a different PV). The mirror can then be repaired with <code>lvconvert</code>, and the old physical volume can be removed from the volume group:

{{RootCmd|vgextend vg0 /dev/sdc1
|lvconvert --repair -b vg0/lvm_raid1
|vgreduce --removemissing vg0}}

==== Тонкий RAID1 ====

It is not (yet) possible to create a RAID1 thin pool or thin volume. It is possible to create a RAID1 thin pool by creating a normal mirrored logical volume and then converting the logical volume to a thin pool with <code>lvconvert</code>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will then merge them into a single logical volume.

{{Warning|LVM 2.02.98 or above is required for this to work properly. Prior versions are either not capable or will segfault and corrupt the VG. Also, conversion of a RAID1 into a thin pool '''destroys''' all existing data in the mirror!}}

{{RootCmd|lvcreate -m 1 --type raid1 -l40%VG -n thin_pool vg0
|lvcreate -m 1 --type raid1 -L4MB -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg00/thin_meta}}

==== Striping with parity (RAID4 and RAID5) ====

{{Note|Striping with parity requires at least 3 physical volumes.}}

RAID0 is not fault-tolerant - if any of the physical volumes fail then the logical volume is unusable. By adding a parity stripe to RAID0 the logical volume can still function if a physical volume is missing. A new physical volume can then be added to restore fault tolerance.

Stripsets with parity come in 2 flavors: RAID4 and RAID5. Under RAID4, all the parity stripes are stored on the same physical volume. This can become a bottleneck because all writes hit that physical volume, and it gets worse the more physical volumes are in the array. With RAID5, the parity data is distributed evenly across the physical volumes so none of them become a bottleneck. For that reason, RAID4 is rare and is considered obsolete/historical. In practice, all stripesets with parity are RAID5.

{{RootCmd|lvcreate --type raid5 -l 20%VG -i 2 -n lvm_raid5 vg0}}

Only the data physical volumes are specified with -i, LVM adds one to it automatically for the parity. So for a 3 physical volume RAID5, ''-i 2'' is passed on and not ''-i 3''.

When a physical volume fails, then the volume group will need to be brought up in degraded mode:

{{RootCmd|vgchange -ay --partial vg0}}

The volume will work normally at this point, however this degrades the array to RAID0 until a replacement physical volume is added. Performance is unlikely to be affected while the array is degraded - although it does need to recompute its missing data via parity, it only requires simple XOR for the parity block with the remaining data. The overhead is negligible compared to the disk I/O.

Чтобы починить RAID5:

{{RootCmd|lvconvert --repair vg0/lvm_raid5
|vgreduce --removemissing vg0}}

It is possible to replace a still working physical volume in RAID5 as well:

{{RootCmd|lvconvert --replace /dev/sdb1 vg0/lvm_raid5
|vgreduce vg0 /dev/sdb1}}

The same restrictions of stripe sets apply to stripe sets with parity as well: it is not possible to enable striping with parity on an existing volume, nor reshape the stripes with parity across more/less physical volumes, nor to convert to a different RAID level/linear volume. A stripe set with parity can be mirrored. It is possible to extend a stripe set with parity across additional physical volumes, but they must be added in multiples of the original stripe set with parity (which will effectively linearly append a new stripe set with parity).

==== Thin RAID5 logical volumes ====

It is not (yet) possible to create stripe set with parity (RAID5) thin pools or thin logical volumes. It is possible to create a RAID5 thin pool by creating a normal RAID5 logical volume and then converting the logical volume into a thin pool with <code>lvconvert</code>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will merge them into a single logical volume.

{{Warning|LVM 2.02.98 or above is required for this to work properly. Prior versions are either not capable or will segfault and corrupt the VG. Also, coversion of a RAID5 LV into a thin pool '''destroys''' all existing data in the LV!}}

{{RootCmd|lvcreate --type raid5 -i 2  -l20%VG -n thin_pool vg0
|lvcreate --type raid5 -i 2 -L4MB -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg00/thin_meta}}

==== Striping with double parity (RAID6) ====

{{Note|RAID6 requires at least 5 physical volumes.}}

RAID6 is similar to RAID5, however RAID6 can survive up to '''two''' physical volume failures, thus offering more fault tolerance than RAID5 at the expense of extra physical volumes. 

{{RootCmd|lvcreate --type raid6 -l 20%VG -i 3 -n lvm_raid6 vg00}}

Like RAID5, the -i option is used to specify the number of physical volumes to stripe, excluding the 2 physical volumes for parity. So for a 5 physical volume RAID6, pass on ''-i 3'' and not ''-i 5''.

Recovery for RAID6 is the same as RAID5.

{{Note|Unlike RAID5 where parity block is cheap to recompute vs disk I/O, this is only half true in RAID6. RAID6 uses 2 parity stripes: One stripe is computed the same way as RAID5 (simple XOR). The second parity stripe is much harder to compute - see [https://www.kernel.org/pub/linux/kernel/people/hpa/raid6.pdf|raid6 (pdf)] for more information.}}

==== Thin RAID6 logical volumes ====

It is not (yet) possible to create a RAID6 thin pool or thin volumes. It is possible to create a RAID6 thin pool by creating a normal RAID6 logical volume and then converting the logical volume into a thin pool with <code>lvconvert</code>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will merge them into a single logical volume.

{{Warning|LVM 2.02.98 or above is required for this to work properly. Prior versions are either not capable or will segfault and corrupt the VG. Also, conversion of a RAID6 LV into a thin pool '''destroys''' all existing data in the LV!}}

{{RootCmd|lvcreate --type raid6 -i 2  -l20%VG -n thin_pool vg0
|lvcreate --type raid6 -i 2 -L4MB -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg0/thin_meta}}

==== LVM RAID10 ====

{{Note|RAID10 requires at least 4 physical volumes. Also LVM syntax requires the number of physical volumes be multiple of the numbers stripes and mirror, even though RAID10 format does not}}

RAID10 is a combination of RAID0 and RAID1. It is more powerful than RAID0+RAID1 as the mirroring is done at the stripe level instead of the logical volume level, and therefore the layout doesn't need to be symmetric. A RAID10 volume can tolerate at least a single missing physical volume, and possibly more.

{{Note|LVM currently limits RAID10 to a single mirror.}}

{{RootCmd|lvcreate --type raid10 -l 1020 -i 2 -m 1 --nosync -n lvm_raid10 vg0}}

Both the ''-i and -m'' options are specified: ''-i'' is the number of stripes and ''-m'' is the number of mirrors. Two stripes and 1 mirror requires 4 physical volumes.

==== Тонкий RAID10 ====

It is not (yet) possible to create a RAID10 thin pool or thin volumes. It is possible to create a RAID10 thin pool by creating a normal RAID10 logical volume and then converting the logical volume into a thin pool with <code>lvconvert</code>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will merge them into a single logical volume.
 
{{Warning|Conversion of a RAID10 logical volume into a thin pool '''destroys''' all existing data in the logical volume!}}

{{RootCmd|lvcreate -i 2 -m 1 --type raid10 -l 1012 -n thin_pool vg0
|lvcreate -i 2 -m 1 --type raid10 -l 6 -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg0/thin_meta}}

== Эксперименты с LVM ==

It is possible to experiment with LVM without using real storage devices. To accomplish this, loopback devices are created.

First make sure to have the loopback module loaded. 

{{RootCmd|modprobe -r loop && modprobe loop max_part{{=}}63}}

{{Note|If loopback support is built into the kernel, then use <code>loop.max_part{{=}}63</code> as boot option.}}

Next configure LVM to not use [[udev]] to scan for devices:

{{FileBox|filename=/etc/lvm/lvm.conf|title=Отключаем udev в конфигурации LVM|lang=ini|1=
obtain_device_list_from_udev = 0
}}

{{Important|This is for testing only, make sure to change the setting back when dealing with real devices since it is much faster to use udev!}}

Create some image files which will become the storage devices. The next example uses five files for a total of about ~10GB of real hard drive space:

{{RootCmd|mkdir /var/lib/lvm_img
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm0.img bs{{=}}1024 seek{{=}}2097152
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm1.img bs{{=}}1024 seek{{=}}2097152
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm2.img bs{{=}}1024 seek{{=}}2097152
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm3.img bs{{=}}1024 seek{{=}}2097152
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm4.img bs{{=}}1024 seek{{=}}2097152}}

Check which loopback devices are available:

{{RootCmd|losetup -a}}

Assuming all loopback devices are available, next create the devices:

{{RootCmd|losetup /dev/loop0 /var/lib/lvm_img/lvm0.img
|losetup /dev/loop1 /var/lib/lvm_img/lvm1.img
|losetup /dev/loop2 /var/lib/lvm_img/lvm2.img
|losetup /dev/loop3 /var/lib/lvm_img/lvm3.img
|losetup /dev/loop4 /var/lib/lvm_img/lvm4.img}}

The {{Path|/dev/loop[0-4]}} devices are now available to use as any other hard drive in the system (and thus be perfect for physical volumes).

{{Note|On the next reboot, all the loopback devices will be released and the folder {{Path|/var/lib/lvm_img}} can be deleted.}}

== Устранение проблем ==

LVM has a few features that already provide some level of redundancy. However, there are situations where it is possible to restore lost physical volumes or logical volumes.

=== Утилита vgcfgrestore ===

By default, on any change to a LVM physical volume, volume group, or logical volume, LVM2 create a backup file of the metadata in {{Path|/etc/lvm/archive}}. These files can be used to recover from an accidental change (like deleting the wrong logical volume). LVM also keeps a backup copy of the most recent metadata in {{Path|/etc/lvm/backup}}. These can be used to restore metadata to a replacement disk, or repair corrupted metadata.

To see what states of the volume group are available to be restored (partial output to improve readability):

{{RootCmd|vgcfgrestore --list vg00|output=<pre>
  File:		/etc/lvm/archive/vg0_00042-302371184.vg
  VG name:    	vg0
  Description:	Создан *до* выполнения 'lvremove vg0/lvm_raid1'
  Backup Time:	Sat Jul 13 01:41:32 201
</pre>}}

==== Recovering an accidentally deleted logical volume ====

Assuming the logical volume ''lvm_raid1'' was accidentally removed from volume group ''vg0'', it is possible to recover it as follows:

{{RootCmd|vgcfgrestore -f /etc/lvm/archive/vg0_00042-302371184.vg vg0}}

{{Important|<code>vgcfgrestore</code> only restores LVM metadata, ''not'' the data inside the logical volume. However <code>pvremove</code>, <code>vgremove</code>, and <code>lvremove</code> only wipe metadata, leaving any data intact. If <code>issue_discards</code> is set in {{Path|/etc/lvm/lvm.conf}} though, then these command ''are'' destructive to data.}}

==== Replacing a failed physical volume ====

It possible to do a true "replace" and recreate the metadata on the new physical volume to be the same as the old physical volume:

{{RootCmd|vgdisplay --partial --verbose|output=<pre>
  --- Physical volumes ---
  PV Name               /dev/loop0     
  PV UUID               iLdp2U-GX3X-W2PY-aSlX-AVE9-7zVC-Cjr5VU
  PV Status             allocatable
  Total PE / Free PE    511 / 102
  
  PV Name               unknown device     
  PV UUID               T7bUjc-PYoO-bMqI-53vh-uxOV-xHYv-0VejBY
  PV Status             allocatable
  Total PE / Free PE    511 / 102
</pre>}}

The important line here is the UUID "unknown device". 

{{RootCmd|pvcreate --uuid T7bUjc-PYoO-bMqI-53vh-uxOV-xHYv-0VejBY --restorefile /etc/lvm/backup/vg0 /dev/loop1|output=<pre>
  Couldn't find device with uuid T7bUjc-PYoO-bMqI-53vh-uxOV-xHYv-0VejBY.
  Physical volume "/dev/loop1" successfully created</pre>}}

This recreates the physical volume metadata, but not the missing logical volume or volume group data on the physical volume.

{{RootCmd|vgcfgrestore -f /etc/lvm/backup/vg0 vg0|output=<pre>
  Restored volume group vg0
</pre>}}

This now reconstructs all the missing metadata on the physical volume, including the logical volume and volume group data. However it doesn't restore the data, so the mirror is out of sync.

{{RootCmd|vgchange -ay vg0|output=<pre>
  device-mapper: reload ioctl on  failed: Invalid argument
  1 logical volume(s) in volume group "vg0" now active
</pre>}}

{{RootCmd|lvchange --resync vg0/lvm_raid1|output=<pre>
Do you really want to deactivate logical volume lvm_raid1 to resync it? [y/n]: y
</pre>}}

This will resync the mirror. This works with RAID 4,5 and 6 as well.

=== Деактивация логического тома ===

Деактивировать логический том можно с помощью следующей команды:

{{RootCmd|umount /dev/vg0/lvol1
|lvchange -a n /dev/vg0/lvol1}}

Нельзя смонтировать где-либо логический том, пока он не будет реактивирован:

{{RootCmd|lvchange -a y /dev/vg0/lvol1}}

== Внешние источники ==

* [http://sourceware.org/lvm2/ LVM2 sourceware.org]
* [http://tldp.org/HOWTO/LVM-HOWTO/ LVM tldp.org]
* [http://sources.redhat.com/lvm2/wiki/ LVM2 Wiki redhat.com]


[[Category:Core system]]
