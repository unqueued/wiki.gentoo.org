<languages />

{{Metadata|abstract=LVM permet aux administrateurs de créer des méta-périphériques qui fournissent une couche d'abstraction entre un système de fichiers et le stockage physique sous-jacent.}}

{{InfoBox stack
|{{InfoBox wikipedia|Logical Volume Manager (Linux)|header=true}}
}}

'''LVM''' ('''L'''ogical '''V'''olume '''M'''anager - Gestionnaire de volumes logiques) permet aux administrateurs de créer des méta-périphériques qui fournissent une couche d'abstraction entre un système de fichiers et le stockage physique sous-jacent. Les méta-périphériques (sur lesquels sont placés les systèmes de fichiers) sont des ''volumes logiques'', qui utilisent de l'espace de stockages tiré de pools de stockage appelés ''groupes de volumes''. Un groupe de volumes dispose d'un ou plusieurs ''volumes physiques'' qui représentent les véritables périphériques sur lesquels les données sont stockées.

Les volumes physiques peuvent être des partitions, des disques durs SATA entiers regroupés sous formes de  JBOD  ('''J'''ust a '''B'''unch '''O'''f '''D'''isks - Un simple groupe de disques), des systèmes RAID , iSCSI, un canal de fibre, eSATA etc.

== Installation ==

La configuration de LVM est gérée à la fois par des pilotes au niveau du noyau et des applications de l'espace utilisateur.

=== Noyau ===

Activez les options suivantes du noyau:

{{Kernel||<pre>
Device Drivers  --->
   Multiple devices driver support (RAID and LVM)  --->
       <*> Device mapper support
           <*> Crypt target support
           <*> Snapshot target
           <*> Mirror target
       <*> Multipath target
           <*> I/O Path Selector based on the number of in-flight I/Os
           <*> I/O Path Selector based on the service time
</pre>}}

{{Note|Tout ne doit pas être activé; quelques unes des options ne sont nécessaires  que pour  les [[#LVM2_Snapshots_and_LVM2_Thin_Snapshots|Instantanés LVM2  et les Instantanés  LVM2 minces]], [[#LVM2_Mirrors|LVM2 Mirrors]], [[#LVM2_RAID_0.2FStripeset|LVM2 RAID 0/Stripeset]]  et le cryptage.}}

=== Logiciel ===

Installez le paquet {{Package|sys-fs/lvm2}}:

{{USEflag|package=sys-fs/lvm2
|clvm
|cman
|lvm1+yes
|readline+yes
|selinux++no
|static
|static-libs
|thin+yes
|udev+yes
}}

{{Emerge|lvm2}}

== Configuration ==

La configuration de  LVM se fait à plusieurs niveaux:
# Gestion de LV, PV et VG via l'utilitaire de gestion
# Peaufinage du sous-système LVM via les fichiers de configuration
# Gestion du service au niveau de la distribution
# configuration via un disque virtuel de démarrage

La gestion des volumes physiques et logiques,  tout autant que celles des groupes de volumes est traitée dans le chapitre  [[#Usage|Usage]].

=== Fichiers de configuration de LVM  ===

LVM dispose d'un fichier de configuration très étendu, soit {{Path|/etc/lvm/lvm.conf}}. La plupart des utilisateurs n'ont pas besoin de modifier les paramètres de ce fichier pour commencer à utiliser LVM.

=== Gestion du service ===

Gentoo fournit le service LVM pour détecter automatiquement et activer les groupes de volumes et les volumes logiques.

Le service peut être géré via le système ''init''.

==== openrc ====

Pour démarre LVM à la main :

{{RootCmd|/etc/init.d/lvm start}}

Pour démarrer LVM au démarrage de la machine.

{{RootCmd|rc-update add lvm boot}}

==== systemd ====

Pour démarrer lvm à la main

{{RootCmd|systemctl start lvm2-monitor.service}}

Pour démarrer LVM au démarrage de la machine.

{{RootCmd|systemctl enable lvm2-monitor.service}}

=== Utilisation de  LVM dans un disque virtuel de démarrage -- initramfs ===

La plupart des chargeurs d'amorçage ne peuvent pas démarrer depuis LVM directement - Ni GRUB patrimonial, ni LILO ne le peuvent. GRUB2 peut amorcer depuis un volume logique linéaire, un volume logique miroir et possiblement depuis quelques volume logiques RAID. Aucun chargeur d'amorçage ne prend actuellement en charge les volumes logiques minces. 

Pour cette raison, il est conseillé d'utiliser une partition de démarrage (boot) non LVM et de monter la racine LVM depuis un système de fichiers virtuel de démarrage (initramfs). Un tel système de fichiers virtuel peut être généré automatiquement via [[Genkernel|genkernel]], genkernel-next et [[dracut]]:

*'''genkernel''' ne peut pas amorcer depuis tous les types sauf depuis les volumes minces (puisqu'il ne compile/construit ni ne copie les binaires des outils thin-provisionning (founiture mince) de l'hôte de compilation/construction) et peut-être RAID10 (la prise en charge de RAID10 nécessite LVM2 2.02.98, mais genkernel compile 2.02.89, néanmoins si des binaires statiques sont disponibles, il peut les copier)
*'''genkernel-next''' ne peut pas amorcer depuis tous les types volumes, mais a besoin d'un paquet app-misc/pax-utils suffisamment récent sous peine de voir les binaires ''thin'' (minces) cassés (voir {{Bug|482504}}) 
*'''dracut''' devrait amorcer tous les types, mais n'inclut que la prise en charge ''thin'' dans le système de fichiers virtuel de démarrage (initramfs) si l'hôte sur lequel on l'exécute à une racine ''thin'' (mince).

==== Genkernel/Genkernel-next ====

Installez le paquet {{Package|sys-kernel/genkernel}} ou le paquet {{Package|sys-kernel/genkernel-next}}. L'option ''static'' de la variable  USE doit aussi être activée sur le paquet  {{Package|sys-fs/lvm2}} de façon à ce que genkernel utilise les binaires système (autrement il construirait sa propre copie privée). L'exemple suivant construit seulement un système de fichiers virtuel de démarrage (pas un noyau entier) et active la prise en charge de LVM.

{{RootCmd|genkernel --lvm initramfs}}

La page de manuel de genkernel présente d'autres options qui dépendent des besoins du système.

Le disque virtuel de démarrage (initrd) demande certains paramètres pour savoir démarrer lvm. Ils lui sont fournis de la même manière que les autres paramètres du noyau. Par exemple :

{{File|/etc/default/grub|Ajouter dolvm en tant que paramètre de démarrage du noyau|<pre>
GRUB_CMDLINE_LINUX="dolvm"
</pre>}}

==== Dracut ====

Le paquet {{Package|sys-kernel/dracut}}, qui a été porté depuis le projet RedHat, tient lieu d'outil similaire pour générer un système de fichiers virtuel de démarrage.
 Comme il est actuellement sous  ~arch pour test, les utilisateurs doivent[[Knowledge_Base:Accepting_a_keyword_for_a_single_package| l'accepter explicitement]] (via  {{Path|/etc/portage/package.accept_keywords}}) avant de l'installer.  Avant cela, l'otpion  <code>DRACUT_MODULES="lvm"</code> doit être déclarée dans  {{Path|/etc/portage/make.conf}}.  D'autres modules peuvent être requis. Reportez-vous à [[Dracut]]. En général, la commande suivante devrait générer un système virtuel de fichiers de démarrage utilisable (initramfs).

{{RootCmd|dracut -a lvm}}

Le disque virtuel de démarrage requiert quelques paramètres pour le renseigner sur l'endroit où démarrer lvm. Ces derniers sont transmis de la même façon que les autres paramètres du noyau. Par exemple :

{{File|/etc/default/grub|Ajouter la prise en charge de  LVM aux paramètres de démarrage du noyau|<pre>
GRUB_CMDLINE_LINUX="rd.lvm.vg=vol00"
</pre>}}

Pour une liste exhaustive des options de lvm avec dracut, reportez-vous à la section concernée du [https://www.kernel.org/pub/linux/utils/boot/dracut/dracut.html#_lvm  manuel de dracut].

== Utilisation ==

LVM organise le stockage sur 3 niveaux différents comme suit :
* disques durs, partitions, systèmes RAID systems ou autres moyens de stockage sont initialisés en tant que volumes physiques (PVs)
* les volumes physiques ('''P'''hysical '''V'''olumes) sont regroupés en groupe de volumes ('''V'''olumes '''G'''roups)
* les volumes logiques ('''L'''ogical '''V'''olumes) sont gérés en groupes de volumes (VG)

=== Volumes physiques ('''P'''ysical '''V'''olumes) ===
Les volumes physiques sont les systèmes physiques réels sur lesquels LVM est construit.

==== Partitionnement ====

{{Note|L'utilisation de partitions séparées pour allouer de l'espace de stockage aux groupes de volumes n'est nécessaire qui si on ne désire pas utiliser le disque entier pour un unique groupe de volumes LVM. Si le disque entier peut être utilisé, sautez cette étape et initialisez le disque dur tout entier en tant que volume physique. }}

Le code du type de partition pour  ''LVM'' est''8e'' (Linux LVM).

Par exemple, pour définir le type, via <code>fdisk</code>, d'une partition  {{Path|/dev/sda}}:

{{RootCmd|fdisk /dev/sda}}

Dans <code>fdisk</code>, ajoutez une partition en tapant  '''n''' (nouvelle) et changez le type de la partition en tapant  '''t''' (type) et en saisissant  ''8e''.

==== Créer un volume physique ====

Les volumes physiques peuvent être créé et initialisé avec la commande <code>pvcreate</code>.

Par exemple, la commande suivante crée un volume physique sur la première partition de {{Path|/dev/sda}} et {{Path|/dev/sdb}}:

{{RootCmd|pvcreate /dev/sd[ab]1}}

==== Lister les volumes physiques ====

La commande <code>pvdisplay</code> retourne une vue d'ensemble de tous les volumes physiques du système.

{{RootCmd|pvdisplay|output=<pre>
 --- Physical volume ---
  PV Name               /dev/sda1
  VG Name               volgrp
  PV Size               160.01 GiB / not usable 2.31 MiB
  Allocatable           yes 
  PE Size               4.00 MiB
  Total PE              40962
  Free PE               4098
  Allocated PE          36864
  PV UUID               3WHAz3-dh4r-RJ0E-5o6T-9Dbs-4xLe-inVwcV
  
 --- Physical volume ---
  PV Name               /dev/sdb1
  VG Name               volgrp
  PV Size               160.01 GiB / not usable 2.31 MiB
  Allocatable           yes 
  PE Size               4.00 MiB
  Total PE              40962
  Free PE               40962
  Allocated PE          0
  PV UUID               b031x0-6rej-BcBu-bE2C-eCXG-jObu-0Boo0x
</pre>}}

Si plus de volumes physiques devraient être affichés, alors al commande <code>pvscan</code> est capable de détecter les volumes physiques inactifs et peut les activer.

{{RootCmd|pvscan|output=<pre>
  PV /dev/sda1  VG volgrp        lvm2 [160.01 GiB / 16.01 GiB free]
  PV /dev/sdb1  VG volgrp        lvm2 [160.01 GiB / 160.01 GiB free]
  Total: 2 [320.02 GB] / in use: 2 [320.02 GiB] / in no VG: 0 [0]
</pre>}}

==== Retirer un volume physique ====

LVM répartit les données automatiquement sur tous les volumes physiques (sauf si on lui demande de procéder différemment) mais dans une démarche linéaire. Si un volume logique requis (dans un groupe de volumes) est plus petit que le volume de l'espace libre sur un unique volume physique, alors tout l'espace de ce volume physique (unique) est alloué à ce volume logique d'une manière contigüe. Ceci vise à optimiser la performance.

S'il est nécessaire de retirer un volume physique d'un groupe de volumes, les données doivent d'abord être déplacées en dehors de ce volume physique. La commande  <code>pvmove</code> permet de déplacer toutes les données d'un volume physique à un autre à l'intérieur d'un même groupe de volumes.

{{RootCmd|pvmove -v /dev/sda1}}

Une telle opération peut prendre beaucoup de temps selon l'importance volumique des données à déplacer. Une fois terminé, il ne devrait rester aucune donnée sur ce périphérique. Vérifiez que le volume physique n'est plus utilisé par un volume logique avec la commande <code>pvdisplay</code>.

L'étape suivante consiste à retirer le volume physique du groupe de volumes. La commande  <code>vgreduce</code> est là pour cela. Après l'avoir utilisée, le volume peut être détaché du groupe de volumes par la commande  <code>pvremove</code>:

{{RootCmd|vgreduce vg0 /dev/sda1 && pvremove /dev/sda1}}

=== Groupe de volumes ('''V'''olum '''G'''roup)  ===

Un groupe de volumes (VG) regroupe un certain nombre de volumes physiques et se présente comme {{Path|/dev/VG_NAME}} dans le système de fichiers. Le nom du groupe de volumes est choisi par l'administrateur.

==== Créer un groupe de volumes ====

La commande suivante crée un groupe de volume appelé ''vg0'' en lui assignant deux volumes physiques {{Path|/dev/sda1}} et {{Path|/dev/sdb1}}.

{{RootCmd|vgcreate vg0 /dev/sd[ab]1}}

==== Lister les groupes de volumes ====

La commande <code>vgdisplay</code> retourne la liste des groupes de volumes actifs:

{{RootCmd|vgdisplay|output=<pre>
  --- Volume group ---
  VG Name               vg0
  System ID             
  Format                lvm2
  Metadata Areas        1
  Metadata Sequence No  8
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                6
  Open LV               6
  Max PV                0
  Cur PV                1
  Act PV                1
  VG Size               320.02 GiB
  PE Size               4.00 MiB
  Total PE              81924
  Alloc PE / Size       36864 / 144.00 GiB
  Free  PE / Size       45056 /176.01 GiB
  VG UUID               mFPXj3-DdPi-7YJ5-9WKy-KA5Y-Vd4S-Lycxq3
</pre>}}

Si des groupes de volumes n'apparaissent pas, utilisez la commande  <code>vgscan</code> pour les localiser :

{{RootCmd|vgscan|output=<pre>
  Reading all physical volumes.  This may take a while...
  (lecture de tous les volumes physiques. Ceci peut prendre un moment...)
  Found volume group "vg0" using metadata type lvm2
  (Groupe de volumes trouvé ''vg0'' qui utilise la méta-donnée de type lvm2)
</pre>}}

==== Étendre un groupe de volumes ====

Les groupes de volumes regroupent des volumes physiques, permettant ainsi aux administrateurs d'utiliser un pool de stockage pour allouer de l'espace aux systèmes de fichiers. Lorsqu'un groupe de volumes manque de ressources physiques, il est nécessaire de l'étendre en lui ajoutant de nouveaux volumes physiques.

La commande suivante étend le groupe de volumes ''vg0'' en lui ajoutant le volume physique {{Path|/dev/sdc1}}:

{{RootCmd|vgextend vg0 /dev/sdc1}}

N'oubliez pas que le volume physique doit d'abord être initialisé en tant que tel !

==== Réduire un groupe de volumes ====

Si des volumes physiques doivent être retiré d'un groupe de volumes, toutes les données encore utilisées de ce volume physique doivent être déplacées vers d'autres volumes physiques à l'intérieur du groupe de volumes. Comme nous l'avons vu précédemment, on utilise pour cela la commande <code>pvmove</code>. Après cela, le volume physique peut être retiré du groupe en utilisant la commande <code>vgreduce</code> :

{{RootCmd|pvmove -v /dev/sdc1
|vgreduce vg0 /dev/sdc1}}

==== Retirer un groupe de volumes ====

Si un groupe de volumes n'est plus nécessaire (ou en d'autres termes, le pool de stockage qu'il représente n'est plus utilisé et que les volumes physiques qui le composent peuvent être libérés pour une autre utilisation), il peut être retiré avec la commande <code>vgremove</code>. Ceci ne fonctionne que si aucun volume logique n'est défini pour ce groupe de volumes, et que si tous les volumes physiques, à l'exception d'un seul, ont été retirés de ce pool.

{{RootCmd|vgremove vg0}}

=== Volume logique (Logical Volume) ===

Les volumes logiques sont les méta-périphériques finaux mis à la disposition du système, ordinairement pour y placer des systèmes de fichiers. Ils sont créés et gérés en groupes de volumes et apparaissent sous la forme {{Path|/dev/VG_NAME/LV_NAME}}. Comme avec les groupes de volumes, le nom utilisé pour désigner le volume est fixé par l'administrateur.

==== Créer un volume logique  ====

La commande  <code>lvcreate</code> crée un volume logique. Les paramètres passés à la commande sont la taille requise (qui ne peut excéder la taille de l'espace libre dans le groupe de volume), le groupe de volumes à qui l'espace est réclamé et le nom du volume logique à créer.

Dans l'exemple qui suit, un volume logique nommé ''lvol1'' est créé à partir du groupe de volumes nommé ''vg0'' avec une taille de 150MB.

{{RootCmd|lvcreate -L 150M -n lvol1 vg0}}

Il est possible de spécifier à la commande <code>lvcreate</code> d'utiliser tout l'espace libre d'un groupe de volumes. On utilise pour cela le paramètre ''-l'' qui spécifie la taille en nombre d' ''extents'' plutôt qu'une taille (interprétable par un humain).  Les volumes logiques sont éclatés en ''logical extents'' qui représentent des blocs de données dans un groupe de volumes.  Tous les ''extents'' dans un groupe de volumes possèdent la même taille. Via le paramètre ''-l'', on spécifie à la  commande  <code>lvcreate</code>  d'allouer tous les ''extents'' libres.

{{RootCmd|lvcreate -l 100%FREE -n lvol1 vg0}}

Next to ''FREE'' the ''VG'' key can be used to denote the entire size of a volume group.

==== Lister les volumes logiques  ====

Pour lister tous les groupes de volumes, utilisez la commande <code>lvdisplay</code>.

{{RootCmd|lvdisplay}}

Si des volumes logiques n'apparaissent pas, on peut utiliser  la commande <code>lvscan</code> pour détecter tous les volumes logiques dans tous les groupes de volumes disponibles.

{{RootCmd|lvscan}}

==== Étendre un volume logique ====

Lorsqu'un volume logique doit être étendu, on peut utiliser la commande <code>lvextend</code>  pour étendre l'espace alloué à ce volume.

Par exemple pour augment la taille du volume logique 'lvol1'' jusqu'à 500 MO :

{{RootCmd|lvextend -L500M /dev/vg0/lvol1}}

On peut aussi utiliser la taille à ajouter plutôt que de spécifier la taille totale.

{{RootCmd|lvextend -L+350MB /dev/vg0/lvol1}}

Un groupe de volumes ne met pas immédiatement un espace de stockage additionnel à la disposition de l'utilisateur. Pour cela, le système de fichiers qui occupe ce groupe de volumes doit être élargi lui aussi. Tous les systèmes de fichiers ne permettent pas le redimensionnement en ligne de commande. Vérifiez la documentation de votre système de fichier pour plus d'information.

À titre d'exemple, pour redimensionner un système de fichiers ext4 pour lui donner une taille de 500 MO :

{{RootCmd|resize2fs /mnt/data 500M}}

==== Réduire un volume logique ====

Si un volume logique doit être réduit en taille, commencez par réduire l'étendue du système de fichier. Tous les systèmes de fichiers ne peuvent être redimensionné système en marche.

Par exemple, ext4 ne peut être redimensionner avec un système qui tourne dessus, il faut d'abord le ''démonter''. Il est également recommandé de vérifier le système de fichiers afin d'être certain qu'il ne comporte pas d'incohérence.

{{RootCmd|umount /mnt/data
|e2fsck -f /dev/vg0/lvol1
|resize2fs /dev/vg0/lvol1 150M}}

 Une fois le système de fichiers réduit, il est possible de réduire le volume logique à son tour.

{{RootCmd|lvreduce -L150M /dev/vg0/lvol1}}

==== LV Permissions ====

LVM supports permission states on the logical volumes.

For instance, a logical volume can be set to ''read only'' using the <code>lvchange</code> command:

{{RootCmd|lvchange -p r /dev/vg0/lvol1
|mount -o remount /dev/vg0/lvol1}}

The remount is needed as the change is not enforced immediately.

To mark the logical volume as writable again, use the ''rw'' permission bit:

{{RootCmd|lvchange -p rw /dev/vg0/lvol1 && mount -o remount /dev/vg0/lvol1}}

==== Remove LV ====

Before removing a logical volume, make sure it is no longer mounted:

{{RootCmd|umount /dev/vg0/lvol1}}

Deactivate the logical volume so that no further write activity can take place:

{{RootCmd|lvchange -a n /dev/vg0/lvol1}}

With the volume unmounted and deactivated, it can now be removed, freeing the extents allocated to it for use by other logical volumes in the volume group:

{{RootCmd|lvremove /dev/vg0/lvol1}}

== Features ==

LVM provides quite a few interesting features for storage administrators, including (but not limited to)
* thin provisioning (over-committing storage)
* snapshot support
* volume types with different storage allocation methods

=== Thin provisioning ===

Recent versions of LVM2 (2.02.89) support "thin" volumes. Thin volumes are to block devices what sparse files are to file systems. Thus, a thin logical volume within a pool can be "over-committed": its presented size can be larger than the allocated size - it can even be larger than the pool itself. Just like a sparse file, the extents are allocated as the block device gets populated. If the file system has ''discard'' support extents are freed again as files are removed, reducing space utilization of the pool.

Within LVM, such a thin pool is a special type of logical volume, which itself can host logical volumes.

==== Creating a thin pool ====

{{Warning|If an overflow occurs within the thin pool metadata, then the pool will be corrupted. '''LVM cannot recover from this'''.}} 

{{Note|If the thin pool gets exhausted, any process that would cause the thin pool to allocate more (unavailable) extents will be stuck in "killable sleep" state until either the thin pool is extended or the process recieves SIGKILL.}}

Each thin pool has metadata associated with it, which is added to the thin pool size. LVM will compute the size of the metadata based on the size of the thin pool as the minimum of <tt>pool_chunks * 64 bytes</tt> or 2MiB, whichever is larger. The administrator can select a different metadata size as well.

To create a thin pool, add the ''--type thin-pool --thinpool thin_pool'' parameters to <code>lvcreate</code>:

{{RootCmd|lvcreate -L 150M --type thin-pool --thinpool thin_pool vg0}}

The above example creates a thin pool called ''thin_pool'' with a total size of 150 MB. This is the real allocated size for the thin pool (and thus the total amount of actual storage that can be used).

To explicitly ask for a certain metadata size, use the ''--metadatasize'' parameter:

{{RootCmd|lvcreate -L 150M --metadatasize 2M --type thin-pool --thinpool thin_pool vg0}}

Due to the metadata that is added to the thin pool, the intuitive way of using all available size in a volume group for a logical volume does not work (see LVM bug [https://bugzilla.redhat.com/show_bug.cgi?id=812726|812726]):

{{RootCmd|lvcreate -l 100%FREE --type thin-pool --thinpool thin_pool vg0|output=<pre>
Insufficient suitable allocatable extents for logical volume thin_pool: 549 more required
</pre>}}

Note the thin pool does not have an associated device node like other LV's.

==== Creating a thin logical volume ====

A ''thin logical volume'' is a logical volume inside the thin pool (which itself is a logical volume). As thin logical volumes are ''sparse'', a virtual size instead of a physical size is specified using the ''-V'' parameter:

{{RootCmd|lvcreate -T vg0/thin_pool -V 300M -n lvol1}}

In this example, the (thin) logical volume ''lvol1'' is exposed as a 300MB-sized device, even though the underlying pool only holds 150MB of real allocated storage.

It is also possible to create both the thin pool as well as the logical volume inside the thin pool in one command:

{{RootCmd|lvcreate -T vg0/thin_pool -V 300M -L150M -n lvol1}}

==== Listing thin pools and thin logical volumes ====

Thin pools and thin logical volumes are special types of logical volumes, and as such as displayed through the <code>lvdisplay</code> command. The <code>lvscan</code> command will also detect these logical volumes.

==== Extending a thin pool ====

{{Warning|As of LVM2 2.02.89, the metadata size of the thin pool cannot be expanded, it is fixed at creation}}

The thin pool is expanded like a non-thin logical volume using <code>lvextend</code>. For instance:

{{RootCmd|lvextend -L500M vg0/thin_pool}}

==== Extending a thin logical volume ====

A thin logical volume is expanded just like a regular one:

{{RootCmd|lvextend -L1G vg0/lvol1}}

Note that the <code>lvextend</code> command uses the ''-L'' option (or ''-l'' if extent counts are used) and not a "virtual size" option as was used during the creation. 

==== Reducing a thin pool ====

Currently, LVM cannot reduce the size of the thin pool. See LVM bug [https://bugzilla.redhat.com/show_bug.cgi?id=812731|812731].

==== Reducing a thin logical volume ====

Thin logical volumes are reduced just like regular logical volumes.

For instance:
{{RootCmd|lvreduce -L300M vg0/lvol1l}}

Note that the <code>lvreduce</code> command uses the ''-L'' option (or ''-l'' if extent counts are used) and not a "virtual size" option as was used during the creation.

==== Removing thin pools ====

Thin pools cannot be removed until all the thin logical volumes inside it are removed.

When a thin pool no longer services any thin logical volume, it can be removed through the <code>lvremove</code> command:

{{RootCmd|lvremove vg0/thin_pool}}

=== LVM2 snapshots and thin snapshots ===

A snapshot is a logical volume that acts as copy of another logical volume. It displays the state of the original logical volume at the time of snapshot creation.

==== Creating a snapshot logical volume ====

A snapshot logical volume is created using the ''-s'' option to <code>lvcreate</code>. Snapshot logical volumes are still given allocated storage as LVM "registers" all changes made to the original logical volume and stores these changes in the allocated storage for the snapshot. When querying the snapshot state, LVM will start from the original logical volume and then check all changes registered, "undoing" the changes before showing the result to the user.

A snapshot logical volume henceforth "growths" at the rate that changes are made on the original logical volume. When the allocated storage for the snapshot is completely used, then the snapshot will be removed automatically from the system.

{{RootCmd|lvcreate -l 10%VG -s -n 20140412_lvol1 /dev/vg0/lvol1}}

The above example creates a snapshot logical volume called ''20140412_lvol1'', based on the logical volume ''lvol1'' in volume group ''vg0''. It uses 10% of the space (extents actually) allocated to the volume group.

==== Accessing a snapshot logical volume ====

Snapshot logical volumes can be mounted like regular logical volumes. They are even not restricted to read-only operations - it is possible to modify snapshots and thus use it for things such as testing changes before doing these on a "production" file system.

As long as snapshot logical volumes exist, the regular/original logical volume cannot be reduced in size or removed.

==== LVM thin snapshots ====

{{Note|A thin snapshot can only be taken on a thin pool for a thin logical volume. The thin device mapper target supports thin snapshots of read-only non-thin logical volumes, but the LVM2 tooling does not support this. However, it is possible to create a regular (non-thin) snapshot logical volume of a thin logical volume.}}

To create a thin snapshot, the <code>lvcreate</code> command is used with the <code>-s</code> option. No size declaration needs to be passed on:

{{RootCmd|lvcreate -s -n 20140413_lvol1 /dev/vg0/lvol1}}

Thin logical volume snapshots have the same size as their original thin logical volume, and use a physical allocation of 0 just like all other thin logical volumes. 

{{Important|If ''-l'' or ''-L'' is specified, a snapshot will still be created, but the resulting snapshot will be a regular snapshot, not a thin snapshot.}}

It is also possible to take snapshots of snapshots:

{{RootCmd|lvcreate -s -n 1_20140413_lvol1 /dev/vg0/20140413_lvol1}}

Thin snapshots have several advantages over regular snapshots. First, thin snapshots are independent of their original logical volume once created. The original logical volume can be shrunk or deleted without affecting the snapshot. Second, thin snapshots can be efficiently created recursively (snapshots of snapshots) without the "chaining" overhead of regular recursive LVM snapshots.

==== Rolling back to snapshot state ====

To rollback the logical volume to the version of the snapshot, use the following command:

{{RootCmd|lvconvert --merge /dev/vg0/20140413_lvol1}}

This might take a couple of minutes, depending on the size of the volume.

{{Important|The snapshot will disappear and this change is not revertible}}

==== Rolling back thin snapshots ====

For thin volumes, <code>lvconvert --merge</code> does not work. Instead, delete the original logical volume and rename the snapshot:

{{RootCmd|umount /dev/vg0/lvol1
|lvremove /dev/vg0/lvol1
|lvrename vg0/20140413_lvol1 lvol1}}

=== Different storage allocation methods ===

LVM supports different allocation methods for storage:
* linear volumes (which is the default)
* mirrored volumes (in a more-or-less active/standby setup)
* striping (RAID0)
* mirrored volumes (RAID1 - which is more an active/active setup)
* striping with parity (RAID4 and RAID5)
* striping with double parity (RAID6)
* striping and mirroring (RAID10)

==== Linear volumes ====

Linear volumes are the most common kind of LVM volumes. LVM will attempt to allocate the logical volume to be as physically contiguous as possible. If there is a physical volume large enough to hold the entire logical volume, then LVM will allocate it there, otherwise it will split it up into as few pieces as possible.

The commands introduced earlier on to create volume groups and logical volumes create linear volumes.

Because linear volumes have no special requirements, they are the easiest to manipulate and can be resized and relocated at will. If a logical volume is allocated across multiple physical volumes, and any of the physical volumes become unavailable, then that logical volume cannot be started anymore and will be unusable.

==== Mirrored volumes ====

LVM supports ''mirrored'' volumes, which provide fault tolerance in the event of drive failure. Unlike RAID1, there is no performance benefit - all reads and writes are delivered to a single side of the mirror.

To keep track of the mirror state, LVM requires a ''log'' to be kept. It is recommended (and often even mandatory) to position this log on a physical volume that does not contain any of the mirrored logical volumes. There are three kind of logs that can be used for mirrors:

# '''Disk''' is the default log type. All changes made are logged into extra metadata extents, which LVM manages. If a device fails, then the changes are kept in the log until the mirror can be restored again.
# '''Mirror''' logs are '''disk''' logs that are themselves mirrored. 
# '''Core''' mirror logs record the state of the mirror in memory only. LVM will have to rebuild the mirror every time it is activated. This type is useful for temporary mirrors.

To create a logical volume with a single mirror, pass the ''-m 1'' argument (to select standard mirroring) with optionally ''--mirrorlog'' to select a particular log type:

{{RootCmd|lvcreate -m 1 --mirrorlog mirror -l 40%VG --nosync -n lvol1 vg0}}

The <tt>-m 1</tt> tells LVM to create one (additional) mirror, so requiring 2 physical volumes. The <tt>--nosync</tt> option is an optimization - without it LVM will try synchronize the mirror by copying empty sectors from one logical volume to another.

It is possible to create a mirror of an existing logical volume:

{{RootCmd|lvconvert -m 1 -b vg0/lvol1}}

The ''-b'' option does the conversion in the background as this can take quite a while.

To remove a mirror, set the number of mirrors (back) to 0:

{{RootCmd|lvconvert -m0 vg0/lvol1}}

If part of the mirror is unavailable (usually because the disk containing the physical volume has failed), the volume group will need to be brought up in degraded mode:

{{RootCmd|vgchange -ay --partial vg0}}

On the first write, LVM will notice the mirror is broken. The default policy ("remove") is to automatically reduce/break the mirror according to the number of pieces available. A 3-way mirror with a missing physical volume will be reduced to 2-way mirror; a 2-way mirror will be reduced to a regular linear volume. If the failure is only transient, and the missing physical volume returns after LVM has broken the mirror, the mirrored logical volume will need to be recreated on it. 

To recover the mirror, the failed physical volume needs to be removed from the volume group, and a replacement physical volume needs to be added (or if the volume group has a free physical volume, it can be created on that one). Then the mirror can be recreated with <tt>lvconvert</tt> at which point the old physical volume can be removed from the volume group:

{{RootCmd|vgextend vg0 /dev/sdc1
|lvconvert -b -m 1 --mirrorlog disk vg0/lvol1
|vgreduce --removemissing vg0}}

It is possible to have LVM recreate the mirror with free extents on a different physical volume if one side fails. To accomplish that, set <code>mirror_image_fault_policy</code> to ''allocate'' in {{Path|lvm.conf}}.

==== Thin mirrors ====

It is not (yet) possible to create a mirrored thin pool or thin volume. It is possible to create a mirrored thin pool my creating a normal mirrored logical volume and then converting the logical volume to a thin pool with <tt>lvconvert</tt>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will merge them into a single logical volume.

{{Warning|LVM 2.02.98 or above is required for this to work properly. Prior versions are either not capable or will segfault and corrupt the volume group. Also, conversion of a mirror into a thin pool '''destroys''' all existing data in the mirror!}}

{{RootCmd|lvcreate -m 1 --mirrorlog mirrored -l40%VG -n thin_pool vg0
|lvcreate -m 1 --mirrorlog mirrored -L4MB -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg0/thin_meta}}

==== Striping (RAID0) ====

Instead of a linear volume, where multiple contiguous physical volumes are appended, it possible to create a ''striped'' or ''RAID 0'' volume for better performance. This will alternate storage allocations across the available physical volumes.

To create a striped volume over three physical volumes:

{{RootCmd|lvcreate -i 3 -l 20%VG -n lvol1_stripe vg0|output=<pre>
Using default stripesize 64.00 KiB
</pre>}}

The -i option indicates over how many physical volumes the striping should be done.

It is possible to mirror a stripe set. The -i and -m options can be combined to create a striped mirror:

{{RootCmd|lvcreate -i 2 -m 1 -l 10%VG vg0}}

This creates a 2 physical volume stripe set and mirrors it on 2 different physical volumes, for a total of 4 physical volumes. An existing stripe set can be mirrored with <tt>lvconvert</tt>.

A thin pool can be striped like any other logical volume. All the thin volumes created from the pool inherit that settings - do not specify it manually when creating a thin volume.

It is not possible to stripe an existing volume, nor reshape the stripes across more/less physical volumes, nor to convert to a different RAID level/linear volume. A stripe set can be mirrored. It is possible to extend a stripe set across additional physical volumes, but they must be added in multiples of the original stripe set (which will effectively linearly append a new stripe set).

==== Mirroring (RAID1) ====

Unlike RAID 0, which is striping, RAID 1 is mirroring, but implemented differently than the original LVM mirror. Under RAID1, reads are spread out across physical volumes, improving performance. RAID1 mirror failures do not cause I/O to block because LVM does not need to break it on write.

Any place where an LVM mirror could be used, a RAID 1 mirror can be used in its place. It is possible to have LVM create RAID1 mirrors instead of regular mirrors implicitly by setting <tt>mirror_segtype_default</tt> to ''raid1'' in {{Path|lvm.conf}}.

To create a logical volume with a single mirror:

{{RootCmd|lvcreate -m 1 --type raid1 -l 40%VG --nosync -n lvm_raid1 vg0}}

Note the difference for creating a mirror: There is no ''mirrorlog'' specified, because RAID1 logical volumes do not have an explicit mirror log - it built-in to the logical volume.

It is possible to convert an existing logical volume to RAID 1:

{{RootCmd|lvconvert -m 1 --type raid1 -b vg0/lvol1}}

To remove a RAID 1 mirror, set the number of mirrors to 0:

{{RootCmd|lvconvert -m0 vg0/lvm_raid1}}

If part of the RAID1 is unavailable (usually because the disk containing the physical volume has failed), the volume group will need to be brought up in degraded mode:

{{RootCmd|vgchange -ay --partial vg0}}

Unlike an LVM mirror, writing does NOT breaking the mirroring. If the failure is only transient, and the missing physical volume returns, LVM will resync the mirror by copying cover the out-of-date segments instead of the entire logical volume. If the failure is permanent, then the failed physical volume needs to be removed from the volume group, and a replacement physical volume needs to be added (or if the volume group has a free physical volume, it can be created on a different PV). The mirror can then be repaired with ''lvconvert'', and the old physical volume can be removed from the volume group:

{{RootCmd|vgextend vg0 /dev/sdc1
|lvconvert --repair -b vg0/lvm_raid1
|vgreduce --removemissing vg0}}

==== Thin RAID1 ====

It is not (yet) possible to create a RAID 1 thin pool or thin volume. It is possible to create a RAID 1 thin pool by creating a normal mirrored logical volume and then converting the logical volume to a thin pool with <tt>lvconvert</tt>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will then merge them into a single logical volume.

{{Warning|LVM 2.02.98 or above is required for this to work properly. Prior versions are either not capable or will segfault and corrupt the VG. Also, conversion of a RAID 1 into a thin pool '''destroys''' all existing data in the mirror!}}

{{RootCmd|lvcreate -m 1 --type raid1 -l40%VG -n thin_pool vg0
|lvcreate -m 1 --type raid1 -L4MB -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg00/thin_meta}}

==== Striping with parity (RAID4 and RAID5) ====

{{Note|Striping with parity requires at least 3 physical volumes.}}

RAID 0 is not fault-tolerant - if any of the physical volumes fail then the logical volume is unusable. By adding a parity stripe to RAID 0 the logical volume can still function if a physical volume is missing. A new physical volume can then be added to restore fault tolerance.

Stripsets with parity come in 2 flavors: RAID 4 and RAID 5. Under RAID 4, all the parity stripes are stored on the same physical volume. This can become a bottleneck because all writes hit that physical volume, and it gets worse the more physical volumes are in the array. With RAID 5, the parity data is distributed evenly across the physical volumes so none of them become a bottleneck. For that reason, RAID 4 is rare and is considered obsolete/historical. In practice, all stripesets with parity are RAID 5.

{{RootCmd|lvcreate --type raid5 -l 20%VG -i 2 -n lvm_raid5 vg0}}

Only the data physical volumes are specified with -i, LVM adds one to it automatically for the parity. So for a 3 physical volume RAID5, ''-i 2'' is passed on and not ''-i 3''.

When a physical volume fails, then the volume group will need to be brought up in degraded mode:

{{RootCmd|vgchange -ay --partial vg0}}

The volume will work normally at this point, however this degrades the array to RAID 0 until a replacement physical volume is added. Performance is unlikely to be affected while the array is degraded - although it does need to recompute its missing data via parity, it only requires simple XOR for the parity block with the remaining data. The overhead is negligible compared to the disk I/O.

To repair the RAID5:

{{RootCmd|lvconvert --repair vg0/lvm_raid5
|vgreduce --removemissing vg0}}

It is possible to replace a still working physical volume in RAID5 as well:

{{RootCmd|lvconvert --replace /dev/sdb1 vg0/lvm_raid5
|vgreduce vg0 /dev/sdb1}}

The same restrictions of stripe sets apply to stripe sets with parity as well: it is not possible to enable striping with parity on an existing volume, nor reshape the stripes with parity across more/less physical volumes, nor to convert to a different RAID level/linear volume. A stripe set with parity can be mirrored. It is possible to extend a stripe set with parity across additional physical volumes, but they must be added in multiples of the original stripe set with parity (which will effectively linearly append a new stripe set with parity).

==== Thin RAID5 logical volumes ====

It is not (yet) possible to create stripe set with parity (RAID5) thin pools or thin logical volumes. It is possible to create a RAID5 thin pool by creating a normal RAID5 logical volume and then converting the logical volume into a thin pool with <tt>lvconvert</tt>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will merge them into a single logical volume.

{{Warning|LVM 2.02.98 or above is required for this to work properly. Prior versions are either not capable or will segfault and corrupt the VG. Also, coversion of a RAID5 LV into a thin pool '''destroys''' all existing data in the LV!}}

{{RootCmd|lvcreate --type raid5 -i 2  -l20%VG -n thin_pool vg0
|lvcreate --type raid5 -i 2 -L4MB -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg00/thin_meta}}

==== Striping with double parity (RAID6) ====

{{Note|RAID6 requires at least 5 physical volumes.}}

RAID 6 is similar to RAID 5, however RAID 6 can survive up to '''two''' physical volume failures, thus offering more fault tolerance than RAID5 at the expense of extra physical volumes. 

{{RootCmd|lvcreate --type raid6 -l 20%VG -i 3 -n lvm_raid6 vg00}}

Like raid5, the -i option is used to specify the number of physical volumes to stripe, excluding the 2 physical volumes for parity. So for a 5 physical volume RAID6, pass on ''-i 3'' and not ''-i 5''.

Recovery for RAID6 is the same as RAID5.

{{Note|Unlike RAID5 where parity block is cheap to recompute vs disk I/O, this is only half true in RAID6. RAID6 uses 2 parity stripes: One stripe is computed the same way as RAID5 (simple XOR). The second parity stripe is much harder to compute - see [https://www.kernel.org/pub/linux/kernel/people/hpa/raid6.pdf|raid6 (pdf)] for more information.}}

==== Thin RAID6 logical volumes ====

It is not (yet) possible to create a RAID6 thin pool or thin volumes. It is possible to create a RAID6 thin pool by creating a normal RAID6 logical volume and then converting the logical volume into a thin pool with <tt>lvconvert</tt>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will merge them into a single logical volume.

{{Warning|LVM 2.02.98 or above is required for this to work properly. Prior versions are either not capable or will segfault and corrupt the VG. Also, conversion of a RAID6 LV into a thin pool '''destroys''' all existing data in the LV!}}

{{RootCmd|lvcreate --type raid6 -i 2  -l20%VG -n thin_pool vg0
|lvcreate --type raid6 -i 2 -L4MB -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg0/thin_meta}}

==== LVM RAID10 ====

{{Note|RAID10 requires at least 4 physical volumes. Also LVM syntax requires the number of physical volumes be multiple of the numbers stripes and mirror, even though RAID10 format does not}}

RAID10 is a combination of RAID0 and RAID1. It is more powerful than RAID0+RAID1 as the mirroring is done at the stripe level instead of the logical volume level, and therefore the layout doesn't need to be symmetric. A RAID10 volume can tolerate at least a single missing physical volume, and possibly more.

{{Note|LVM currently limits RAID10 to a single mirror.}}

{{RootCmd|lvcreate --type raid10 -l 1020 -i 2 -m 1 --nosync -n lvm_raid10 vg0}}

Both the ''-i and -m'' options are specified: ''-i'' is the number of stripes and ''-m'' is the number of mirrors. Two stripes and 1 mirror requires 4 physical volumes.

==== Thin RAID10 ====

It is not (yet) possible to create a RAID10 thin pool or thin volumes. It is possible to create a RAID10 thin pool by creating a normal RAID10 logical volume and then converting the logical volume into a thin pool with <code>lvconvert</code>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will merge them into a single logical volume.
 
{{Warning|Conversion of a RAID10 logical volume into a thin pool '''destroys''' all existing data in the logical volume!}}

{{RootCmd|lvcreate -i 2 -m 1 --type raid10 -l 1012 -n thin_pool vg0
|lvcreate -i 2 -m 1 --type raid10 -l 6 -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg0/thin_meta}}

== Experimenting with LVM ==

It is possible to experiment with LVM without using real storage devices. To accomplish this, loopback devices are created.

First make sure to have the loopback module loaded. 

{{RootCmd|modprobe -r loop && modprobe loop max_part{{=}}63}}

{{Note|If loopback support is built into the kernel, then use <code>loop.max_part{{=}}63</code> as boot option.}}

Next configure LVM to not use [[udev]] to scan for devices:

{{File|/etc/lvm/lvm.conf|Disabling udev in LVM config|<pre>
obtain_device_list_from_udev = 0
</pre>}}

{{Important|This is for testing only, make sure to change the setting back when dealing with real devices since it is much faster to use udev!}}

Create some image files which will become the storage devices. The next example uses five files for a total of about ~10GB of real hard drive space:

{{RootCmd|mkdir /var/lib/lvm_img
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm0.img bs{{=}}1024 seek{{=}}2097152
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm1.img bs{{=}}1024 seek{{=}}2097152
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm2.img bs{{=}}1024 seek{{=}}2097152
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm3.img bs{{=}}1024 seek{{=}}2097152
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm4.img bs{{=}}1024 seek{{=}}2097152}}

Check which loopback devices are available:

{{RootCmd|losetup -a}}

Assuming all loopback devices are available, next create the devices:

{{RootCmd|losetup /dev/loop0 /var/lib/lvm_img/lvm0.img
|losetup /dev/loop1 /var/lib/lvm_img/lvm1.img
|losetup /dev/loop2 /var/lib/lvm_img/lvm2.img
|losetup /dev/loop3 /var/lib/lvm_img/lvm3.img
|losetup /dev/loop4 /var/lib/lvm_img/lvm4.img}}

The {{Path|/dev/loop[0-4]}} devices are now available to use as any other hard drive in the system (and thus be perfect for physical volumes).

{{Note|On the next reboot, all the loopback devices will be released and the folder {{Path|/var/lib/lvm_img}} can be deleted.}}

== Troubleshooting ==

LVM has a few features that already provide some level of redundancy. However, there are situations where it is possible to restore lost physical volumes or logical volumes.

=== vgcfgrestore utility ===

By default, on any change to a LVM physical volume, volume group, or logical volume, LVM2 create a backup file of the metadata in {{Path|/etc/lvm/archive}}. These files can be used to recover from an accidental change (like deleting the wrong logical volume). LVM also keeps a backup copy of the most recent metadata in {{Path|/etc/lvm/backup}}. These can be used to restore metadata to a replacement disk, or repair corrupted metadata.

To see what states of the volume group are available to be restored (partial output to improve readability):

{{RootCmd|vgcfgrestore --list vg00|output=<pre>
  File:		/etc/lvm/archive/vg0_00042-302371184.vg
  VG name:    	vg0
  Description:	Created *before* executing 'lvremove vg0/lvm_raid1'
  Backup Time:	Sat Jul 13 01:41:32 201
</pre>}}

==== Recovering an accidentally deleted logical volume ====

Assuming the logical volume ''lvm_raid1'' was accidentally removed from volume group ''vg0'', it is possible to recover it as follows:

{{RootCmd|vgcfgrestore -f /etc/lvm/archive/vg0_00042-302371184.vg vg0}}

{{Important|<tt>vgcfgrestore</tt> only restores LVM metadata, ''not'' the data inside the logical volume. However <code>pvremove</code>, <code>vgremove</code>, and <code>lvremove</code> only wipe metadata, leaving any data intact. If <code>issue_discards</code> is set in {{Path|/etc/lvm/lvm.conf}} though, then these command ''are'' destructive to data.}}

==== Replacing a failed physical volume ====

It possible to do a true "replace" and recreate the metadata on the new physical volume to be the same as the old physical volume:

{{RootCmd|vgdisplay --partial --verbose|output=<pre>
  --- Physical volumes ---
  PV Name               /dev/loop0     
  PV UUID               iLdp2U-GX3X-W2PY-aSlX-AVE9-7zVC-Cjr5VU
  PV Status             allocatable
  Total PE / Free PE    511 / 102
  
  PV Name               unknown device     
  PV UUID               T7bUjc-PYoO-bMqI-53vh-uxOV-xHYv-0VejBY
  PV Status             allocatable
  Total PE / Free PE    511 / 102
</pre>}}

The important line here is the UUID "unknown device". 

{{RootCmd|pvcreate --uuid T7bUjc-PYoO-bMqI-53vh-uxOV-xHYv-0VejBY --restorefile /etc/lvm/backup/vg0 /dev/loop1|output=<pre>
  Couldn't find device with uuid T7bUjc-PYoO-bMqI-53vh-uxOV-xHYv-0VejBY.
  Physical volume "/dev/loop1" successfully created</pre>}}

This recreates the physical volume metadata, but not the missing logical volume or volume group data on the physical volume.

{{RootCmd|vgcfgrestore -f /etc/lvm/backup/vg0 vg0|output=<pre>
  Restored volume group vg0
</pre>}}

This now reconstructs all the missing metadata on the physical volume, including the logical volume and volume group data. However it doesn't restore the data, so the mirror is out of sync.

{{RootCmd|vgchange -ay vg0|output=<pre>
  device-mapper: reload ioctl on  failed: Invalid argument
  1 logical volume(s) in volume group "vg0" now active
</pre>}}

{{RootCmd|lvchange --resync vg0/lvm_raid1|output=<pre>
Do you really want to deactivate logical volume lvm_raid1 to resync it? [y/n]: y
</pre>}}

This will resync the mirror. This works with RAID 4,5 and 6 as well. 

=== Deactivating a logical volume ===

It is possible to deactivate a logical volume with the following command:

{{RootCmd|umount /dev/vg0/lvol1
|lvchange -a n /dev/vg0/lvol1}}

It is not possible to mount the logical volume anywhere before it gets reactivated:

{{RootCmd|lvchange -a y /dev/vg0/lvol1}}

== External resources ==

* [http://sourceware.org/lvm2/ LVM2 sourceware.org]
* [http://tldp.org/HOWTO/LVM-HOWTO/ LVM tldp.org]
* [http://sources.redhat.com/lvm2/wiki/ LVM2 Wiki redhat.com]


[[Category:Core system]]
