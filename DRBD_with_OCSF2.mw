
= Introduction =
This is a guide for DRBR with OCFS2.<br />
The tough came with found that samba Active Directory (AD) Domain Controller (DC) sysvol is not being replicated between DC as current Samba4 known limitation.<br />
{{Warning|
Drop the sysvol though, bad idea. without CTDB, files would corrupted with 2 access. But CTDB is not compatible with Samba AD.}}

Q: Why DRBD only is not enough...<br />
There might be case where there are multiple write access to the sysvol by multiple DC member, thus without cluster files system we might be looking for trouble since the DRBD volume are being mount by 2 or more DC member, so OCFS2 is selected for it performance and also popularity.<br />

You can use always other Cluster files system.<br />

{{Warning|
Samba Team don't suggest to use this solution, so this is '''''purely experimental'''''. But still DRBD + OCFS2 example are everywhere we can make use of that.<br />
Please wait for Samba official replication solution and don't use this in any production server.<br />
I wouldn't held any responsible about your lost... But still you are welcome to TRY :)<br />
This example can only support Max 2 DC + 1 RODC. because of the limitation of DRBD.<br />
}}

= Prerequisite =
Below are the kernel need.
{{kernel|3.10.25-gentoo|<pre>
File systems --->   
    [*] OCFS2 file system support
    [*]   O2CB Kernelspace Clustering
    [*]   OCFS2 statistics
    [*]   OCFS2 logging support
    [ ]   OCFS2 expensive checks
    <*> Distributed Lock Manager (DLM)
Device Drivers  --->
    [*] Block devices  --->
        [*]   DRBD Distributed Replicated Block Device support
        [ ]     DRBD fault injection
</pre>}}

= Packages =
Please emerge the following package 
{{Emerge|sys-cluster/drbd}}
{{USEflag
|package=sys-cluster/drbd
|bash-completion+yes
|udev+yes
|heartbeat++
|pacemaker++
|xen++
}}

Check more on [[DRBD]]

DRBD 8.4 guide do suggest the use of heartbeat and also pacemaker. But I think that was just too complicated...<br />
You are welcome to try and add on an option with heartbeat and peacemaker. 

{{Emerge|sys-fs/ocfs2-tools|}}
{{USEflag
|package=sys-fs/ocfs2-tools
|debug
|external
|gtk
}}

= DRBD Configuration =
We will now configure DRBD.
{{Note|DRBD version check and prompt are a bit annoying as it will suggest you to upgrade every time you execute a command.}}

Please check the DRBD User’s Guide Ver 8.4<ref name="drbdManual">[http://www.drbd.org/users-guide/drbd-users-guide.html The DRBD User’s Guide Ver 8.4], The DRBD User’s Guide</ref> on their website it is well written with example.


This is a Global DRBD configuration.
It will need to change according to your DRBD need.
Please check the on the manual for more info.

{{File|/etc/drdb.d/global_common.conf||<pre>
global {
        usage-count yes;
        # minor-count dialog-refresh disable-ip-verification
}

common {
        handlers {
                pri-on-incon-degr "/usr/lib64/drbd/notify-pri-on-incon-degr.sh; /usr/lib64/drbd/notify-emergency-reboot.sh; echo b > /proc/sysrq-trigger ; reboot -f";
                pri-lost-after-sb "/usr/lib64/drbd/notify-pri-lost-after-sb.sh; /usr/lib64/drbd/notify-emergency-reboot.sh; echo b > /proc/sysrq-trigger ; reboot -f";
                local-io-error "/usr/lib64/drbd/notify-io-error.sh; /usr/lib64/drbd/notify-emergency-shutdown.sh; echo o > /proc/sysrq-trigger ; halt -f";
                # fence-peer "/usr/lib64/drbd/crm-fence-peer.sh";
                # split-brain "/usr/lib64/drbd/notify-split-brain.sh root";
                out-of-sync "/usr/lib64/drbd/notify-out-of-sync.sh root";
                # before-resync-target "/usr/lib64/drbd/snapshot-resync-target-lvm.sh -p 15 -- -c 16k";
                # after-resync-target /usr/lib64/drbd/unsnapshot-resync-target-lvm.sh;
                outdate-peer "/sbin/kill-other-node.sh";
        }

        startup {
                # wfc-timeout degr-wfc-timeout outdated-wfc-timeout wait-after-sb
        }

        options {
                # cpu-mask on-no-data-accessible
        }

        disk {
                # size max-bio-bvecs on-io-error fencing disk-barrier disk-flushes
                # disk-drain md-flushes resync-rate resync-after al-extents
                # c-plan-ahead c-delay-target c-fill-target c-max-rate
                # c-min-rate disk-timeout
                fencing resource-and-stonith;
        }

        net {
                # protocol timeout max-epoch-size max-buffers unplug-watermark
                # connect-int ping-int sndbuf-size rcvbuf-size ko-count
                # allow-two-primaries cram-hmac-alg shared-secret after-sb-0pri
                # after-sb-1pri after-sb-2pri always-asbp rr-conflict
                # ping-timeout data-integrity-alg tcp-cork on-congestion
                # congestion-fill congestion-extents csums-alg verify-alg
                # use-rle
                protocol C;
                #allow-two-primaries yes; (Enable this after sync completed)
        }
}
</pre>}}
{{Note|DRBD manual don't recommended to set the allow-two-primaries option to yes until the initial resource synchronization has completed.}}

{{Warning|By configuring auto-recovery policies, you are configuring effectively configuring automatic data-loss! Be sure you understand the implications. - By DRBD Manual}}

Please create the files below.<br />
If you have multiple resource setup, you can do it one resource per files to avoid confusion.<br />

Below files is a 2 Node Resource example.

{{File|/etc/drdb.d/r0.res|Resource name r0 DRBD |<pre>
resource r0 {
        startup {
                become-primary-on both;
        }
        on serverNode01 {
                device    /dev/drbd1;
                disk      /dev/sda5;
                address   192.168.11.23:7789;
                meta-disk internal;
        }
        on serverNode02 {
                device    /dev/drbd1;
                disk      /dev/sda5;
                address   192.168.11.27:7789;
                meta-disk internal;
        }
        net {
                after-sb-0pri discard-zero-changes;
                after-sb-1pri discard-secondary;
                after-sb-2pri disconnect;
        }
}
</pre>}}

In plain English, we will use {{Path|/dev/sda5}} on serverNode01 (IP 192.168.11.23) and {{Path|/dev/sda5}} on serverNode02 (IP 192.168.11.27) to create a drbd node {{Path|/dev/drbd1}} respectively on both server. Both drbd node will store the metadata on internally.

{| class="wikitable"
|-
! Name !! Description
|-
| resource r0|| The DRBD resource named r0
|-
| on serverNode01<br />on serverNode01
 || The Server Name for each node for your reference.
|-
| device    /dev/drbd1; || The device node which will be created by BRDB that need to be mount.
|-
| disk      /dev/sda5; || The physical partition that DRBD will be using to store the information. 
|-
| address   192.168.11.23:7789;<br />address   192.168.11.27:7789;
 || The server ip address for DRBD will use to sync between all nodes, You will have to assign this ip to the interface.<br />
Also DRBD don't support multiple ip address. Use Channel Bonding/Teaming like LACP.<br />
|-
| meta-disk internal; || This tell DRBD where to store the metadata, it can be external as well. 
|}

= OCFS2 Configuration =
We will now continue with OCFS2 Setup.<br />
Unfortunately ocfs2 configuration don't include the cluster files example and thus you will need to copy the files below as an example.<br />
Please change what even which suit your needm

{{File|/etc/ocfs2/cluster.conf|Create this files|<pre>
cluster:
        node_count = 2
        name = sysvol

node:
        number = 1
        cluster = sysvol
        ip_port = 7777
        ip_address = 192.168.11.23
        name = serverNode01

node:
        number = 2
        cluster = sysvol
        ip_port = 7777
        ip_address = 192.168.11.27
        name = serverNode02
</pre>}}

{| class="wikitable"
|-
! Name !! Descriptions
|-
| cluster: || OCFS2 cluster declaration 
|-
| node_count = 2 || How many node do we have in this cluster?
|-
| name = sysvol || The ocfs2 cluster name
|-
| node: || The Node declaration
|-
| number = 1<br />number = 2
 || The node number, it must be different on all node 
|-
| cluster = sysvol || The name of the cluster
|-
| ip_port = 7777 || The ocfs2 node service port, you can change it you want to.
|-
| ip_address = 192.168.11.23<br />ip_address = 192.168.11.27|| The ocfs2 node ip address. Please change this to your interface ip
|-
| name = serverNode01<br />name = serverNode02 || The node servername for your reference.
|}

Now it is time to change {{Path|/etc/conf.d/ocfs2}} files.
There are a lot of configurable parameters there but we will only make one change.
<code>OCFS2_CLUSTER</code> and change it to ocfs2 cluster name.

{{File|/etc/conf.d/ocfs2|Change OCFS2 Cluster Name|<pre>
OCFS2_CLUSTER="sysvol"
</pre>}}

It is also time we add the required mount to {{Path|/etc/fstab}}. Please add the following.

{{File|/etc/fstab|add the following|<pre>
# Needed by ocfs2
none                    /sys/kernel/config      configfs        defaults                0 0
none                    /sys/kernel/dlm         ocfs2_dlmfs     defaults                0 0

# Our DRBD and OCSF2 mount
/dev/drbd1              /var/lib/samba/sysvol/  ocfs2           user_xattr,acl,noatime  0 0
</pre>}}

= DRBD Initialization and Setup =
After we have all this.<br />
It is time to initial DRBD

{{Note|Run this command on both node.}}
{{RootCmd|drbdadm create-md r0|output=<pre>
DRBD module version: 8.4.3
   userland version: 8.4.2
you should upgrade your drbd tools!
Writing meta data...
initializing activity log
NOT initializing bitmap
New drbd meta data block successfully created.
success
</pre>}}

Let us up this DRBD devices.
{{Note|Run this command on both node.}}
{{RootCmd|drbdadm up r0|output=<pre>
DRBD module version: 8.4.3
   userland version: 8.4.2
you should upgrade your drbd tools!
</pre>}}

When both node is up.
We can check on the BRDB status.
{{RootCmd|cat /proc/drbd|output=<pre>
version: 8.4.3 (api:1/proto:86-101)
built-in

 1: cs:WFConnection ro:Secondary/Unknown ds:Inconsistent/DUnknown C r----s
    ns:0 nr:0 dw:0 dr:0 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:2026376
</pre>}}

When Both node is up, we will see something...<br />
The '''Secondary/''Unknown''''' become '''Secondary/''Secondary'''''.
{{RootCmd|cat /proc/drbd|output=<pre>
version: 8.4.3 (api:1/proto:86-101)
built-in

 1: cs:Connected ro:Secondary/Secondary ds:Inconsistent/Inconsistent C r-----
    ns:0 nr:0 dw:0 dr:0 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:2026376
</pre>}}

We now have the DRBD device.
{{Note|
I do have question if we should format this partition to OCFS2 at this time then only sync it or not...<br />
Please try and let me know.
}}
Let start to synchronization both node BRDB by making it primary<br />
{{Note|
We should only run this command on one node only}}
{{RootCmd|drbdadm primary --force r0|output=<pre>
DRBD module version: 8.4.3
   userland version: 8.4.2
you should upgrade your drbd tools!
</pre>}}

Let us check the synchronization status
{{RootCmd|cat /proc/drbd|output=<pre>
version: 8.4.3 (api:1/proto:86-101)
built-in

 1: cs:SyncSource ro:Primary/Secondary ds:UpToDate/Inconsistent C r---n-
    ns:313236 nr:0 dw:0 dr:315032 al:0 bm:19 lo:5 pe:2 ua:7 ap:0 ep:1 wo:f oos:1715080
        [==>.................] sync'ed: 15.6% (1715080/2026376)K
        finish: 0:00:44 speed: 38,912 (38,912) K/sec
</pre>}}

Please wait until it is done.
{{RootCmd|cat /proc/drbd|output=<pre>
version: 8.4.3 (api:1/proto:86-101)
built-in

 1: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r-----
    ns:2026376 nr:0 dw:0 dr:2027040 al:0 bm:124 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0
</pre>}}

Let make 2nd Node primary<br />
{{Note|
We should only run this command the 2nd node only}}
{{RootCmd|drbdadm primary --force r0|output=<pre>
DRBD module version: 8.4.3
   userland version: 8.4.2
you should upgrade your drbd tools!
</pre>}}

Let start the DRBD services
{{RootCmd
|/etc/init.d/drbd start
}}

Yes, Our DRBD cluster are done.
Now let continue to the OCFS2 Setup

= OCFS2 Setup =
The 1st things we need to do is to mount the required kernel config and dlm that needed by OCFS2.
Since that we already modified the /etc/fstab.
You can simple follow the command below.

{{RootCmd
|mount /sys/kernel/config
|mount /sys/kernel/dlm 
}}

We should now start the OCFS2 Cluster
{{RootCmd|/etc/init.d/ocfs2 start|output=<pre>
 * Starting OCFS2 cluster
 *  - sysvol ...    
</pre>}}

Now let us make the DRBD cluster run OCFS2 
{{RootCmd|mkfs.ocfs2 -N 2 -L "amtbsysvol" /dev/drbd1|output=<pre>
mkfs.ocfs2 1.8.2
Cluster stack: classic o2cb
Label: sysvol
Features: sparse extended-slotmap backup-super unwritten inline-data strict-journal-super xattr indexed-dirs refcount discontig-bg
Block size: 4096 (12 bits)
Cluster size: 4096 (12 bits)
Volume size: 2075009024 (506594 clusters) (506594 blocks)
Cluster groups: 16 (tail covers 22754 clusters, rest cover 32256 clusters)
Extent allocator size: 4194304 (1 groups)
Journal size: 67108864
Node slots: 2
Creating bitmaps: done
Initializing superblock: done
Writing system files: done
Writing superblock: done
Writing backup superblock: 1 block(s)
Formatting Journals: done
Growing extent allocator: done
Formatting slot map: done
Formatting quota files: done
Writing lost+found: done
mkfs.ocfs2 successful
</pre>}}

We can now mount it the OCFS2 cluster

{{RootCmd
|mount /var/lib/samba/sysvol/
}}


= Adding both DRBD and OCFS2 on system boot =
Let us start both DRBD and OCFS2 when system boot up.
{{RootCmd|rc-update add brdb}}
{{RootCmd|rd-update add ocfs2}}

= See also =
[[DRDB]]

= External Reference =
<references />

[[Category:Cluster]]
[[Category:Filesystems]]
