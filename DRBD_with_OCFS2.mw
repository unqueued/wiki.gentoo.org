
= Introduction =
This is a guide for DRBR with OCFS2.<br />
The tough came with found that samba Active Directory (AD) Domain Controller (DC) sysvol is not being replicated between DC as current Samba4 known limitation.<br />
{{Warning|
Drop the sysvol though, bad idea. without CTDB, files would corrupted with 2 access. But CTDB is not compatible with Samba AD.}}

Q: Why DRBD only is not enough...<br />
There might be case where there are multiple write access to the block device by multiple server, thus without cluster files system we will have trouble on files locking and etc since the DRBD volume are being mount by 2 or more server, so OCFS2 is selected for it performance and also popularity.<br />

You can use always other Cluster files system.<br />

This guide only 2 node, dual primary configuration. If you required more node, please check on drbd for more information.

{{Warning|
Samba Team don't suggest to use this solution, so this is '''''purely experimental'''''. But still DRBD + OCFS2 example are everywhere we can make use of that.<br />
}}

= Prerequisite =
Below are the kernel need.
{{kernel|3.10.25-gentoo|<pre>
File systems --->   
    [*] OCFS2 file system support
    [*]   O2CB Kernelspace Clustering
    [*]   OCFS2 statistics
    [*]   OCFS2 logging support
    [ ]   OCFS2 expensive checks
    <*> Distributed Lock Manager (DLM)
Device Drivers  --->
    [*] Block devices  --->
        [*]   DRBD Distributed Replicated Block Device support
        [ ]     DRBD fault injection
</pre>}}

= Packages =
Please emerge the following package 
{{Emerge|sys-cluster/drbd}}
{{USEflag
|package=sys-cluster/drbd
|bash-completion+yes
|udev+yes
|heartbeat++
|pacemaker++
|xen++
}}

Check more on [[DRBD]]

DRBD 8.4 guide do suggest the use of heartbeat and also pacemaker. But I think that was just too complicated...<br />
You are welcome to try and add on an option with heartbeat and peacemaker. 

{{Emerge|sys-fs/ocfs2-tools|}}
{{USEflag
|package=sys-fs/ocfs2-tools
|debug
|external
|gtk
}}

= DRBD Configuration =
We will now configure DRBD.
{{Note|DRBD version check and prompt are a bit annoying as it will suggest you to upgrade every time you execute a command.}}

Please check the DRBD User’s Guide Ver 8.4<ref name="drbdManual">[http://www.drbd.org/users-guide/drbd-users-guide.html The DRBD User’s Guide Ver 8.4], The DRBD User’s Guide</ref> on their website it is well written with example.


This is a Global DRBD configuration.
It will need to change according to your DRBD need.
Please check the on the manual for more info.

{{File|/etc/drdb.d/global_common.conf||<pre>
global {
        usage-count yes;
        # minor-count dialog-refresh disable-ip-verification
}

common {
        handlers {
                pri-on-incon-degr "/usr/lib64/drbd/notify-pri-on-incon-degr.sh; /usr/lib64/drbd/notify-emergency-reboot.sh; echo b > /proc/sysrq-trigger ; reboot -f";
                pri-lost-after-sb "/usr/lib64/drbd/notify-pri-lost-after-sb.sh; /usr/lib64/drbd/notify-emergency-reboot.sh; echo b > /proc/sysrq-trigger ; reboot -f";
                local-io-error "/usr/lib64/drbd/notify-io-error.sh; /usr/lib64/drbd/notify-emergency-shutdown.sh; echo o > /proc/sysrq-trigger ; halt -f";
                # fence-peer "/usr/lib64/drbd/crm-fence-peer.sh";
                # split-brain "/usr/lib64/drbd/notify-split-brain.sh root";
                out-of-sync "/usr/lib64/drbd/notify-out-of-sync.sh root";
                # before-resync-target "/usr/lib64/drbd/snapshot-resync-target-lvm.sh -p 15 -- -c 16k";
                # after-resync-target /usr/lib64/drbd/unsnapshot-resync-target-lvm.sh;
                outdate-peer "/sbin/kill-other-node.sh";
        }

        startup {
                # wfc-timeout degr-wfc-timeout outdated-wfc-timeout wait-after-sb
        }

        options {
                # cpu-mask on-no-data-accessible
        }

        disk {
                # size max-bio-bvecs on-io-error fencing disk-barrier disk-flushes
                # disk-drain md-flushes resync-rate resync-after al-extents
                # c-plan-ahead c-delay-target c-fill-target c-max-rate
                # c-min-rate disk-timeout
                fencing resource-and-stonith;
        }

        net {
                # protocol timeout max-epoch-size max-buffers unplug-watermark
                # connect-int ping-int sndbuf-size rcvbuf-size ko-count
                # allow-two-primaries cram-hmac-alg shared-secret after-sb-0pri
                # after-sb-1pri after-sb-2pri always-asbp rr-conflict
                # ping-timeout data-integrity-alg tcp-cork on-congestion
                # congestion-fill congestion-extents csums-alg verify-alg
                # use-rle
                # protocol C;
                # allow-two-primaries yes;
        }
}
</pre>}}
{{Note|DRBD manual don't recommended to set the allow-two-primaries option to yes until the initial resource synchronization has completed.}}

{{Warning|By configuring auto-recovery policies, you are configuring effectively configuring automatic data-loss! Be sure you understand the implications. - By DRBD Manual}}

Please create the files below.<br />
If you have multiple resource setup, you can do it one resource per files to avoid confusion.<br />

Below files is a 2 Node Resource example.
Note on the Comment Part, they will need to be uncomment at a much later part when we enable dual primary.
{{File|/etc/drdb.d/r0.res|Resource name r0 DRBD |<pre>
resource r0 {
        handlers {
                split-brain "/usr/lib64/drbd/notify-split-brain.sh root";
        }
        startup {
                # become-primary-on both;
        }
        on serverNode01 {
                device    /dev/drbd1;
                disk      /dev/sda5;
                address   192.168.11.23:7789;
                meta-disk internal;
        }
        on serverNode02 {
                device    /dev/drbd1;
                disk      /dev/sda5;
                address   192.168.11.27:7789;
                meta-disk internal;
        }
        net {
                after-sb-0pri discard-zero-changes;
                after-sb-1pri discard-secondary;
                after-sb-2pri disconnect;
                protocol C;
                # allow-two-primaries yes;
        }
}
</pre>}}

In plain English, we will use {{Path|/dev/sda5}} on serverNode01 (IP 192.168.11.23) and {{Path|/dev/sda5}} on serverNode02 (IP 192.168.11.27) to create a drbd node {{Path|/dev/drbd1}} respectively on both server. Both drbd node will store the metadata on internally.

{| class="wikitable"
|-
! Name !! Description
|-
| resource r0|| The DRBD resource named r0
|-
| on serverNode01<br />on serverNode01
 || The Server Name for each node for your reference.
|-
| device    /dev/drbd1; || The device node which will be created by BRDB that need to be mount.
|-
| disk      /dev/sda5; || The physical partition that DRBD will be using to store the information. 
|-
| address   192.168.11.23:7789;<br />address   192.168.11.27:7789;
 || The server ip address for DRBD will use to sync between all nodes, You will have to assign this ip to the interface.<br />
Also DRBD don't support multiple ip address. Use Channel Bonding/Teaming like LACP.<br />
|-
| meta-disk internal; || This tell DRBD where to store the metadata, it can be external as well. 
|}

= OCFS2 Configuration =
We will now continue with OCFS2 Setup.<br />
Unfortunately ocfs2 configuration don't include the cluster files example and thus you will need to copy the files below as an example.<br />
Please change what even which suit your needm

{{File|/etc/ocfs2/cluster.conf|Create this files|<pre>
cluster:
        node_count = 2
        name = ocfs2cluster

node:
        number = 1
        cluster = ocfs2cluster
        ip_port = 7777
        ip_address = 192.168.11.23
        name = serverNode01

node:
        number = 2
        cluster = ocfs2cluster
        ip_port = 7777
        ip_address = 192.168.11.27
        name = serverNode02
</pre>}}

{| class="wikitable"
|-
! Name !! Descriptions
|-
| cluster: || OCFS2 cluster declaration 
|-
| node_count = 2 || How many node do we have in this cluster?
|-
| name = ocfs2cluster|| The ocfs2 cluster name
|-
| node: || The Node declaration
|-
| number = 1<br />number = 2
 || The node number, it must be different on all node 
|-
| cluster = ocfs2cluster|| The name of the cluster
|-
| ip_port = 7777 || The ocfs2 node service port, you can change it you want to.
|-
| ip_address = 192.168.11.23<br />ip_address = 192.168.11.27|| The ocfs2 node ip address. Please change this to your interface ip
|-
| name = serverNode01<br />name = serverNode02 || The node servername for your reference.
|}

Now it is time to change {{Path|/etc/conf.d/ocfs2}} files.
There are a lot of configurable parameters there but we will only make one change.
<code>OCFS2_CLUSTER</code> and change it to ocfs2 cluster name.

{{File|/etc/conf.d/ocfs2|Change OCFS2 Cluster Name|<pre>
OCFS2_CLUSTER="ocfs2cluster"
</pre>}}

It is also time we add the required mount to {{Path|/etc/fstab}}. Please add the following.

{{File|/etc/fstab|add the following|<pre>
# Needed by ocfs2
none                    /sys/kernel/config            configfs        defaults                                  0 0
none                    /sys/kernel/dlm               ocfs2_dlmfs     defaults                                  0 0

# Our DRBD and OCFS2 mount
/dev/drbd1              /ocfs2cluster/                ocfs2            _netdev,nointr,user_xattr,acl            0 0
</pre>}}

= DRBD Initialization and Setup =
After we have all this.<br />
It is time to initial DRBD

{{Note|Run this command on both node.}}
{{RootCmd|drbdadm create-md r0|output=<pre>
DRBD module version: 8.4.3
   userland version: 8.4.2
you should upgrade your drbd tools!
Writing meta data...
initializing activity log
NOT initializing bitmap
New drbd meta data block successfully created.
success
</pre>}}

Let us up this DRBD devices.
{{Note|Run this command on both node.}}
{{RootCmd|drbdadm up r0|output=<pre>
DRBD module version: 8.4.3
   userland version: 8.4.2
you should upgrade your drbd tools!
</pre>}}

When both node is up.
We can check on the BRDB status.
{{RootCmd|cat /proc/drbd|output=<pre>
version: 8.4.3 (api:1/proto:86-101)
built-in

 1: cs:WFConnection ro:Secondary/Unknown ds:Inconsistent/DUnknown C r----s
    ns:0 nr:0 dw:0 dr:0 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:2026376
</pre>}}

When Both node is up, we will see something...<br />
The '''Secondary/''Unknown''''' become '''Secondary/''Secondary'''''.
{{RootCmd|cat /proc/drbd|output=<pre>
version: 8.4.3 (api:1/proto:86-101)
built-in

 1: cs:Connected ro:Secondary/Secondary ds:Inconsistent/Inconsistent C r-----
    ns:0 nr:0 dw:0 dr:0 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:2026376
</pre>}}

We now have the DRBD device.
{{Note|
I do have question if we should format this partition to OCFS2 at this time then only sync it or not...<br />
Please try and let me know.
}}
Let start to synchronization both node BRDB by making it primary<br />
{{Note|
We should only run this command on one node only}}
{{RootCmd|drbdadm primary --force r0|output=<pre>
DRBD module version: 8.4.3
   userland version: 8.4.2
you should upgrade your drbd tools!
</pre>}}

Let us check the synchronization status
{{RootCmd|cat /proc/drbd|output=<pre>
version: 8.4.3 (api:1/proto:86-101)
built-in

 1: cs:SyncSource ro:Primary/Secondary ds:UpToDate/Inconsistent C r---n-
    ns:313236 nr:0 dw:0 dr:315032 al:0 bm:19 lo:5 pe:2 ua:7 ap:0 ep:1 wo:f oos:1715080
        [==>.................] sync'ed: 15.6% (1715080/2026376)K
        finish: 0:00:44 speed: 38,912 (38,912) K/sec
</pre>}}

Please wait until it is done.
{{RootCmd|cat /proc/drbd|output=<pre>
version: 8.4.3 (api:1/proto:86-101)
built-in

 1: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r-----
    ns:2026376 nr:0 dw:0 dr:2027040 al:0 bm:124 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0
</pre>}}

Let make 2nd Node primary<br />
{{Note|
We should only run this command the 2nd node only}}
{{RootCmd|drbdadm primary --force r0|output=<pre>
DRBD module version: 8.4.3
   userland version: 8.4.2
you should upgrade your drbd tools!
</pre>}}

Let start the DRBD services
{{RootCmd
|/etc/init.d/drbd start
}}

Yes, Our DRBD cluster are done.
Now let continue to the OCFS2 Setup

= OCFS2 Setup =
The 1st things we need to do is to mount the required kernel config and dlm that needed by OCFS2.
Since that we already modified the {{Path|/etc/fstab}}.
You can simple follow the command below.

{{RootCmd
|mount /sys/kernel/config
|mount /sys/kernel/dlm 
}}

We should now start the OCFS2 Cluster
{{RootCmd|/etc/init.d/ocfs2 start|output=<pre>
 * Starting OCFS2 cluster
 *  - ocfs2cluster...    
</pre>}}

Now let us make the DRBD cluster run OCFS2 
{{Note|You should create more node than what you need on OCFS in case when more server need to access this OCFS2 cluster.}}
{{RootCmd|mkfs.ocfs2 -N 8 -L "ocfs2cluster" /dev/drbd1|output=<pre>
mkfs.ocfs2 1.8.2
Cluster stack: classic o2cb
Label: ocfs2cluster
Features: sparse extended-slotmap backup-super unwritten inline-data strict-journal-super xattr indexed-dirs refcount discontig-bg
Block size: 4096 (12 bits)
Cluster size: 4096 (12 bits)
Volume size: 2075009024 (506594 clusters) (506594 blocks)
Cluster groups: 16 (tail covers 22754 clusters, rest cover 32256 clusters)
Extent allocator size: 4194304 (1 groups)
Journal size: 67108864
Node slots: 8
Creating bitmaps: done
Initializing superblock: done
Writing system files: done
Writing superblock: done
Writing backup superblock: 1 block(s)
Formatting Journals: done
Growing extent allocator: done
Formatting slot map: done
Formatting quota files: done
Writing lost+found: done
mkfs.ocfs2 successful
</pre>}}

== Enable Dual-Primary Mode == 
To enable Dual-Primary mode is easy, just uncomment the resources files we have made on top.

{{File|/etc/drdb.d/r0.res|Resource name r0 DRBD Dual Primary |<pre>
resource r0 {
        handlers {
                split-brain "/usr/lib64/drbd/notify-split-brain.sh root";
        }
        startup {
                become-primary-on both;
        }
        on serverNode01 {
                device    /dev/drbd1;
                disk      /dev/sda5;
                address   192.168.11.23:7789;
                meta-disk internal;
        }
        on serverNode02 {
                device    /dev/drbd1;
                disk      /dev/sda5;
                address   192.168.11.27:7789;
                meta-disk internal;
        }
        net {
                after-sb-0pri discard-zero-changes;
                after-sb-1pri discard-secondary;
                after-sb-2pri disconnect;
                protocol C;
                allow-two-primaries yes;
        }
}
</pre>}}

Than time to make change on both node, in sequence primary then only secondary.
{{RootCmd
|drbdadm adjust r0
|drbdadm primary r0
}}

Check if drbd are now dual primary 
{{RootCmd|cat /proc/drbd|output=<pre>
version: 8.4.3 (api:1/proto:86-101)
built-in

 1: cs:Connected ro:Primary/Primary ds:UpToDate/UpToDate C r-----
    ns:0 nr:535088 dw:535088 dr:928 al:0 bm:131068 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0
</pre>}}
We can now mount it the OCFS2 cluster on both node.

{{RootCmd
|mount /ocfs2cluster/
}}

= Adding both DRBD and OCFS2 on system boot =
Let us start both DRBD and OCFS2 when system boot up.
{{RootCmd|rc-update add drbd}}
{{RootCmd|rd-update add ocfs2}}

= Manual Split Brain Recovery =
Summary from <ref name="ManualSplitBrinaRecovery">[http://www.drbd.org/users-guide/s-resolve-split-brain.html Manual Split Brain Recovery Ver 8.4]</ref>
Under certain condition, Split Brain will happen and cannot be recover automatically by system.
And we have to manually recovery the DRBD disk.

== Split Brain Victim, Choose a node which that the changes will be discarded  ==
We will disconnect this node, make it secondary and connect it back to the drbd array and we will discard all the different.
So that it will sync with the latest version from other node.

{{RootCmd
|drbdadm disconnect r0
|drbdadm secondary r0
|drbdadm connect --discard-my-data r0
}}

== Split Brain Survivor, Choose The node which we have the latest version of files  ==
We just connect this back to the drbd array so secondary will sync from here.

{{RootCmd
|drbdadm connect r0
}}

== Result ==
Sync should happen and we should have our array back.
What if that don't happen.

Troubleshooting and error recovery<ref name="TroubleshootingandErrorRecovery">[http://www.drbd.org/users-guide/ch-troubleshooting.html Troubleshooting and error recovery Ver 8.4]</ref>

= See also =
* [[DRBD]]

= External Reference =
<references />

[[Category:Cluster]]
[[Category:Filesystems]]
