{{WIP|author=Salahx}}
{{InfoBox stack|
{{InfoBox todo
|Figure out how to compute how large the metadata device should be give the origin size for the mirror/striped targets
|Complete the Targets section
|header=true}}
}}

Normally, users rarely use '''dmsetup''' directly. The '''dmsetup''' is a very low level, and difficult tool to use. LVM, mdtool or dmsetup is generally the preferred way to do it, as it takes care of saving the metadata and issuing the dmsetup commands for you. However, sometimes one want to deal with it directly: sometimes for recovery purposes, or because LVM doesn't yet support what you want.

{{Important|The device mapper, the the rest of the Linux block layer deals with things at the sector level. A sector defined as '''512 bytes''', regardless of the actual physical geometry the the block device}} 
{{Note|The article uses both IEC (1024-based) and Metric (1000-based). So 1GB is 1,000,000,000 bytes, but 1 GiB is 1,073,741,824 bytes}}

== Dmsetup commands ==
=== Create ===
The ''create'' command activates a new device mapper device. It appears in /dev/mapper. In addition, if the target has metadata, it reads it, or if this its first use, it initializes the metadata devices. Note the prior device mapper devices can be passed as paramters (if the target takes a device), thus it is possible to "stack" them. The syntax is:
<code>dmsetup create <new device name> --tables <start sector> <end sector> <target name> <target parmaters></code>
=== Remove ===
The ''remove'' command deactivates a device mapper device. It removes it from /dev/mapper. Syntax is
<code>dmsetup remove [-f] <device name></code>
Note is not possible to remove a device that's in use. The '''-f''' option may be passed the replace the target with one that fails all I/O, hopefully allowing the reference count to drop to 0.
=== Message ===
The ''message'' command send a message to the device. What message are supported depend on the target Syntax is:
<code>dmsetup message <sector number> <device name> <target message></code>
The <sector number> tends not to be used and is almost always 0.  
=== Suspend ===
The ''suspend''' command stops any NEW I/O. Existing I/O will still be completed. This can be used to quiesce a device. Syntax is:
<code>dmsetup message suspend <device name></code>
=== Resume ===
The '''resume''' command allows I/O to be submitted to a previously suspended device. Syntax is:
<code>dmsetup message suspend <device name></code>
== Device-mapper targets ==
=== Zero ===
See <tt>Documentation/device-mapper/zero.txt</tt> . This target has no target-specific parameters.  

The "zero" target create that functions similarly to /dev/zero: All reads return binary zero, and all writes are discarded> Normaly used in tests, but also useful in recovering linear and raid-type targets, when combined with the 'snapshot' target: a "zero" target of the same size as the missing piece(s) is created, a (writable) snapshot created (usually a loop device backed by a large sparse file, but it can be far smaller than the missing piece since it only has to the hold the changes). Then the snapshot can be mounted, fsck'd, or recover tools run against it. 

This creates a 1GB (1953125-sector) zero target:
{{RootCmd|<nowiki>dmsetup create 1gb-zero --table '0 1953125 zero'</nowiki>}}
=== Linear ===
{{Note|All the following examples for this target will use 4 1GB+4MiB (1961317-sector) disks as /dev/loop{0,1,2,3} }} 
See <tt>Documentation/device-mapper/linear.txt</tt> for paramters in usage. This target is the basic building block for the device mapper - it is used to both join and split (and often both at once) block device. For a simple identify mapping:
{{RootCmd|dmsetup create test-linear --table '0 1961317 linear /dev/loop0 0'}}
The 4 disks can be joined to together as one:
{{RootCmd|echo -e '0 1961317 linear /dev/loop0 0'\\n'1961317 1961317 linear /dev/loop1 0'\\n'3922634 1961317 linear /dev/loop2 0'\\n'5883951 1961317 linear /dev/loop3 0' {{!}} dmsetup create test-linear}}
Note the peculiar syntax on the join. The <tt>--table</tt> argument only allows single-line tables. Multi-line tables must be read from stdin. Also notice the ''logical_start_sector'' is not 0 in this case, as each device were appending need to start where the previous ends.
Its possible to split a disk, in this case into a 4 MiB (8192 sector) "small" and 1 GB "large" (1953125 sector) disks:
{{RootCmd
|dmsetup create test-linear-small --table '0 8192 linear /dev/loop0 0'
|dmsetup create test-linear-large --table '0 1953125 linear /dev/loop0 8192'}}
Note that in the second device, the offset is not 0, since it is desired to start 4 MiB (8192 sectors) in
Both joining an splitting can be combined:
{{RootCmd|echo -e '0 1953125 linear /dev/loop0 8192'\\n'1953125 1953125 linear /dev/loop1 8192'\\n'3906250 1953125 linear /dev/loop2 8192'\\n'5859375 1953125 linear /dev/loop3 8192' {{!}} dmsetup create test-linear}}
This creates a 4GB device using last 1GB of each disk.

=== Snapshot ===
=== Mirror and RAID1 ===
{{Note|All the following examples for this target will use 4 1GB+4MiB (1961317-sector) disks as /dev/loop{0,1}, split as follows:
{{RootCmd
|dmsetup create test-mirror-log0 --table '0 8192 linear /dev/loop0 0'
|dmsetup create test-mirror-log1 --table '0 8192 linear /dev/loop1 0'
|dmsetup create test-mirror-data0 --table '0 1953125 linear /dev/loop0 8192'
|dmsetup create test-mirror-data1 --table '0 1953125 linear /dev/loop1 8192'}}
}}
==== Mirror ====
There is no kernel documentation for the ''mirror'' target. Parameters obtained from Linux sources: <tt>drivers/md/dm-log.c</tt> and <tt>drivers/md/dm-raid1.c</tt>
{{Code||<starting_sector> <length> mirror <log_type> <#log_args> <log_arg1>...<log_argN> <#devs> <device_name_1> <offset_1>...<device name N> <offset N> <#features> <feature_1>...<feature_N>}}

For ''log_type'' there are 4 values with different arguments:
* '''core''' <region_size> [[no]sync] [block_on_error]
* '''disk''' <logdevice> <region_size> [[no]sync] [block_on_error] 

And the values of each argument:
* ''region_size'' is the region size of the mirror in sectors. It must be power of 2 and at least of a kernel page (for Intel x86/x64 processors, this is 4 KiB (8 sectors)  This is the granularity in which the mirror is kept to update. Its a tradeoff between increased metadata and wasted I/O. LVM uses a value of 512 KiB (1024 sectors).
* ''logdevice'' is the device in which to store the metadata, for the '''disk''' log types
* '''[no]sync''' is an optional argument. Default is '''sync'''. '''nosync''' skips the sync step, but any reads to unwritten regions to since the mirror was established are undefined. This is appropriate to use then the initial device is empty.  

And there is only 1 feature:
* '''handle_errors''' causes the mirror to respond to an error. Default is to ignore all errors. LVM enables this feature.

To create a mirror with in-memory log:
{{RootCmd|dmsetup create test-mirror --table '0 1953125 mirror core 1 1024 2 /dev/mapper/test-mirror-data0 0 /dev/mapper/test-mirror-data1 0 1 handle_errors'}}
Without a persistent log, the mirror will have to be recreated every time by copying the entire block device to the other "legs". To avoid this, the log may be stored on disk:
{{RootCmd|dmsetup create test-mirror --table '0 1953125 mirror disk 2 /dev/mapper/test-mirror-log0 1024 2 /dev/mapper/test-mirror-data0 0 /dev/mapper/test-mirror-data0 0 1 handle_errors'}}
Its possible to do LVM "--mirrorlog mirror" by creating 2 mirrors: a '''core''' mirror for the log device, and a '''disk''' mirror the data devices:
{{RootCmd
|dmsetup create test-mirror-log --table '0 8192 mirror core 1 1024 2 /dev/mapper/test-mirror-log0 0 /dev/mapper/test-mirror-log1 0 1 handle_errors'
|dmsetup create test-mirror --table '0 1953125 mirror disk 2 /dev/mapper/test-mirror-log 1024 2 /dev/mapper/test-mirror-data0 0 /dev/mapper/test-mirror-data1 0 1 handle_errors'}}
==== RAID1 ====
See <tt>Documentation/device-mapper/dm-raid.txt</tt>. Note that <chunk_size> is unused for RAID1, but a value is still required, therefore is value should be set to 0. There 2 other important, though optional, parameters: ''region_size'' and '''[no]sync'''. 

* ''region_size'' has the same meaning as it does in the ''mirror'' target. Unlike the ''mirror'' target. it has a default of 4 MiB (8192 sectors). LVM uses a region size of 512 KiB (1024 sectors).
* '''[no]sync''' has the same meaning as it does in the ''mirror'' target

To create a simple 1 GB raid1 with no metadata devices.
{{RootCmd|dmsetup create test-raid1 --table '0 1953125 raid raid1 3 0 region_size 1024 2 - /dev/mapper/test-mirror-data0 - /dev/mapper/test-mirror-data1'}}
Note that because there's no metadata device, the array must be re-mirrored each time it is created. So normally, a metadata device is desired. Each 
"leg" needs it own metadata device  If /dev/loop2 and /dev/loop3 are small metadata devices (4 MiB), then to create a 1G RAID1 would be:
{{RootCmd|dmsetup create test-raid1 --table '0 1953125 raid raid1 3 0 region_size 1024 2 /dev/mapper/test-mirror-log0 /dev/mapper/test-mirror-data0 /dev/mapper/test-mirror-log1 /dev/mapper/test-mirror-data1'}}

=== Striped (RAID 0) and RAID 4/5/6/10 ===
{{Note|All the following examples for this target will use 4 1GB+4MiB (1961317-sector) disks as /dev/loop{0,1}, split as follows:
{{RootCmd|for S in `seq 0 3`; do dmsetup create test-raid-metadata$S --table "0 8192 linear /dev/loop$S 0"; dmsetup create test-raid-data$S --table "0 1953125 linear /dev/loop$S 8192"; done}} }}
See <tt>Documentation/device-mapper/striped.txt</tt> and <tt>Documentation/device-mapper/dm-raid.txt</tt> for the parameters of this target. Three in particular are important:
* ''chunk_size'' is the size I/O (in sectors) before its "split" across the array  It must be both a power a two and a least a large as a kernel memory page (for x86/x64 processors, pages are 4 KiB, so <chunk size> must be at least 8.) LVM uses a default value of 64 KiB (128 sectors). Using LVM defaults, a 1 MiB (2048 sector) write will be split in 16 chunks, distributed as evenly as possible across the array. The size of the array MUST be a multiple of this value. Otherwise the target will give the error "Array size does not match requested target length". 
* ''region size'' has the same meaning and defaults as it does for the RAID1 target. 
* '''[no]sync''' has the same meaning as it does for the RAID1  target. It is usually not appropriate for RAID 4,5 and 6 as even for blank devices parity must still be computed, unless creating a degraded array. 
Because the number of sectors (1953125) is not a multiple of 128, it must be rounded down to the nearest multiple of 128 sectors, which can be done using this formula:
{{Code||echo 'scale{{=}}0; x{{=}}<dev_dize> / <chunk_size>; x*{{=}}<chunk_size>; x' {{!}} bc}}
So in this case:
{{Cmd|echo 'scale{{=}}0; x{{=}}1953125 / 128; x*{{=}}128; x' {{!}} bc|output=1953024}}
==== Striped (RAID0) ====
Stripe sets allow multiple disks to be combined into one with improved performance. The striped target parameters is asymmetric to the RAID ones. First, the # devices comes first, not the cluster size. Second, one must specify the offset (usually 0) of each device the makes up the stripe set.Because there are 4 disks of 1953024 sectors each, the total array size will be 7812096 sectors.
To create a stripe set (RAID0):
{{RootCmd|dmsetup create test-raid0 --table '0 7812096 striped 4 128 /dev/mapper/test-raid-data0 0 /dev/mapper/test-raid-data1 0 /dev/mapper/test-raid-data2 0 /dev/mapper/test-raid-data3 0'}}
==== RAID4  ====
RAID4 is striped set that can tolerate the failure of a single disk. Because RAID4 uses a dedicated parity disk, one disk is "unusable", therefore the total space is 3 disks * 1953024 sectors, or a total of 5859072 sectors. 
To create a RAID4 set with no metadata devices:
{{RootCmd|dmsetup create test-raid4 --table '0 5859072 raid raid4 3 64 region_size 1024 4 - /dev/mapper/test-raid-data0 - /dev/mapper/test-raid-data1 - /dev/mapper/test-raid-data2 - /dev/mapper/test-raid-data3'}}
As RAID1, because there are no metadata devices, the parity disk will have to be rebuilt every time it is assembled. To create a RAID4 WITH metadata devices: 
{{RootCmd|dmsetup create test-raid4 --table '0 5859072 raid raid4 3 64 region_size 1024 4 /dev/mapper/test-raid-metadata0 /dev/mapper/test-raid-data0 /dev/mapper/test-raid-metadata1 /dev/mapper/test-raid-data1 /dev/mapper/test-raid-metadata2 /dev/mapper/test-raid-data2 /dev/mapper/test-raid-metadata3 /dev/mapper/test-raid-data3'}}
It is possible to create a RAID4 in degraded mode initially. It is necessary to not specify any metadata devices, and "nosync" must added
{{RootCmd|dmsetup create test-raid4 --table '0 5859072 raid raid4 4 64 region_size 1024 nosync 4 - /dev/mapper/test-raid-data0 - /dev/mapper/test-raid-data1 - /dev/mapper/test-raid-data2 - -'}}
The reason for doing this is its faster to create a degraded array, populate it, tear it down, and then reassemble the array with the missing metadata devices and data device, so that the parity is only computed once, not twice.
==== RAID5  ====
RAID5 is similar to RAID4, except in RAID5 the parity data is distributed across the stripe set. There are 4 "flavors" of RAID5. For LVM, the default is '''raid5_ls'''. The amount of parity used is the same as RAID4, so the total space is 5859072 sectors.
To create a RAID5 set with no metadata devices:
{{RootCmd|dmsetup create test-raid5 --table '0 5859072 raid raid5_ls 3 64 region_size 1024 4 - /dev/mapper/test-raid-data0 - /dev/mapper/test-raid-data1 - /dev/mapper/test-raid-data2 - /dev/mapper/test-raid-data3'}}
To create a RAID5 with metadata: 
{{RootCmd|dmsetup create test-raid5 --table '0 5859072 raid raid5_ls 3 64 region_size 1024 4 /dev/mapper/test-raid-metadata0 /dev/mapper/test-raid-data0 /dev/mapper/test-raid-metadata1 /dev/mapper/test-raid-data1 /dev/mapper/test-raid-metadata2 /dev/mapper/test-raid-data2 /dev/mapper/test-raid-metadata3 /dev/mapper/test-raid-data3'}}
To create a degraded RAID5:
{{RootCmd|dmsetup create test-raid5 --table '0 5859072 raid raid5_ls 4 64 region_size 1024 nosync 4 - /dev/mapper/test-raid-data0 - /dev/mapper/test-raid-data1 - /dev/mapper/test-raid-data2 - -'}}
==== RAID6  ====
RAID6 is a stripe set that can tolerate the failure of up to 2 disks. Like RAID5, parity is distributed across the stripe set. There are 3 "flavors" of RAID 6. For LVM, the default is "raid6_zr". The total available space is 3906048 sectors.
To create a RAID6 set with no metadata devices:
{{RootCmd|dmsetup create test-raid6 --table '0 3906048 raid raid6_zr 3 64 region_size 1024 4 - /dev/mapper/test-raid-data0 - /dev/mapper/test-raid-data1 - /dev/mapper/test-raid-data2 - /dev/mapper/test-raid-data3'}}
To create a RAID6 with metadata: 
{{RootCmd|dmsetup create test-raid6 --table '0 3906048 raid raid6_zr 3 64 region_size 1024 4 /dev/mapper/test-raid-metadata0 /dev/mapper/test-raid-data0 /dev/mapper/test-raid-metadata1 /dev/mapper/test-raid-data1 /dev/mapper/test-raid-metadata2 /dev/mapper/test-raid-data2 /dev/mapper/test-raid-metadata3 /dev/mapper/test-raid-data3'}}
To create a degraded RAID6:
{{RootCmd|dmsetup create test-raid6 --table '0 3906048 raid raid6_zr 4 64 region_size 1024 nosync 4 - /dev/mapper/test-raid-data0 - /dev/mapper/test-raid-data1 - - - -'}}
Note 2 devices are left out instead of 1.
==== RAID10  ====
RAID10 combines mirroring (RAID 1) and striping (RAID10). Note is a better than stacking a RAID1 on top of RAID0 (or vice versa) - it is possible to do RAID10 on an odd number of disks. Half the disks are lost to the mirror, so the the total available space is 3906048 sectors.
To create a RAID10 set with no metadata devices:
{{RootCmd|dmsetup create test-raid10 --table '0 3906048 raid raid10 3 64 region_size 1024 4 - /dev/mapper/test-raid-data0 - /dev/mapper/test-raid-data1 - /dev/mapper/test-raid-data2 - /dev/mapper/test-raid-data3'}}
To create a RAID10 set with metadata:
{{RootCmd|dmsetup create test-raid10 --table '0 3906048 raid raid10 3 64 region_size 1024 4 /dev/mapper/test-raid-metadata0 /dev/mapper/test-raid-data0 /dev/mapper/test-raid-metadata1 /dev/mapper/test-raid-data1 /dev/mapper/test-raid-metadata2 /dev/mapper/test-raid-data2 /dev/mapper/test-raid-metadata3 /dev/mapper/test-raid-data3'}}
If all the devices are empty, the '''nosync''' may be used to skip the initial sync, with the same caveats as '''mirror''' target.

=== Crypt ===
=== Thin ===
=== Cache ===
{{Note|All the following examples for this target will 2 disks: /dev/loop0 will be 1G+12 MiB (1977701 sectors), and /dev/loop1 will be 200MB+12MiB (415201 sectors), split as follows:
{{RootCmd
|for S in `seq 0 1`; do dmsetup create test-cache-meta$S --table "0 16384 linear /dev/loop$S 0"; dmsetup create test-mirror-cache-meta$S --table "0 8192 linear /dev/loop$S 16384"; done
|dmsetup create test-cache-data0 --table '0 1953125 linear /dev/loop0 24576'
|dmsetup create test-cache-data1 --table '0 390625 linear /dev/loop1 24576'}}
}}
See <tt>Documentation/device-mapper/cache.txt</tt> and Documentation/device-mapper/cache-policies.txt for parameters and usage. This target is intended to speed up access to a slow but large rotational disk by using a faster but smaller SSD as a cache. 
There is one important parameter:
* ''block_size'' is the granularity of the cache. Data is promoted/demoted to/from the cache in blocks. It must be a multiple of 32k (64 sectors). LVM uses 64k (128 sectors) by default. 

The recommended metadata device size is <tt>8192 sectors + (nr_blocks/32) sectors</tt> where ''nr_blocks'' is the number of sectors on the "fast" device divided by the ''block_size''. For this device:
{{Cmd|echo '390625 / (128 * 32) + 8192  {{!}} bc|output=8287}}
Will round up to 8 MiB (16384 sectors) for safety, however 4 MiB (8192 sectors) would likely be more than enough anyway.

To create a cache device:
{{RootCmd|dmsetup create test-cache --table '0 1953024 cache /dev/mapper/test-cache-meta0 /dev/mapper/test-cache-data1 /dev/mapper/test-cache-data0 128 0 default 0'}}
Its recommended to mirror the metadata device on the origin and cache device. To do so:
{{RootCmd
|dmsetup create test-mirror-cache-meta --table '0 16384 raid raid1 1 0 2 /dev/mapper/test-mirror-cache-meta0 /dev/mapper/test-cache-meta0 /dev/mapper/test-mirror-cache-meta1 /dev/mapper/test-cache-meta1'
|dmsetup create test-cache --table '0 1953024 cache /dev/mapper/test-mirror-cache-meta /dev/mapper/test-cache-data1 /dev/mapper/test-cache-data0 128 0 default 0'}}

=== Era ===
=== Multipath ===
=== Debuggging targets ===
==== Delay ====
==== Flakey ====
==== Error ====
