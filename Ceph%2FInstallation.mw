All necessary Ceph software is available through the {{Package|sys-cluster/ceph}} package. It contains all services as well as basic administration utilities for managing a Ceph cluster.

== Design ==

Before embarking on a Ceph deployment scenario, take the time to make a basic Ceph cluster design. 

What is the purpose of the Ceph cluster? Is it to play around and experiment with Ceph? Is it to host all critical data in form of [[Ceph/Rados_Block_Device|rbd devices]]? Is it to create a highly available file server?

What features are needed on the Ceph cluster? How many monitors are likely to be needed? How much storage will be used, and how will this storage be represented (as in, how many OSDs will be available and where will they run)? Will the cluster provide S3- or Swift-like APIs to the outside world?

What are the IP addresses that will be used by the cluster? Ceph requires a static IP environment, so making a well designed network infrastructure is important for Ceph to function properly.

How will the servers be distributed across the environment? Ceph has a number of ''buckets'' that it can use to differentiate servers and make well-thought-through distribution and replication decisions. The default is an ''OSD'' on a ''host'' in a ''rack'' in a ''row'' in a ''room'' inside a ''data center''.

There are a number of best practices to account for through:

* Most clusters require 3 monitor servers, perhaps 5. Clusters generally do not need more than 5 monitor servers to function in even the harshest environments.
* Distribute the monitor servers across the environment. If the cluster is over a couple of racks, make sure that the monitor servers are distributed across the racks as well.
* There is usually no need for RAID on the file system that an OSD uses. Instead, rely on the Ceph availability and distribution.
* OSD services do not need a lot of CPU or RAM. A metadata server however does benefit from high-speed CPU and lots of memory.

== System configuration ==

The first configuration to decide on is which Ceph version to deploy. At the time of writing, Ceph version 0.87 ("Giant") is available in the tree in ~arch while version 0.80 ("Firefly") is available as stable release. To use the ~arch version, add {{Package|sys-cluster/ceph}} to {{Path|package.accept_keywords}}:

{{FileBox|filename=/etc/portage/package.accept_keywords/ceph|1=
sys-cluster/ceph
}}

Next, validate that the Linux kernel is configured to support Ceph.

{{KernelBox|title=Linux kernel configuration for Ceph|1=
Device Drivers --->
  [*] Block devices --->
    <*> Rados block device (RBD)
 
File systems --->
  [*] Network File Systems --->
    <*> Ceph distributed file system
}}

{{Important|Ensure that support for extended attributes and POSIX ACL support is enabled in all file systems (such as [[Ext4]], [[Btrfs]], etc.) that will be used to host Ceph.}}

== Installation ==

With the system configuration done, install the Ceph software.

The following USE flags are available for fine-tuning the installation.

{{USEflag
|package=sys-cluster/ceph
|babeltrace+++Add support for LTTng babeltrace
|cryptopp+++Use dev-libs/crypto++ for cryptography
|debug+++
|fuse+++Build fuse client
|gtk+++
|libaio+++Use libaio as asynchronous input/output library
|libatomic+++Use libatomic instead of builtin atomic operations
|lttng+++Add support for LTTng
|nss+++Use dev-libs/nss for cryptography
|radosgw+++Add radosgw support
|staticlibs+++
|tcmalloc+++
|xfs+++Add xfs support
|zfs+++Add zfs support
}}

With the USE flags defined, install the software:

{{Emerge|sys-cluster/ceph}}

== Cluster creation ==

== Monitors ==

== Object store devices ==

== Metadata server ==
