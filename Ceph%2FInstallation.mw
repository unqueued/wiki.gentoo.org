All necessary Ceph software is available through the {{Package|sys-cluster/ceph}} package. It contains all services as well as basic administration utilities for managing a Ceph cluster.

== Design ==

Before embarking on a Ceph deployment scenario, take the time to make a basic Ceph cluster design. 

What is the purpose of the Ceph cluster? Is it to play around and experiment with Ceph? Is it to host all critical data in form of [[Ceph/Rados_Block_Device|rbd devices]]? Is it to create a highly available file server?

What features are needed on the Ceph cluster? How many monitors are likely to be needed? How much storage will be used, and how will this storage be represented (as in, how many OSDs will be available and where will they run)? Will the cluster provide S3- or Swift-like APIs to the outside world?

What are the IP addresses that will be used by the cluster? Ceph requires a static IP environment, so making a well designed network infrastructure is important for Ceph to function properly.

How will the servers be distributed across the environment? Ceph has a number of ''buckets'' that it can use to differentiate servers and make well-thought-through distribution and replication decisions. The default is an ''OSD'' on a ''host'' in a ''rack'' in a ''row'' in a ''room'' inside a ''data center''.

There are a number of best practices to account for through:

* Most clusters require 3 monitor servers, perhaps 5. Clusters generally do not need more than 5 monitor servers to function in even the harshest environments.
* Distribute the monitor servers across the environment. If the cluster is over a couple of racks, make sure that the monitor servers are distributed across the racks as well.
* There is usually no need for RAID on the file system that an OSD uses. Instead, rely on the Ceph availability and distribution.
* OSD services do not need a lot of CPU or RAM. A metadata server however does benefit from high-speed CPU and lots of memory.

== Hardware layout ==

The hardware specification of this example consist of three machines: host1, host2, host3, each has three harddisk, first driver (/dev/sda) for OS installation, second, third (/dev/sdb, /dev/sdb) for OSD service,
Ceph Monitor will be deployed at each machine, while Metadata serive will be deployed only at host1

== System configuration ==

The first configuration to decide on is which Ceph version to deploy. At the time of writing, Ceph version 0.87 ("Giant") is available in the tree in ~arch while version 0.80 ("Firefly") is available as stable release. To use the ~arch version, add {{Package|sys-cluster/ceph}} to {{Path|package.accept_keywords}}:

{{FileBox|filename=/etc/portage/package.accept_keywords/ceph|1=
sys-cluster/ceph
}}

Next, validate that the Linux kernel is configured to support Ceph.

{{KernelBox|title=Linux kernel configuration for Ceph|1=
Device Drivers --->
  [*] Block devices --->
    <*> Rados block device (RBD)
 
File systems --->
  [*] Network File Systems --->
    <*> Ceph distributed file system
}}

{{Important|Ensure that support for extended attributes and POSIX ACL support is enabled in all file systems (such as [[Ext4]], [[Btrfs]], etc.) that will be used to host Ceph.}}

== Installation ==

With the system configuration done, install the Ceph software.

The following USE flags are available for fine-tuning the installation.

{{USEflag|package=sys-cluster/ceph}}

With the USE flags defined, install the software:

{{Emerge|sys-cluster/ceph}}

== Cluster creation ==

Use <tt>uuidgen</tt> to generate a cluster id.

{{Cmd|uuidgen|output=<pre>
a0ffc974-222e-449a-a078-121bdfcb110b
</pre>}}

Create the basic skeleton for the {{Path|ceph.conf}} file, and use the generated id for the <code>fsid</code> parameter.

{{FileBox|filename=/etc/ceph/ceph.conf|title=Global part in ceph.conf|lang=ini|1=
[global]
  fsid = a0ffc974-222e-449a-a078-121bdfcb110b
  cluster = ceph
  public network = 192.168.100.0/24
  # Enable cephx authentication (which uses shared keys for almost everything)
  auth cluster required = cephx
  auth service required = cephx
  auth client required = cephx
  # Replication
  osd pool default size = 2
  osd pool default min size = 1
}}

In this example, a cluster is used with a replication factor of 2 (which means it is replicated once - there are two instances of each block) and a minimum of 1 (i.e. as long as one copy of the data is available, continue).

Next create the administrative key. The default administrative key is called <tt>client.admin</tt>:

{{RootCmd|ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --set-uid{{=}}0 --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow'
| grep "key {{=}} " /etc/ceph/ceph.client.admin.keyring {{!}} awk '{print $3}' > /etc/ceph/ceph.client.admin.secret}}

== Monitors ==

To create the monitors, first add in the information to the {{Path|ceph.conf}} file:

{{FileBox|filename=/etc/ceph/ceph.conf|title=Snippet for monitors|lang=ini|1=
[mon]
  # Global settings for monitors
  mon host = host1, host2, host3
  mon addr = 192.168.100.10:6789, 192.168.100.11:6789, 192.168.100.12:6789
  mon initial members = 0, 1, 2

[mon.0]
  host = host1
  mon addr = 192.168.100.10:6789

[mon.1]
  host = host2
  mon addr = 192.168.100.11:6789

[mon.2]
  host = host3
  mon addr = 192.168.100.12:6789
}}

Next create the keyring for the monitor (so that the Ceph monitors can integrate and interact with the Ceph cluster) and add the administrative keyring to it:

{{RootCmd|ceph-authtool --create-keyring /etc/ceph/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *' --import-keyring /etc/ceph/ceph.client.admin.keyring
}}

Now create the initial monitor map (which is a binary file that the Ceph monitors use to find the default, initial monitor list).

{{RootCmd|monmaptool --create --fsid $(uuidgen) --add 0 192.168.100.10 --add 1 192.168.100.11 --add 2 192.168.100.12 /etc/ceph/ceph.initial-monmap}}

Create the file system that the monitors will use to keep their information in.

{{RootCmd|mkdir -p /var/log/ceph|mkdir -p /var/lib/ceph/mon|ceph-mon --mkfs -i 0 --monmap /etc/ceph/ceph.initial-monmap --keyring /etc/ceph/ceph.mon.keyring}}

Repeat this step on each system for the right id (<code>-i 0</code> becomes <code>-i 1</code> etc.)

Finally, create the init script to launch the monitor at boot:

{{RootCmd|ln -s /etc/init.d/ceph /etc/init.d/ceph-mon.0|rc-update add ceph-mon.0 default|rc-service ceph-mon.0 start}}

Also repeat this on each system for the right id.

== Object store devices ==

Get a UUID for the file system:

{{RootCmd|uuidgen|output=<pre>
e33dcfb0-31d5-4953-896d-007c7c295410
</pre>}}

Create a new OSD in the cluster:

{{RootCmd|ceph osd create e33dcfb0-31d5-4953-896d-007c7c295410|output=<pre>
0
</pre>}}

Create the mountpoint on which the data of the OSD will be stored. The {id} to use is the output of the previous command, as 0 here:

{{RootCmd|mkdir -p /var/lib/ceph/osd/ceph-{id} }}

Mount the storage that will be used for the OSD. Then, create the OSD file system on it:

{{RootCmd|ceph-osd -i {id} --mkfs --mkkey --osd-uuid e33dcfb0-31d5-4953-896d-007c7c295410}}

Add the OSD keyring to the clusters' authentication database:

{{RootCmd|ceph auth add osd.{id} osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-{id}/keyring}}

Add the current host to the CRUSH map if it is the first OSD of this host that participates in the cluster:

{{RootCmd|ceph osd crush add-bucket $(hostname) host
|ceph osd crush move $(hostname) root{{=}}default}}

Add each OSD to the map with a default weight value:

{{RootCmd|ceph osd crush add osd.{id} 1.0 host{{=}}$(hostname)}}

Update the OSD information in {{Path|/etc/ceph/ceph.conf}}:

{{FileBox|filename=/etc/ceph/ceph.conf|title=OSD snippet|lang=ini|1=
[osd]
  # Global defaults for OSDs
  osd journal size = 1024
  # Needed for Ext4 file systems only
  filestore xattr use omap = true
  # choose filsystem type: ext4 or xfs
  osd mkfs type = ext4
  # Mount option for ext4
  osd mount options ext4 = user_xattr,rw,noatime
  # Mount option for xfs
  osd mount options xfs = rw,inode64

[osd.0]
  host = host1
  devs = /dev/sdb1

[osd.1]
  host = host1
  devs = /dev/sdc1

[osd.2]
  host = host2
  devs = /dev/sdb1

[osd.3]
  host = host2
  devs = /dev/sdc1

[osd.4]
  host = host3
  devs = /dev/sdb1

[osd.5]
  host = host3
  devs = /dev/sdc1
}}

Create the init script for the OSD and have it start at boot:

{{RootCmd|ln -s /etc/init.d/ceph /etc/init.d/ceph-osd.{id}|rc-update add ceph-osd.{id} default|rc-service ceph-osd.{id} start}}

== Metadata server ==

Update the {{Path|ceph.conf}} information for the MDS:

{{FileBox|filename=/etc/ceph/ceph.conf|title=MDS snippet|lang=ini|1=
[mds.0]
  host = host1
}}

Create two pools - one for data and one for metadata. The number <tt>128</tt> in the example below is the number of placement groups to assign inside the pool. Tune this correctly depending on the size of the cluster (see [http://ceph.com/docs/master/rados/operations/placement-groups/ Ceph's placement groups information]).

{{RootCmd|ceph osd pool create data 128
|ceph osd pool create metadata 128}}

Now create a file system that uses these pools. The name of the file system can be chosen freely - the example uses <tt>cephfs</tt>:

{{RootCmd|ceph fs new cephfs metadata data}}

Create the keyring for the MDS service:

{{RootCmd|mkdir -p /var/lib/ceph/mds/ceph-0|ceph auth get-or-create mds.0 mds 'allow' osd 'allow *' mon 'allow rwx' > /var/lib/ceph/mds/ceph-0/keyring}}

Create the init script and have it start at boot:

{{RootCmd|ln -s /etc/init.d/ceph /etc/init.d/ceph-mds.0|rc-update add ceph-mds.0 default|rc-service ceph-mds.0 start}}
