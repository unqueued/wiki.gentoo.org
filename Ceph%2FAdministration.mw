Ceph administration focuses on the various services inside the Ceph cluster.

== File system ==

A Ceph file system requires two pools to start with. One pool contains the data while another pool is meant for the metadata. During the installation these pools should already be created:

{{RootCmd|ceph osd lspools|output=<pre>
11 data,12 metadata,
</pre>}}

If this is not the case, create the pools:

{{RootCmd|ceph osd pool create data 128|ceph osd pool create metadata 128}}

With these pools available, a file system can be created. First make sure that no file system already exists:

{{RootCmd|ceph fs ls|output=<pre>
name: cephfs, metadata pool: metadata, data pools: [data ]
</pre>}}

If it already exists, then no action needs to be undertaken anymore. Otherwise the file system can be created:

{{RootCmd|ceph fs new cephfs metadata data}}

To remove a file system, it is necessary to first fail the MDS service:

{{RootCmd|ceph mds fail 0|ceph fs rm cephfs}}

When a file system exists, it can be mounted on the Linux clients that participate in the cluster. With the Cephx authentication it is necessary to pass on the client name (the <tt>client.admin</tt> one which is created when the cluster is created can be used, but a less privileged one can be used as well) with the key (usually by referring to the secret file created when creating the user key):

{{RootCmd|mount -t ceph -o name{{=}}admin,secretfile{{=}}/etc/ceph/ceph.client.admin.secret host1,host2:/ /srv/ceph}}

== Pools ==

TODO

== Placement groups ==

TODO

== Authentication keys ==

TODO
