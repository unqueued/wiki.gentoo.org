<languages />

{{Metadata|abstract=LVM permite a los administradores crear metadispositivos que ofrecen una capa de abstracción entre un sistema de ficheros y el almacenamiento físico que se utiliza por debajo.}}

{{InfoBox stack
|{{InfoBox wikipedia|LVM|header=true}}
}}

'''LVM''' ('''L'''ogical '''V'''olume '''M'''anager) permite a los administradores crear metadispositivos que ofrecen una capa de abstracción entre un sistema de ficheros y el almacenamiento físico que se utiliza por debajo. Los metadispositivos (en los cuales se alojan los sistemas de ficheros) son ''volúmenes lógicos'' que utilizan almacenamiento que se obtiene de espacios de almacenamiento  llamados ''grupos de volumen''. Un grupo de volumen se abastece con uno o más ''volúmenes físicos'' que son los auténticos dispositivos en los que se almacenan los datos.

Los volúmenes físicos pueden ser particiones, discos SATA completos agrupados como JBOD ('''J'''ust a '''B'''unch '''O'''f '''D'''isks o '''Simplemente un puñado de discos'''), sistemas RAID, iSCSI, Fibre Channel, eSATA etc.

== Instalación ==

LVM se gestiona por los controladores al nivel de núcleo y por aplicaciones en el espacio de usuario que permiten gestionar la configuración de LVM.

=== Núcleo ===

Activar las siguientes opciones del núcleo:

{{Kernel||<pre>
Device Drivers --->
 Multiple devices driver support (RAID and LVM) --->
  <*> Device mapper support
    <*> Crypt target support
    <*> Snapshot target
    <*> Mirror target
  <*> Multipath target
    <*> I/O Path Selector based on the number of in-flight I/Os
    <*> I/O Path Selector based on the service time </pre>}}

{{Note/es|No se necesita activar todo lo de arriba, algunas opciones son necesarias únicamente para [[#LVM2_Snapshots_and_LVM2_Thin_Snapshots|LVM2 Snapshots and LVM2 Thin Snapshots]], [[#LVM2_Mirrors|LVM2 Mirrors]], [[#LVM2_RAID_0.2FStripeset|LVM2 RAID 0/Stripeset]] y el cifrado.}}

=== Software ===

Instalar {{Package|sys-fs/lvm2}}:

{{USEflag|package=sys-fs/lvm2
|clvm
|cman
|lvm1+yes
|readline+yes
|selinux++no
|static
|static-libs
|thin+yes
|udev+yes
}}

{{Emerge|lvm2}}

== Configuración ==

La configuración de LVM se realiza en diferentes niveles:
# La gestión de LV, PV y VG a través de las utilidades de gestión
# El ajuste fino del subsistema LVM a través del fichero de configuración
# La gestión del servicio al nivel de distribución
# La configuración a través de un sistema de ficheros RAM inicial

La gestión de los volúmenes físicos y lógicos así como de los grupos de volumen se gestiona en el capítulo [[#Utilización|Utilización]].

=== Fichero de configuración de LVM ===

LVM posee un fichero de configuración muy extenso en {{Path|/etc/lvm/lvm.conf}}. La mayoría de usuarios no necesitan modificar ningún aspecto de este fichero para comenzar a usar LVM.

=== Gestión de servicios ===

Gentoo ofrece el servicio LVM para detectar automáticamente los grupos de volumen y los volúmenes lógicos.

Se puede gestionar este servicio a través del sistema de inico.

==== openrc ====

Para iniciar LVM manualmente:

{{RootCmd|/etc/init.d/lvm start}}

Para iniciar LVM cuando arranque el sistema:

{{RootCmd|rc-update add lvm boot}}

==== systemd ====

Para iniciar LVM manualmente:

{{RootCmd|systemctl start lvm2-monitor.service}}

Para iniciar LVM cuando arranque el sistema:

{{RootCmd|systemctl enable lvm2-monitor.service}}

=== Usar LVM en un initramfs ===

La mayoría de los cargadores de arranque no pueden iniciar directamente desde LVM. Ni GRUB Legacy ni LILO pueden hacerlo. Grub 2 PUEDE iniciar desde un volumen lógico LVM lineal, desde un volumen lógico en espejo y probablemente desde algunos tipos de volúmenes lógicos RAID. Ningún cargador de arranque ofrece soporte actualmente para volúmenes lógicos delgados (thin). 

Debido a esto, se recomienda utilizar una partición de arranque que no sea LVM y montar la partición raíz LVM desde un initramfs. Se puede generar este initramfs de forma automática mediante [[Genkernel/es|genkernel]], genkernel-next y [[dracut]]:

* '''genkernel''' puede iniciar desde cualquier tipo excepto volúmenes delgados (ya que ni construye ni copia los binarios de thin-provisioning-tools del equipo de construcción) y quizá desde RAID10 (El soporte de RAID10 requiere LVM2 2.02.98, pero genkernel construye la versión 2.02.89. Sin embargo, si se dispone de binarios estáticos, éstos se puede copiar)
* '''genkernel-next''' puede iniciar desde todos los tipos de volúmenes, pero necesita un nuevo app-misc/pax-utils o, de lo contrario, los binarios para volúmenes delgados estarán rotos (Mirar la incidencia {{Bug|482504}})
* '''dracut''' debería poder iniciar desde todos los tipos, pero únicamente incluye soporte para volúmenes delgados en el initramfs si el equipo en el que corre tiene un volumen raíz delgado.

==== Genkernel/Genkernel-next ====

Haga emerge de {{Package|sys-kernel/genkernel}} o de {{Package|sys-kernel/genkernel-next}}. El ajuste USE static también se puede activar en el paquete {{Package|sys-fs/lvm2}} de modo que genkernel utilice los binarios del sistema (de lo contrario construirá su propia copia privada). El siguiente ejemplo construirá únicamente un initramfs (no el núcleo completo) y habilitará el soporte para lvm.

{{RootCmd|genkernel --lvm initramfs}}

La página del manual de genkernel describe otras opciones que dependen de los requisitos del sistema.

El initrd requerirá parámetros para indicarle cómo iniciar lvm. Éstos se pasan igual que el resto de parámetros del núcleo. Por ejemplo:

{{File|/etc/default/grub|Añadir dolvm como parámetro de inicio del núcleo|<pre>
GRUB_CMDLINE_LINUX="dolvm"
</pre>}}

==== Dracut ====

El paquete {{Package|sys-kernel/dracut}} se portó desde el proyecto RedHat y es una herramienta similar para generar un initramfs. Ya que actualmente  Since it is currently se encuentra en pruebas (~arch), los usuarios necesitarán [[Knowledge_Base:Accepting_a_keyword_for_a_single_package|aceptar este hecho]] (a través de {{Path|/etc/portage/package.accept_keywords}}) para poder hacer emerge. Antes de hacerlo, se debería añadir la variable <code>DRACUT_MODULES="lvm"</code> a {{Path|/etc/portage/make.conf}}. Puede que se quieran instalar otros módulos, por favor, eche un vistazo a [[Dracut]]. Normalmente la siguiente orden genera un initramfs funcional por defecto.

{{RootCmd|dracut -a lvm}}

El initrd requerirá parámetros para indicarle cómo iniciar lvm. Éstos se pasan igual que el resto de parámetros del núcleo. Por ejemplo:

{{File|/etc/default/grub|Añadir soporte para LVM a los parámetros de inicio del núcleo|<pre>
GRUB_CMDLINE_LINUX="rd.lvm.vg=vol00"
</pre>}}

Para obtener una lista completa de opciones de dracut, por favor, lea la sección en el [https://www.kernel.org/pub/linux/utils/boot/dracut/dracut.html#_lvm Manual de Dracut].

== Utilización ==

LVM organiza el almacenamiento en tres niveles diferentes como se muestra a continuación:
* Los discos duros, particiones, sistemas RAID y otros modos de almacenamiento se inicializan como volúmenes físicos (PVs)
* Los Volúmenes Físicos (PV) se agrupan en Grupos de Volumen (VG)
* Los Volúmenes Lógicos (LV) se gestionan en los Grupos de Volumen (VG)

=== PV (Volumen Físico) ===
Los Volúmenes Físicos son el hardware o el sistema de almacenamiento sobre el que se construye LVM.

==== Particionamiento ====

{{Note/es|Solo se necesita usar particiones separadas para proveer de almacenamiento a los grupos de volumen en el caso de que no se quiera utilizar todo el disco para un solo grupo de volúmenes LVM. Si se puede utilizar todo el disco, entonces ignore esto e inicialice el disco duro completo como un volumen físico.}}

El tipo de partición para ''LVM'' es ''8e'' (Linux LVM).

Por ejemplo, para definir el tipo usando <code>fdisk</code> para una partición en {{Path|/dev/sda}}:

{{RootCmd|fdisk /dev/sda}}

En <code>fdisk</code>, cree particiones usando la tecla '''n''' y a continuación cambie el tipo de partición con la tecla '''t''' a ''8e''.

==== Crear PV ====

Puede crear o inicializar los volúmenes físico con la orden <code>pvcreate</code>.

Por ejemplo, la siguiente orden crea un volumen físico en la primera partición primar de los discos {{Path|/dev/sda}} y {{Path|/dev/sdb}}:

{{RootCmd|pvcreate /dev/sd[ab]1}}

==== Listar PV ====

Con la orden <code>pvdisplay</code> se puede obtener información de todos los volúmenes físicos activos del sistema.

{{RootCmd|pvdisplay|output=<pre>
--- Physical volume ---
PV Name /dev/sda1
VG Name volgrp
PV Size 160.01 GiB / not usable 2.31 MiB
Allocatable yes
PE Size 4.00 MiB
Total PE 40962
Free PE 4098
Allocated PE 36864
PV UUID 3WHAz3-dh4r-RJ0E-5o6T-9Dbs-4xLe-inVwcV

--- Physical volume ---
PV Name /dev/sdb1
VG Name volgrp
PV Size 160.01 GiB / not usable 2.31 MiB
Allocatable yes
PE Size 4.00 MiB
Total PE 40962
Free PE 40962
Allocated PE 0
PV UUID b031x0-6rej-BcBu-bE2C-eCXG-jObu-0Boo0x
</pre>}}

Si se necesitan mostrar más volúmenes físicos, <code>pvscan</code> puede detectar volúmenes físicos inactivos y activarlos.

{{RootCmd|pvscan|output=<pre>
PV /dev/sda1 VG volgrp lvm2 [160.01 GiB / 16.01 GiB free]
PV /dev/sdb1 VG volgrp lvm2 [160.01 GiB / 160.01 GiB free]
Total: 2 [320.02 GB] / in use: 2 [320.02 GiB] / in no VG: 0 [0]
</pre>}}

==== Eliminar PV ====

LVM distribuye los datos automáticamente entre todos los volúmenes físicos disponibles (a menos que se le indique lo contrario) usando un enfoque lineal. Si un volumen lógico (dentro de un grupo de volumen) es menor que la cantidad total de espacio libre en un volumen físico, entonces todo el espacio se reclama de ese (único) volumen físico de una forma continua. Esto se hace así por cuestiones de rendimiento.

Si se necesita eliminar un volumen físico de un grupo de volumen, se necesita en primer lugar mover los datos de ese volumen físico. Con la orden <code>pvmove</code>, todos los datos de un volumen físico se mueven a otros volúmenes físicos dentro del mismo grupo de volumen.

{{RootCmd|pvmove -v /dev/sda1}}

Esta operación puede llevar tiempo dependiendo de la cantidad de datos que se van a mover. Una vez haya terminado, no debería haber datos en el dispositivo. Puede comprobar con <code>pvdisplay</code> que ningún volumen lógico utiliza ese volumen físico.

El siguiente paso es eliminar el volumen físico del grupo de volumen mediante <code>vgreduce</code> después de lo cual el dispositivo se puede "deseleccionar" como un volumen físico usando <code>pvremove</code>:

{{RootCmd|vgreduce vg0 /dev/sda1 && pvremove /dev/sda1}}

=== VG (Grupo de Volumen) ===

Un grupo de volumen (VG) agrupa varios volúmenes físicos y se muestra como {{Path|/dev/NOMBRE_DEL_VG}} el el sistema de ficheros de los dispositivos. El administrador es el que elige el nombre del grupo de volumen.

==== Crear VG ====

La siguiente orden crea un grupo de volumen llamado ''vg0'' con dos volúmenes físicos asignados: {{Path|/dev/sda1}} y {{Path|/dev/sdb1}}.

{{RootCmd|vgcreate vg0 /dev/sd[ab]1}}

==== Listar VG ====

Para listar todos los grupos de volumen activos, utilice la orden <code>vgdisplay</code>:

{{RootCmd|vgdisplay|output=<pre>
--- Volume group ---
VG Name vg0
System ID
Format lvm2
Metadata Areas 1
Metadata Sequence No 8
VG Access read/write
VG Status resizable
MAX LV 0
Cur LV 6
Open LV 6
Max PV 0
Cur PV 1
Act PV 1
VG Size 320.02 GiB
PE Size 4.00 MiB
Total PE 81924
Alloc PE / Size 36864 / 144.00 GiB
Free PE / Size 45056 /176.01 GiB
VG UUID mFPXj3-DdPi-7YJ5-9WKy-KA5Y-Vd4S-Lycxq3
</pre>}}

Si falta algún grupo de volumen, utilice la orden <code>vgscan</code> para localizarlo:

{{RootCmd|vgscan|output=<pre>
 Reading all physical volumes. This may take a while...
 Found volume group "vg0" using metadata type lvm2
</pre>}}

==== Extender VG ====

Los grupos de volumen agrupan volúmenes físicos, permitiendo a los administradores utilizar un conjunto de recursos de almacenamiento para crear sistemas de ficheros. Cuando un grupo de volumen no tiene los suficientes recursos de almacenamiento, es necesario extenderlo con volúmenes físicos adicionales.

El siguiente ejemplo extiende el grupo de volumen ''vg0'' con un volumen físico en {{Path|/dev/sdc1}}:

{{RootCmd|vgextend vg0 /dev/sdc1}}

¡Recuerde antes necesita inicializar el volumen físico!

==== Reducir VG ====

Si se necesita eliminar volúmenes físicos de un grupo de volumen, todos los datos que todavía se utilizan en ese volumen físicos se deben mover a otros dentro del mismo grupo de volumen. Como hemos visto antes, esto se gestiona con la orden <code>pvmove</code> después de la cual el volumen físico se puede eliminar del grupo de volumen usando <code>vgreduce</code>:

{{RootCmd|pvmove -v /dev/sdc1 |vgreduce vg0 /dev/sdc1}}

==== Eliminar VG ====

Si ya no se necesita un grupo de volumen (o, en otras palabras el conjunto de recursos de almacenamiento que éste representa ya no se necesita y se necesita liberar los volúmenes físicos dentro de él para otras cuestiones) entonces se puede eliminar el grupo de volumen con <code>vgremove</code>. Esto solo funciona si no no se han definido volúmenes lógicos dentro del grupo de volumen y se han eliminado todos los volúmenes físicos del conjunto salvo uno.

{{RootCmd|vgremove vg0}}

=== LV (Volumen Lógico) ===

Los volúmenes lógicos son los metadispositivos finales que se ofrecen al sistema normalmente para crear sistemas de ficheros en ellos. Se crean y se gestionan dentro de grupos de volumen y se muestran como {{Path|/dev/NOMBRE_DEL_VG/NOMBRE_DEL_LV}}. Al igual que con los grupos de volumen el nombre lo decide el administrador.

==== Crear LV ====

Para crear un volumen lógico se utiliza la orden <code>lvcreate</code>. Los parámetros de esta orden son: el tamaño deseado para el volumen lógico (que no puede ser mayor que la cantidad disponible en el grupo de volumen), el grupo de volumen del cual se obtendrá el espacio y el nombre del volumen lógico que se va a crear.

En el ejemplo de abajo, se crea un volumen lógico llamado ''lvol1'' en el grupo de volumen llamado ''vg0'' con un tamaño de 150MB:

{{RootCmd|lvcreate -L 150M -n lvol1 vg0}}

Es posible indicarle a <code>lvcreate</code> que use todo el espacio disponible en el grupo de volumen. Esto se realiza mediante el parámetro ''-l'' el cual selecciona el número de ''extents'' (extensiones) en lugar de un tamaño (legible por los humanos). Los volúmenes lógicos se dividen en ''extensiones lógicas'' que son trozos de datos dentro de un grupo de volumen. Todas la extensiones dentro de un grupo de volumen tienen el mismo tamaño. Con el párametro ''-l'' se puede pedir a <code>lvcreate</code> que use todas las extensiones disponibles:

{{RootCmd|lvcreate -l 100%FREE -n lvol1 vg0}}

A continuación de ''FREE'' se puede utilizar la clave ''VG'' para indicar el tamaño total de un grupo de volumen.

==== Listar LV ====

Para listar todos los volúmenes lógicos, utilice la orden <code>lvdisplay</code>:

{{RootCmd|lvdisplay}}

Si echa de menos algún volumen lógico, entonces la orden <code>lvscan</code> se puede utiliza para escanear los grupos de volumen en busca de otros volúmenes lógicos.

{{RootCmd|lvscan}}

==== Extender LV ====

Cuando se necesita expandir un volumen lógico, entonces se puede utilizar la orden <code>lvextend</code> para ampliar el espacio asociado al volumen lógico.

Por ejemplo, para extender el volumen lógico ''lvol1'' a un total de 500 MB:

{{RootCmd|lvextend -L500M /dev/vg0/lvol1}}

También es posible utilizar el tamaño que se necesita añadir en lugar del tamaño total:

{{RootCmd|lvextend -L+350MB /dev/vg0/lvol1}}

Un grupo de volumen extendido no ofrece de forma inmediata el espacio de almacenamiento adicional a los usuarios finales. Para ello, se necesita igualmente incrementar el sistema de ficheros creado en el grupo de volumen. No todos los sistemas de ficheros admiten el cambio de tamaño cuando se están utilizando, por tanto compruebe la documentación del sistema de ficheros en cuestión para obtener más información.

Por ejemplo, para cambiar el tamaño de un sistema de ficheros ext4 a 500MB:

{{RootCmd|resize2fs /mnt/data 500M}}

==== Reducir LV ====

Si se necesita reducir el tamaño de un volumen lógico, en primer lugar se debe reducir el propio sistema de ficheros. No todos los sistemas de ficheros lo permiten cuando se están utilizando.

Por ejemplo, ext4 no permite reducir el tamaño cuando se está utilizando por lo que se debe desmontar en primer lugar. También se recomienda realizar una comprobación del sistema de ficheros para asegurarse de que no hay inconsistencias:

{{RootCmd|umount /mnt/data
|e2fsck -f /dev/vg0/lvol1
|resize2fs /dev/vg0/lvol1 150M}}

Con un sistema de ficheros reducido ya es posible reducir también el volumen lógico:

{{RootCmd|lvreduce -L150M /dev/vg0/lvol1}}

==== Permisos LV ====

LVM ofrece soporte para estados de permisos en los volúmenes lógicos.

Por ejemplo, se puede definir un volumen lógico a ''solo lectura'' mediante la orden <code>lvchange</code>:

{{RootCmd|lvchange -p r /dev/vg0/lvol1
|mount -o remount /dev/vg0/lvol1}}

La opción para remontar el sistema de fichero se necesita ya que el cambio no se fuerza de forma inmediata.

Para marcar el volumen lógico como escribible, utilice el bit de permisos ''rw'':

{{RootCmd|lvchange -p rw /dev/vg0/lvol1 && mount -o remount /dev/vg0/lvol1}}

==== Eliminar LV ====

Antes de eliminar un volumen lógico, asegúrese de que no está montado:

{{RootCmd|umount /dev/vg0/lvol1}}

Desactive el volumen lógico de modo que no se pueda realizar ninguna actividad de escritura:

{{RootCmd|lvchange -a n /dev/vg0/lvol1}}

Una vez que el volumen está desmontado y desactivado, se puede eliminar, liberando las extensiones asignadas a éste para que otros volúmenes lógicos del grupo de volumen las puedan utilizar:

{{RootCmd|lvremove /dev/vg0/lvol1}}

== Características ==

LVM ofrece algunas características interesantes para los administradores del almacenamiento, incluyendo (pero no limitadas a):
* Provisión ligera (almacenamiento "over-commiting")
* Soporte a instantáneas
* Tipos de volumen con distintos métodos de asignación de espacio de almacenamiento

=== Provisión ligera ===

Las versiones recientes de LVM2  (2.02.89) ofrecen soporte a volúmenes "delgados" (thin). Los volúmenes delgados son a los dispositivos de bloque lo que los ficheros desperdigados son a los sistemas de ficheros. Así, un volumen lógico delgado dentro de un conjunto se puede sobreasignar (over-committed): el tamaño que muestra puede ser mayor que el presente en el propio conjunto. Al igual que un fichero, las extensiones se asignan mientras el dispositivo se va llenando. Si el sistema de ficheros tiene soporte para ''descartes'', las extensiones se liberan de nuevo cuando los ficheros se eliminan, reduciendo el uso del espacio en el conjunto.

Dentro de LVM, este conjunto delgado es un tipo especial de volumen lógico que puede contener a su vez otros volúmenes lógicos.

==== Crear un conjunto delgado ====

{{Warning/es|Si ocurre un desbordamiento (overflow) dentro de los metadatos del conjunto delgado, entonces el conjunto se corromperá. '''LVM no puede recuperarse de esto'''.}} 

{{Note/es|Si se acaba el espacio de almacenamiento del conjunto delgado, cualquier proceso que pudiera causar que este conjunto intentara asignar más extensiones (que no están disponibles) pasaría al estado "dormido y terminable" ("killable sleep") hasta que se extienda el conjunto delgado o el proceso reciba la señal SIGKILL.}}

Cada conjunto delgado tiene unos metadatos asociados que se añaden al tamaño del conjunto. LVM calculará el tamaño de los metadatos basándose en el tamaño del conjunto y como mínimo asignará <tt>pool_chunks * 64 bytes</tt> o 2MiB, el que sea mayor. El administrador puede seleccionar también un tamaño diferente para los metadatos.

Para crear un conjunto ligero, añada los parámetros ''--type thin-pool --thinpool thin_pool'' a la orden <code>lvcreate</code>:

{{RootCmd|lvcreate -L 150M --type thin-pool --thinpool thin_pool vg0}}

The above example creates a thin pool called ''thin_pool'' with a total size of 150 MB. This is the real allocated size for the thin pool (and thus the total amount of actual storage that can be used).

To explicitly ask for a certain metadata size, use the ''--metadatasize'' parameter:

{{RootCmd|lvcreate -L 150M --metadatasize 2M --type thin-pool --thinpool thin_pool vg0}}

Due to the metadata that is added to the thin pool, the intuitive way of using all available size in a volume group for a logical volume does not work (see LVM bug [https://bugzilla.redhat.com/show_bug.cgi?id=812726|812726]):

{{RootCmd|lvcreate -l 100%FREE --type thin-pool --thinpool thin_pool vg0|output=<pre>
Insufficient suitable allocatable extents for logical volume thin_pool: 549 more required
</pre>}}

Note the thin pool does not have an associated device node like other LV's.

==== Creating a thin logical volume ====

A ''thin logical volume'' is a logical volume inside the thin pool (which itself is a logical volume). As thin logical volumes are ''sparse'', a virtual size instead of a physical size is specified using the ''-V'' parameter:

{{RootCmd|lvcreate -T vg0/thin_pool -V 300M -n lvol1}}

In this example, the (thin) logical volume ''lvol1'' is exposed as a 300MB-sized device, even though the underlying pool only holds 150MB of real allocated storage.

It is also possible to create both the thin pool as well as the logical volume inside the thin pool in one command:

{{RootCmd|lvcreate -T vg0/thin_pool -V 300M -L150M -n lvol1}}

==== Listing thin pools and thin logical volumes ====

Thin pools and thin logical volumes are special types of logical volumes, and as such as displayed through the <code>lvdisplay</code> command. The <code>lvscan</code> command will also detect these logical volumes.

==== Extending a thin pool ====

{{Warning|As of LVM2 2.02.89, the metadata size of the thin pool cannot be expanded, it is fixed at creation}}

The thin pool is expanded like a non-thin logical volume using <code>lvextend</code>. For instance:

{{RootCmd|lvextend -L500M vg0/thin_pool}}

==== Extending a thin logical volume ====

A thin logical volume is expanded just like a regular one:

{{RootCmd|lvextend -L1G vg0/lvol1}}

Note that the <code>lvextend</code> command uses the ''-L'' option (or ''-l'' if extent counts are used) and not a "virtual size" option as was used during the creation. 

==== Reducing a thin pool ====

Currently, LVM cannot reduce the size of the thin pool. See LVM bug [https://bugzilla.redhat.com/show_bug.cgi?id=812731|812731].

==== Reducing a thin logical volume ====

Thin logical volumes are reduced just like regular logical volumes.

For instance:
{{RootCmd|lvreduce -L300M vg0/lvol1l}}

Note that the <code>lvreduce</code> command uses the ''-L'' option (or ''-l'' if extent counts are used) and not a "virtual size" option as was used during the creation.

==== Removing thin pools ====

Thin pools cannot be removed until all the thin logical volumes inside it are removed.

When a thin pool no longer services any thin logical volume, it can be removed through the <code>lvremove</code> command:

{{RootCmd|lvremove vg0/thin_pool}}

=== LVM2 snapshots and thin snapshots ===

A snapshot is a logical volume that acts as copy of another logical volume. It displays the state of the original logical volume at the time of snapshot creation.

==== Creating a snapshot logical volume ====

A snapshot logical volume is created using the ''-s'' option to <code>lvcreate</code>. Snapshot logical volumes are still given allocated storage as LVM "registers" all changes made to the original logical volume and stores these changes in the allocated storage for the snapshot. When querying the snapshot state, LVM will start from the original logical volume and then check all changes registered, "undoing" the changes before showing the result to the user.

A snapshot logical volume henceforth "growths" at the rate that changes are made on the original logical volume. When the allocated storage for the snapshot is completely used, then the snapshot will be removed automatically from the system.

{{RootCmd|lvcreate -l 10%VG -s -n 20140412_lvol1 /dev/vg0/lvol1}}

The above example creates a snapshot logical volume called ''20140412_lvol1'', based on the logical volume ''lvol1'' in volume group ''vg0''. It uses 10% of the space (extents actually) allocated to the volume group.

==== Accessing a snapshot logical volume ====

Snapshot logical volumes can be mounted like regular logical volumes. They are even not restricted to read-only operations - it is possible to modify snapshots and thus use it for things such as testing changes before doing these on a "production" file system.

As long as snapshot logical volumes exist, the regular/original logical volume cannot be reduced in size or removed.

==== LVM thin snapshots ====

{{Note|A thin snapshot can only be taken on a thin pool for a thin logical volume. The thin device mapper target supports thin snapshots of read-only non-thin logical volumes, but the LVM2 tooling does not support this. However, it is possible to create a regular (non-thin) snapshot logical volume of a thin logical volume.}}

To create a thin snapshot, the <code>lvcreate</code> command is used with the <code>-s</code> option. No size declaration needs to be passed on:

{{RootCmd|lvcreate -s -n 20140413_lvol1 /dev/vg0/lvol1}}

Thin logical volume snapshots have the same size as their original thin logical volume, and use a physical allocation of 0 just like all other thin logical volumes. 

{{Important|If ''-l'' or ''-L'' is specified, a snapshot will still be created, but the resulting snapshot will be a regular snapshot, not a thin snapshot.}}

También es posible tomar instantáneas de instantáneas:

{{RootCmd|lvcreate -s -n 1_20140413_lvol1 /dev/vg0/20140413_lvol1}}

Thin snapshots have several advantages over regular snapshots. First, thin snapshots are independent of their original logical volume once created. The original logical volume can be shrunk or deleted without affecting the snapshot. Second, thin snapshots can be efficiently created recursively (snapshots of snapshots) without the "chaining" overhead of regular recursive LVM snapshots.

==== Rolling back to snapshot state ====

To rollback the logical volume to the version of the snapshot, use the following command:

{{RootCmd|lvconvert --merge /dev/vg0/20140413_lvol1}}

This might take a couple of minutes, depending on the size of the volume.

{{Important|The snapshot will disappear and this change is not revertible}}

==== Rolling back thin snapshots ====

For thin volumes, <code>lvconvert --merge</code> does not work. Instead, delete the original logical volume and rename the snapshot:

{{RootCmd|umount /dev/vg0/lvol1
|lvremove /dev/vg0/lvol1
|lvrename vg0/20140413_lvol1 lvol1}}

=== Different storage allocation methods ===

LVM supports different allocation methods for storage:
* linear volumes (which is the default)
* mirrored volumes (in a more-or-less active/standby setup)
* striping (RAID0)
* mirrored volumes (RAID1 - which is more an active/active setup)
* striping with parity (RAID4 and RAID5)
* striping with double parity (RAID6)
* striping and mirroring (RAID10)

==== Linear volumes ====

Linear volumes are the most common kind of LVM volumes. LVM will attempt to allocate the logical volume to be as physically contiguous as possible. If there is a physical volume large enough to hold the entire logical volume, then LVM will allocate it there, otherwise it will split it up into as few pieces as possible.

The commands introduced earlier on to create volume groups and logical volumes create linear volumes.

Because linear volumes have no special requirements, they are the easiest to manipulate and can be resized and relocated at will. If a logical volume is allocated across multiple physical volumes, and any of the physical volumes become unavailable, then that logical volume cannot be started anymore and will be unusable.

==== Mirrored volumes ====

LVM supports ''mirrored'' volumes, which provide fault tolerance in the event of drive failure. Unlike RAID1, there is no performance benefit - all reads and writes are delivered to a single side of the mirror.

To keep track of the mirror state, LVM requires a ''log'' to be kept. It is recommended (and often even mandatory) to position this log on a physical volume that does not contain any of the mirrored logical volumes. There are three kind of logs that can be used for mirrors:

# '''Disk''' is the default log type. All changes made are logged into extra metadata extents, which LVM manages. If a device fails, then the changes are kept in the log until the mirror can be restored again.
# '''Mirror''' logs are '''disk''' logs that are themselves mirrored. 
# '''Core''' mirror logs record the state of the mirror in memory only. LVM will have to rebuild the mirror every time it is activated. This type is useful for temporary mirrors.

To create a logical volume with a single mirror, pass the ''-m 1'' argument (to select standard mirroring) with optionally ''--mirrorlog'' to select a particular log type:

{{RootCmd|lvcreate -m 1 --mirrorlog mirror -l 40%VG --nosync -n lvol1 vg0}}

The <tt>-m 1</tt> tells LVM to create one (additional) mirror, so requiring 2 physical volumes. The <tt>--nosync</tt> option is an optimization - without it LVM will try synchronize the mirror by copying empty sectors from one logical volume to another.

It is possible to create a mirror of an existing logical volume:

{{RootCmd|lvconvert -m 1 -b vg0/lvol1}}

The ''-b'' option does the conversion in the background as this can take quite a while.

To remove a mirror, set the number of mirrors (back) to 0:

{{RootCmd|lvconvert -m0 vg0/lvol1}}

If part of the mirror is unavailable (usually because the disk containing the physical volume has failed), the volume group will need to be brought up in degraded mode:

{{RootCmd|vgchange -ay --partial vg0}}

On the first write, LVM will notice the mirror is broken. The default policy ("remove") is to automatically reduce/break the mirror according to the number of pieces available. A 3-way mirror with a missing physical volume will be reduced to 2-way mirror; a 2-way mirror will be reduced to a regular linear volume. If the failure is only transient, and the missing physical volume returns after LVM has broken the mirror, the mirrored logical volume will need to be recreated on it. 

To recover the mirror, the failed physical volume needs to be removed from the volume group, and a replacement physical volume needs to be added (or if the volume group has a free physical volume, it can be created on that one). Then the mirror can be recreated with <tt>lvconvert</tt> at which point the old physical volume can be removed from the volume group:

{{RootCmd|vgextend vg0 /dev/sdc1
|lvconvert -b -m 1 --mirrorlog disk vg0/lvol1
|vgreduce --removemissing vg0}}

It is possible to have LVM recreate the mirror with free extents on a different physical volume if one side fails. To accomplish that, set <code>mirror_image_fault_policy</code> to ''allocate'' in {{Path|lvm.conf}}.

==== Thin mirrors ====

It is not (yet) possible to create a mirrored thin pool or thin volume. It is possible to create a mirrored thin pool my creating a normal mirrored logical volume and then converting the logical volume to a thin pool with <tt>lvconvert</tt>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will merge them into a single logical volume.

{{Warning|LVM 2.02.98 or above is required for this to work properly. Prior versions are either not capable or will segfault and corrupt the volume group. Also, conversion of a mirror into a thin pool '''destroys''' all existing data in the mirror!}}

{{RootCmd|lvcreate -m 1 --mirrorlog mirrored -l40%VG -n thin_pool vg0
|lvcreate -m 1 --mirrorlog mirrored -L4MB -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg0/thin_meta}}

==== Striping (RAID0) ====

Instead of a linear volume, where multiple contiguous physical volumes are appended, it possible to create a ''striped'' or ''RAID 0'' volume for better performance. This will alternate storage allocations across the available physical volumes.

To create a striped volume over three physical volumes:

{{RootCmd|lvcreate -i 3 -l 20%VG -n lvol1_stripe vg0|output=<pre>
Using default stripesize 64.00 KiB
</pre>}}

The -i option indicates over how many physical volumes the striping should be done.

It is possible to mirror a stripe set. The -i and -m options can be combined to create a striped mirror:

{{RootCmd|lvcreate -i 2 -m 1 -l 10%VG vg0}}

This creates a 2 physical volume stripe set and mirrors it on 2 different physical volumes, for a total of 4 physical volumes. An existing stripe set can be mirrored with <tt>lvconvert</tt>.

A thin pool can be striped like any other logical volume. All the thin volumes created from the pool inherit that settings - do not specify it manually when creating a thin volume.

It is not possible to stripe an existing volume, nor reshape the stripes across more/less physical volumes, nor to convert to a different RAID level/linear volume. A stripe set can be mirrored. It is possible to extend a stripe set across additional physical volumes, but they must be added in multiples of the original stripe set (which will effectively linearly append a new stripe set).

==== Mirroring (RAID1) ====

Unlike RAID 0, which is striping, RAID 1 is mirroring, but implemented differently than the original LVM mirror. Under RAID1, reads are spread out across physical volumes, improving performance. RAID1 mirror failures do not cause I/O to block because LVM does not need to break it on write.

Any place where an LVM mirror could be used, a RAID 1 mirror can be used in its place. It is possible to have LVM create RAID1 mirrors instead of regular mirrors implicitly by setting <tt>mirror_segtype_default</tt> to ''raid1'' in {{Path|lvm.conf}}.

To create a logical volume with a single mirror:

{{RootCmd|lvcreate -m 1 --type raid1 -l 40%VG --nosync -n lvm_raid1 vg0}}

Note the difference for creating a mirror: There is no ''mirrorlog'' specified, because RAID1 logical volumes do not have an explicit mirror log - it built-in to the logical volume.

It is possible to convert an existing logical volume to RAID 1:

{{RootCmd|lvconvert -m 1 --type raid1 -b vg0/lvol1}}

To remove a RAID 1 mirror, set the number of mirrors to 0:

{{RootCmd|lvconvert -m0 vg0/lvm_raid1}}

If part of the RAID1 is unavailable (usually because the disk containing the physical volume has failed), the volume group will need to be brought up in degraded mode:

{{RootCmd|vgchange -ay --partial vg0}}

Unlike an LVM mirror, writing does NOT breaking the mirroring. If the failure is only transient, and the missing physical volume returns, LVM will resync the mirror by copying cover the out-of-date segments instead of the entire logical volume. If the failure is permanent, then the failed physical volume needs to be removed from the volume group, and a replacement physical volume needs to be added (or if the volume group has a free physical volume, it can be created on a different PV). The mirror can then be repaired with ''lvconvert'', and the old physical volume can be removed from the volume group:

{{RootCmd|vgextend vg0 /dev/sdc1
|lvconvert --repair -b vg0/lvm_raid1
|vgreduce --removemissing vg0}}

==== Thin RAID1 ====

It is not (yet) possible to create a RAID 1 thin pool or thin volume. It is possible to create a RAID 1 thin pool by creating a normal mirrored logical volume and then converting the logical volume to a thin pool with <tt>lvconvert</tt>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will then merge them into a single logical volume.

{{Warning|LVM 2.02.98 or above is required for this to work properly. Prior versions are either not capable or will segfault and corrupt the VG. Also, conversion of a RAID 1 into a thin pool '''destroys''' all existing data in the mirror!}}

{{RootCmd|lvcreate -m 1 --type raid1 -l40%VG -n thin_pool vg0
|lvcreate -m 1 --type raid1 -L4MB -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg00/thin_meta}}

==== Striping with parity (RAID4 and RAID5) ====

{{Note|Striping with parity requires at least 3 physical volumes.}}

RAID 0 is not fault-tolerant - if any of the physical volumes fail then the logical volume is unusable. By adding a parity stripe to RAID 0 the logical volume can still function if a physical volume is missing. A new physical volume can then be added to restore fault tolerance.

Stripsets with parity come in 2 flavors: RAID 4 and RAID 5. Under RAID 4, all the parity stripes are stored on the same physical volume. This can become a bottleneck because all writes hit that physical volume, and it gets worse the more physical volumes are in the array. With RAID 5, the parity data is distributed evenly across the physical volumes so none of them become a bottleneck. For that reason, RAID 4 is rare and is considered obsolete/historical. In practice, all stripesets with parity are RAID 5.

{{RootCmd|lvcreate --type raid5 -l 20%VG -i 2 -n lvm_raid5 vg0}}

Only the data physical volumes are specified with -i, LVM adds one to it automatically for the parity. So for a 3 physical volume RAID5, ''-i 2'' is passed on and not ''-i 3''.

When a physical volume fails, then the volume group will need to be brought up in degraded mode:

{{RootCmd|vgchange -ay --partial vg0}}

The volume will work normally at this point, however this degrades the array to RAID 0 until a replacement physical volume is added. Performance is unlikely to be affected while the array is degraded - although it does need to recompute its missing data via parity, it only requires simple XOR for the parity block with the remaining data. The overhead is negligible compared to the disk I/O.

To repair the RAID5:

{{RootCmd|lvconvert --repair vg0/lvm_raid5
|vgreduce --removemissing vg0}}

It is possible to replace a still working physical volume in RAID5 as well:

{{RootCmd|lvconvert --replace /dev/sdb1 vg0/lvm_raid5
|vgreduce vg0 /dev/sdb1}}

The same restrictions of stripe sets apply to stripe sets with parity as well: it is not possible to enable striping with parity on an existing volume, nor reshape the stripes with parity across more/less physical volumes, nor to convert to a different RAID level/linear volume. A stripe set with parity can be mirrored. It is possible to extend a stripe set with parity across additional physical volumes, but they must be added in multiples of the original stripe set with parity (which will effectively linearly append a new stripe set with parity).

==== Thin RAID5 logical volumes ====

It is not (yet) possible to create stripe set with parity (RAID5) thin pools or thin logical volumes. It is possible to create a RAID5 thin pool by creating a normal RAID5 logical volume and then converting the logical volume into a thin pool with <tt>lvconvert</tt>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will merge them into a single logical volume.

{{Warning|LVM 2.02.98 or above is required for this to work properly. Prior versions are either not capable or will segfault and corrupt the VG. Also, coversion of a RAID5 LV into a thin pool '''destroys''' all existing data in the LV!}}

{{RootCmd|lvcreate --type raid5 -i 2  -l20%VG -n thin_pool vg0
|lvcreate --type raid5 -i 2 -L4MB -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg00/thin_meta}}

==== Striping with double parity (RAID6) ====

{{Note|RAID6 requires at least 5 physical volumes.}}

RAID 6 is similar to RAID 5, however RAID 6 can survive up to '''two''' physical volume failures, thus offering more fault tolerance than RAID5 at the expense of extra physical volumes. 

{{RootCmd|lvcreate --type raid6 -l 20%VG -i 3 -n lvm_raid6 vg00}}

Like raid5, the -i option is used to specify the number of physical volumes to stripe, excluding the 2 physical volumes for parity. So for a 5 physical volume RAID6, pass on ''-i 3'' and not ''-i 5''.

Recovery for RAID6 is the same as RAID5.

{{Note|Unlike RAID5 where parity block is cheap to recompute vs disk I/O, this is only half true in RAID6. RAID6 uses 2 parity stripes: One stripe is computed the same way as RAID5 (simple XOR). The second parity stripe is much harder to compute - see [https://www.kernel.org/pub/linux/kernel/people/hpa/raid6.pdf|raid6 (pdf)] for more information.}}

==== Thin RAID6 logical volumes ====

It is not (yet) possible to create a RAID6 thin pool or thin volumes. It is possible to create a RAID6 thin pool by creating a normal RAID6 logical volume and then converting the logical volume into a thin pool with <tt>lvconvert</tt>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will merge them into a single logical volume.

{{Warning|LVM 2.02.98 or above is required for this to work properly. Prior versions are either not capable or will segfault and corrupt the VG. Also, conversion of a RAID6 LV into a thin pool '''destroys''' all existing data in the LV!}}

{{RootCmd|lvcreate --type raid6 -i 2  -l20%VG -n thin_pool vg0
|lvcreate --type raid6 -i 2 -L4MB -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg0/thin_meta}}

==== LVM RAID10 ====

{{Note|RAID10 requires at least 4 physical volumes. Also LVM syntax requires the number of physical volumes be multiple of the numbers stripes and mirror, even though RAID10 format does not}}

RAID10 is a combination of RAID0 and RAID1. It is more powerful than RAID0+RAID1 as the mirroring is done at the stripe level instead of the logical volume level, and therefore the layout doesn't need to be symmetric. A RAID10 volume can tolerate at least a single missing physical volume, and possibly more.

{{Note|LVM currently limits RAID10 to a single mirror.}}

{{RootCmd|lvcreate --type raid10 -l 1020 -i 2 -m 1 --nosync -n lvm_raid10 vg0}}

Both the ''-i and -m'' options are specified: ''-i'' is the number of stripes and ''-m'' is the number of mirrors. Two stripes and 1 mirror requires 4 physical volumes.

==== Thin RAID10 ====

It is not (yet) possible to create a RAID10 thin pool or thin volumes. It is possible to create a RAID10 thin pool by creating a normal RAID10 logical volume and then converting the logical volume into a thin pool with <code>lvconvert</code>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will merge them into a single logical volume.
 
{{Warning|Conversion of a RAID10 logical volume into a thin pool '''destroys''' all existing data in the logical volume!}}

{{RootCmd|lvcreate -i 2 -m 1 --type raid10 -l 1012 -n thin_pool vg0
|lvcreate -i 2 -m 1 --type raid10 -l 6 -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg0/thin_meta}}

== Experimenting with LVM ==

It is possible to experiment with LVM without using real storage devices. To accomplish this, loopback devices are created.

First make sure to have the loopback module loaded. 

{{RootCmd|modprobe -r loop && modprobe loop max_part{{=}}63}}

{{Note|If loopback support is built into the kernel, then use <code>loop.max_part{{=}}63</code> as boot option.}}

Next configure LVM to not use [[udev]] to scan for devices:

{{File|/etc/lvm/lvm.conf|Disabling udev in LVM config|<pre>
obtain_device_list_from_udev = 0
</pre>}}

{{Important|This is for testing only, make sure to change the setting back when dealing with real devices since it is much faster to use udev!}}

Create some image files which will become the storage devices. The next example uses five files for a total of about ~10GB of real hard drive space:

{{RootCmd|mkdir /var/lib/lvm_img
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm0.img bs{{=}}1024 seek{{=}}2097152
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm1.img bs{{=}}1024 seek{{=}}2097152
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm2.img bs{{=}}1024 seek{{=}}2097152
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm3.img bs{{=}}1024 seek{{=}}2097152
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm4.img bs{{=}}1024 seek{{=}}2097152}}

Check which loopback devices are available:

{{RootCmd|losetup -a}}

Assuming all loopback devices are available, next create the devices:

{{RootCmd|losetup /dev/loop0 /var/lib/lvm_img/lvm0.img
|losetup /dev/loop1 /var/lib/lvm_img/lvm1.img
|losetup /dev/loop2 /var/lib/lvm_img/lvm2.img
|losetup /dev/loop3 /var/lib/lvm_img/lvm3.img
|losetup /dev/loop4 /var/lib/lvm_img/lvm4.img}}

The {{Path|/dev/loop[0-4]}} devices are now available to use as any other hard drive in the system (and thus be perfect for physical volumes).

{{Note|On the next reboot, all the loopback devices will be released and the folder {{Path|/var/lib/lvm_img}} can be deleted.}}

== Troubleshooting ==

LVM has a few features that already provide some level of redundancy. However, there are situations where it is possible to restore lost physical volumes or logical volumes.

=== vgcfgrestore utility ===

By default, on any change to a LVM physical volume, volume group, or logical volume, LVM2 create a backup file of the metadata in {{Path|/etc/lvm/archive}}. These files can be used to recover from an accidental change (like deleting the wrong logical volume). LVM also keeps a backup copy of the most recent metadata in {{Path|/etc/lvm/backup}}. These can be used to restore metadata to a replacement disk, or repair corrupted metadata.

To see what states of the volume group are available to be restored (partial output to improve readability):

{{RootCmd|vgcfgrestore --list vg00|output=<pre>
  File:		/etc/lvm/archive/vg0_00042-302371184.vg
  VG name:    	vg0
  Description:	Created *before* executing 'lvremove vg0/lvm_raid1'
  Backup Time:	Sat Jul 13 01:41:32 201
</pre>}}

==== Recovering an accidentally deleted logical volume ====

Assuming the logical volume ''lvm_raid1'' was accidentally removed from volume group ''vg0'', it is possible to recover it as follows:

{{RootCmd|vgcfgrestore -f /etc/lvm/archive/vg0_00042-302371184.vg vg0}}

{{Important|<tt>vgcfgrestore</tt> only restores LVM metadata, ''not'' the data inside the logical volume. However <code>pvremove</code>, <code>vgremove</code>, and <code>lvremove</code> only wipe metadata, leaving any data intact. If <code>issue_discards</code> is set in {{Path|/etc/lvm/lvm.conf}} though, then these command ''are'' destructive to data.}}

==== Replacing a failed physical volume ====

It possible to do a true "replace" and recreate the metadata on the new physical volume to be the same as the old physical volume:

{{RootCmd|vgdisplay --partial --verbose|output=<pre>
  --- Physical volumes ---
  PV Name               /dev/loop0     
  PV UUID               iLdp2U-GX3X-W2PY-aSlX-AVE9-7zVC-Cjr5VU
  PV Status             allocatable
  Total PE / Free PE    511 / 102
  
  PV Name               unknown device     
  PV UUID               T7bUjc-PYoO-bMqI-53vh-uxOV-xHYv-0VejBY
  PV Status             allocatable
  Total PE / Free PE    511 / 102
</pre>}}

The important line here is the UUID "unknown device". 

{{RootCmd|pvcreate --uuid T7bUjc-PYoO-bMqI-53vh-uxOV-xHYv-0VejBY --restorefile /etc/lvm/backup/vg0 /dev/loop1|output=<pre>
  Couldn't find device with uuid T7bUjc-PYoO-bMqI-53vh-uxOV-xHYv-0VejBY.
  Physical volume "/dev/loop1" successfully created</pre>}}

This recreates the physical volume metadata, but not the missing logical volume or volume group data on the physical volume.

{{RootCmd|vgcfgrestore -f /etc/lvm/backup/vg0 vg0|output=<pre>
  Restored volume group vg0
</pre>}}

This now reconstructs all the missing metadata on the physical volume, including the logical volume and volume group data. However it doesn't restore the data, so the mirror is out of sync.

{{RootCmd|vgchange -ay vg0|output=<pre>
  device-mapper: reload ioctl on  failed: Invalid argument
  1 logical volume(s) in volume group "vg0" now active
</pre>}}

{{RootCmd|lvchange --resync vg0/lvm_raid1|output=<pre>
Do you really want to deactivate logical volume lvm_raid1 to resync it? [y/n]: y
</pre>}}

This will resync the mirror. This works with RAID 4,5 and 6 as well. 

=== Deactivating a logical volume ===

It is possible to deactivate a logical volume with the following command:

{{RootCmd|umount /dev/vg0/lvol1
|lvchange -a n /dev/vg0/lvol1}}

It is not possible to mount the logical volume anywhere before it gets reactivated:

{{RootCmd|lvchange -a y /dev/vg0/lvol1}}

== Recursos externos ==

* [http://sourceware.org/lvm2/ LVM2 sourceware.org]
* [http://tldp.org/HOWTO/LVM-HOWTO/ LVM tldp.org]
* [http://sources.redhat.com/lvm2/wiki/ LVM2 Wiki redhat.com]


[[Category:Core system]]
