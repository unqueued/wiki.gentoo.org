{{InfoBox stack
|{{InfoBox wikify|header=true}}
|{{InfoBox wikipedia|Solid-state_drive}}
}}
{{Stub}}

This article [[Article description::describes how to set up SSDs ([[Wikipedia:Solid-state drive|Solid State Drives]]) on Linux.]]

It presumes the user has obtained knowledge of setting up, partitioning, and formatting [[HDD|mechanical hard drives]].

== Considerations ==

=== Rootfs ===

The {{C|mount}} <code>-o discard</code> option on a rootfs mount should ''not'' be used.

<code>discard</code> is the ''TRIM'' command that tells the SSD to do its magic. Having discard running constantly could potentially cause performance degradation on older SSDs. Modern SSDs use discard by default. Rather the following command can be used manually or be setup as a cron job (see below) to run twice a day, which should suffice for the rootfs:

{{RootCmd|fstrim -v /}}

{{Note|On a [[btrfs]] system, running the {{C|fstrim}} command on any mounted subvolume will perform the ''TRIM'' command on the device.}}

=== Mount point ===

Situations with a low amount of disk write occurring on an SSD can use the <code>discard</code> mount option in {{Path|[[fstab|/etc/fstab]]}}. In a high-write situation, such as having a database on an SSD, it is better to use TRIM commands rather than the <code>discard</code> mount option.

== After hardware installation ==

=== Partitioning ===

See [[Handbook:AMD64/Blocks/Disks|Handbook]].

==== LVM ====

[[LVM]] aligns to MiB boundaries and allows discards by default. No special configuration is required.

In order to TRIM all unused space in the VG use the {{c|blkdiscard}} utility:

{{RootCmd
|lvcreate -l100%FREE -n trim yourvg
|blkdiscard /dev/yourvg/trim
|lvremove yourvg/trim
}}

Alternatively, there is a discard option in {{Path|lvm.conf}} which makes LVM discard entire LV on lvremove, lvreduce, pvmove and other actions that free physical extents (PE) in a VG. However, enabling it will immediately render the system unable to undo any changes to the LV layout.

{{FileBox|filename=/etc/lvm/lvm.conf|1=
devices {
  issue_discards = 1
} 
}}

==== LUKS ====

For TRIM to work on encrypted [[DM-Crypt_LUKS|LUKS]] devices, they have to be opened with the <code>--allow-discards</code> option.

{{RootCmd|cryptsetup luksOpen --allow-discards /dev/thing luks}}

When root-device exists on [[DM-Crypt_LUKS|LUKS]], enabling TRIM depends on the [[Initramfs]] implementation.
When using [[genkernel]] for creating your initramfs, pass the following kernel option:

{{FileBox|filename=/etc/default/grub|1=
GRUB_CMDLINE_LINUX_DEFAULT="root_trim=yes"
}}

When using [[dracut]] for creating the initramfs, pass the following kernel option:

{{FileBox|filename=/etc/default/grub|1=
GRUB_CMDLINE_LINUX_DEFAULT="rd.luks.allow-discards"
}}


To evaluate if TRIM is enabled on a [[DM-Crypt_LUKS|LUKS]] device, you can check if the output of the following command contains the string <code>allow_discards</code>:
{{RootCmd|dmsetup table /dev/mapper/crypt_dev --showkeys}}

=== Formatting ===

If you can find your erase block size, you can add some extended attributes that may help performance.
For software raid, you really should know the erase block size. Consider this information when making your purchase.
Regardless of whether you're using RAID or not, setting extended values is known to be beneficial. https://raid.wiki.kernel.org/index.php/RAID_setup#ext2.2C_ext3.2C_and_ext4_.282011.29

http://blog.nuclex-games.com/2009/12/aligning-an-ssd-on-linux/

The modern FAQ found on the Arch wiki too:

https://wiki.archlinux.org/index.php/SSD

==== Without knowing the erase block size ====

#Formatting the rootfs partition /dev/sda3:
#Using 4096 byte blocks by default aligns the SSD for writes (see [defaults] section of /etc/mke2fs.conf and also 
#man mkfs.ext4 about "-b block-size" and "-T usage-type[,...]" inside it):

{{RootCmd|mkfs.ext4 /dev/sda3}}

==== With knowing the erase block size | this info can be outdated ====

#Formatting the rootfs partition /dev/sda3:
#Using 4096 byte blocks aligns the SSD for writes;
#Using ERASE_BLOCK_SIZE / 4 as the stride and stripe width size;
#In this example, OCZ Vertex drives have 512 kibbibyte -
#Erase Block size, therefore stride/stripe-width = 512/4 = 128:

{{RootCmd|mkfs.ext4 -E stride{{=}}128,stripe-width{{=}}128 /dev/sda3}}

===== known erase block sizes =====

* crucial m500 240G; stride and stripe width is 2048KB
{{Note|1=Page size is 16KB, there are 512 pages per Block.<ref>http://www.anandtech.com/show/6884/crucial-micron-m500-review-960gb-480gb-240gb-120gb</ref> 16KB * 512 = 8192KB for block size. Block erase size = block size. 8192KB / 4 = 2048KB for stride and stripe width size.}}
* SanDisk z400s; stride an stripe width is 4096KB
{{Note|1=According to Dutch customer care service from SanDisk the block erase size = 16MB.}}

==== blkdiscard ====

If you are formatting a previously used device and your preferred {{c|mkfs}} does not support bulk discards when creating the filesystem, you can use {{c|blkdiscard}} (from {{Package|1=sys-apps/util-linux-2.23}} or later) before creating the filesystem.

=== Mounting ===

Given the considerations above, either add something similar to this:

{{FileBox|filename=/etc/fstab|1=
/dev/sda3          /          ext4          defaults,relatime,discard          0 1
}}

Or:

{{FileBox|filename=/etc/fstab|1=
/dev/sda3          /          ext4          defaults,relatime                  0 1
}}

When choosing the latter, see the [[SSD#Cron|cron]] section below to automate TRIM on the drive.

Once {{Path|/etc/fstab}} can been modified, run the following command to have the drive mounted:

{{RootCmd|mount -a}}

In the same way, the <code>discard</code> mount option can be added to the swap line in fstab too. The following example shows SSD disk swap can be parallelized with SATA disk swap:

{{FileBox|filename=/etc/fstab|1=
/dev/sda2          none           swap         sw,pri=3,discard          0 0
/dev/sdb2          none           swap         sw,pri=3                  0 0
}}

Once the swap has been configured in fstab, run the following command to make swap available:

{{RootCmd|swapon -a}}

== Misc ==

=== Periodic fstrim jobs ===

There are multiple ways how to setup a periodic unused block discarding process.

==== cron ====

Run {{c|fstrim -v /}} from cron twice a day to automatically perform "discard":

   #Mins  Hours  Days   Months  Day of the week   command
   15     1,13   *      *       *                 /sbin/fstrim -v /

There is also a semi-automatic cronjob available on GitHub called [http://chmatse.github.io/SSDcronTRIM/ SSDcronTRIM] which has the following features:

* Distribution Independent script (developed on my Gentoo system).
* The script decides every time depending on the disk usage how often (monthly, weekly, daily, hourly) each partition has to be trimmed.
* Recognizes if it should install itself into /etc/cron.{monthly,weekly,daily,hourly}, {{Path|/etc/cron.d}} or any other defined directory and if it should make an entry into crontab.
* Checks if the kernel meets the requirements, the filesystem is able to and if the SSD supports trimming.
* Simply install it by running it once without any option and uninstall it with the <code>-d</code> option

Future version should implement:

* Use of {{c|nice}} and {{c|ionice}} to let trimm only run when the disc is not busy.
* Overgive options to fstrim (e.g. '-m 1M')
* support for encrypted filesystem (LUKS)

==== fstrimDaemon ====

If your computer turned off when cron scheduled its job, {{C|fstrim}} would not be called at all. You can install [https://github.com/dobek/fstrimDaemon fstrimDaemon] to solve this problem.

==== systemd timer ====

When running a system with [[systemd]] version 212 or newer, a persistent [[systemd#Timer services|systemd timer]] can be created that will run {{C|fstrim}} every 12 hours.

Two systemd unit files need to be created in the {{Path|/etc/systemd/system}} directory:

Service called <code>fstrim</code> which actually executes the {{C|fstrim}}:
{{FileBox|filename=/etc/systemd/system/fstrim.service|lang=ini|title=Service executing fstrim|1=
[Unit]
Description=Runs fstrim on all mounted devices that support TRIM

[Service]
Type=oneshot
ExecStart=/bin/sh -c '/sbin/fstrim -a'
}}

Timer which wakes up the <code>fstrim</code> service every 12 hours:
{{FileBox|filename=/etc/systemd/system/fstrim.timer|lang=ini|title=Timer starting the fstrim service|1=
[Unit]
Description=Run fstrim.service every 12 hours

[Timer]
OnUnitInactiveSec=12h
Persistent=true

[Install]
WantedBy=multi-user.target
}}

Make sure the permissions are correct:

{{RootCmd|chmod 644 /etc/systemd/system/fstrim.*}}

Tell systemd to reload its unit files, then enable it:

{{RootCmd
|systemctl daemon-reload
|systemctl enable fstrim.timer
}}

It is now possible to see if it has been run and when the next time it will be ran by using list-timers:

{{RootCmd|systemctl list-timers|output=<pre>
NEXT                         LEFT     LAST                         PASSED    UNIT                         ACTIVATES
Sun 2017-01-15 07:07:48 PST  11h left Sat 2017-01-14 19:03:26 PST  25min ago fstrim.timer                 fstrim.service
Sun 2017-01-15 19:16:26 PST  23h left Sat 2017-01-14 19:16:26 PST  12min ago systemd-tmpfiles-clean.timer systemd-tmpfiles-clean.service
</pre>}}

This will run fstrim twice a day, and also run it if it was missed. The {{c|journalctl}} command can be used to make sure the timer runs successfully.

=== Reducing amount of writes ===

The flash-based SSDs have a limited write lifetime - the number of writes performed<ref>[https://www.dell.com/support/article/cz/cs/czdhs1/sln156899/hard-drive-why-do-solid-state-devices-ssd-wear-out?lang=en Hard Drive - Why Do Solid State Devices (SSD) Wear Out], Dell. Retrieved on October 28, 2018</ref>. Thus when using a SSD, administrators generally want to reduce the amount of writes.

==== Portage <var>TMPDIR</var> on tmpfs ====

When building packages via [[Portage]] it is possible to perform the operations on [[Tmpfs|tmpfs]] and get the tmpfs' benefits. See [[Portage TMPDIR on tmpfs|Portage <var>TMPDIR</var> on tmpfs]] guide.

==== Temporal files on tmpfs ====

{{Warning|Remember that all data in [[Tmpfs|tmpfs]] reside in volatile memory. So data on tmpfs will be lost after system reboot, shutdown or crash!}}

It is possible to mount desired mount points as [[Tmpfs|tmpfs]]. Since tmpfs stores files in volatile memory all the I/O operations directed to the given mount points are not performed on the solid state disk. This reduces the amount of writes and also improves performance.

This is an example of both {{Path|/tmp}} and {{Path|/var/tmp}} being mounted as tmpfs:

{{FileBox|filename=[[fstab|/etc/fstab]]|1=# temporal mountpoints on tmpfs
tmpfs           /tmp            tmpfs           size=16G,noatime        0 0
tmpfs           /var/tmp        tmpfs           size=1G,noatime         0 0}}


==== XDG cache on tmpfs ====

When running a Gentoo desktop, many programs, using [[X]] Window System ([[Chromium]], [[Firefox]], [[Skype]], etc.) are making frequent disk I/O every few seconds to cache<ref>[https://www.servethehome.com/firefox-is-eating-your-ssd-here-is-how-to-fix-it/ Firefox is eating your SSD - here is how to fix it], Loyolan Ventures. Retrieved on October 28, 2018</ref>.

The cache directory location usually complies to ''XDG Base Directory Specification''<ref>[https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html XDG Base Directory Specification], freedesktop.org. Retrieved on October 28, 2018</ref>, namely to the <var>XDG_CACHE_HOME</var> environment variable. The default cache location is {{Path|~/.cache}}, which is usually mounted on a hard drive and could be moved to [[tmpfs]].

To remap the cache directory location create file {{Path|/etc/profile.d/xdg_cache_home.sh}}:

{{FileBox|filename=/etc/profile.d/xdg_cache_home.sh|lang=bash|1=
export XDG_CACHE_HOME="/tmp/${USER}/.cache"
}}

{{Warning|
With the above change, auto unlocking the gnome keyring didn't work anymore.
/var/log/messages said the following:

   May  5 10:02:52 localhost gnome-keyring-daemon[5561]: couldn't bind to control socket: /home/david/.cache/keyring-hRt5QC/control: No such file or directory
}}

==== Web browser profile/s and cache on tmpfs ====

The web browser profile/s, cache, etc. can be relocated to [[Tmpfs|tmpfs]]. The corresponding I/O associated with using the browser gets redirected from the SSD drive to tmpfs' volatile memory, resulting in reduced wear to the physical drive and also improving browser speed and responsiveness.

You can relocate the browser components mentioned above with the utility {{Package|www-misc/profile-sync-daemon}}:

{{RootCmd|emerge --ask www-misc/profile-sync-daemon}}

{{Note|Note {{Package|www-misc/profile-sync-daemon}} version 6 or greater requires [[Systemd|systemd]].}}

Next add the users whose browser/s profile/s will get symlinked to a tmpfs or another mountpoint in the variable <code>USERS</code>:

{{FileBox|filename=/etc/psd.conf|1= USERS="user user2 root"}}

Finally, close all the browsers, start and enable the daemon.

On systemd:
{{RootCmd|systemctl enable psd && systemctl start psd}}

On [[OpenRC]]:
{{RootCmd|rc-update add psd default && rc-service psd start}}

Now it is possible to view all symlinks by printing the status of the started daemon:

{{RootCmd|psd p}}

More info about Profile-Sync-Daemon can be found on [https://wiki.archlinux.org/index.php/profile-sync-daemon Arch's wiki].

== See also ==

* {{See also|HDD}}
* [[NVMe]] - Non-Volatile Memory Express.

== References ==

{{Reflist}}

[[Category:Storage devices]]
