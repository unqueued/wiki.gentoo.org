'''InfiniBand''' is a switched fabric communications link used in high-performance computing and enterprise data centers.
Its features include high throughput, low latency, quality of service and failover, and it is designed to be scalable. The InfiniBand architecture specification defines a connection between processor nodes and high performance I/O nodes such as storage devices.

== Installation ==

=== Kernel ===

You need to activate the following kernel options:
{{Kernel||<pre>
Device Drivers  --->
    InfiniBand support --->
        <*> InfiniBand userspace MAD support
        <*> InfiniBand userspace access (verbs and CM)
        <M> Mellanox HCA support
        <M> QLogic PCIe HCA support
        <M> Ammasso 1100 HCA support
        <M> Mellanox ConnectX HCA support
        <M> NetEffect RNIC Driver
        <M> Emulex One Connect HCA support
        <M> IP-over-InfiniBand
            [*] IP-over-InfiniBand Connected Mode support
        <M>   InfiniBand SCSI RDMA Protocol
</pre>}}

{{Note|You probably only need userspace api (first two options) and drivers needed for your hardware, and may be ip-over-ib or srp modules if you plan to use it}}


=== Software ===
