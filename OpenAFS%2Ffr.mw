<languages />

Ce guide explique comment installer un serveur et un cilent OpenAFS sur Gentoo Linux.

== Vue d'ensemble ==

=== À propos de ce document ===

Ce document vous guide à travers toutes les étapes nécessaires pour installer un serveur OpenAFS sur Gentoo Linux. Certaines parties de ce document sont issues de la FAQ AFS et du ''AFS Quick Beginnings guide'' d'IBM. Il ne sert à rien de réinventer la roue. 

=== Qu'est-ce qu'AFS ? ===

AFS est un système de fichiers distribué qui permet à des  hôtes coopérants (clients et serveurs) de partager efficacement des ressources de systèmes de fichiers à travers à la fois, le réseau local et des réseaux étendus. Les clients détiennent un cache pour les objets d'usage fréquent (fichiers), pour accélérer leur accès à ces derniers. 

AFS est basé sur un système de fichiers distribué,  développé à l'origine par l'''Information Technology Center de l'université'' de Carnegie-Mellon, appelé ''Andrew File System''. ''Andrew'' était le nom d'un projet de recherche de l'université de Carnégie-Mellon, en l'honneur de ses  fondateurs. Une fois que ''Transarc'' fut créé et que AFS devint un produit, le ''Andrew'' fut abandonné pour indiquer que AFS avait dépassé les limites du projet et était devenu un système de fichiers de qualité et un produit exportable. Néanmoins, il y avait un nombre de cellules existantes qui utilisaient /afs comme racine de leur système de fichiers. À cette époque, changer la racine d'un système de fichiers était loin d'être une entreprise triviale. Aussi, pour éviter aux  premiers site AFS d'avoir à renommer leur systèmes de fichiers, AFS fut retenu, à la fois,  comme le nom du système de fichier  et comme le nom de  sa racine. 

=== Qu'est-ce qu'une cellule  AFS ? ===

Une cellule AFS est un collection de serveurs administrativement regroupés, qui présente un système de fichiers unique et cohésif. Typiquement, une cellule AFS est un ensemble d'hôtes qui utilisent le même nom de domaine (par exemple gentoo.org). Les utilisateurs se connectent par des stations de travail clientes AFS qui demandent des informations et des fichiers à une cellules de serveurs au nom des utilisateurs. Les utilisateurs ne savent pas sur quel serveur le fichier auquel ils accèdent se trouve. Il ne se rendent même pas compte si un serveur est situé dans la pièce d'à coté dans la mesure ou chacun des volumes peut être  répliqué et déplacé vers un autre serveur sans que les utilisateurs en soient avertis. Les fichiers sont toujours accessibles. 

=== Quels sont les bénéfices de l'utilisation  d'AFS ? ===

Les principales forces de AFS sont : ses possibilités de cache ( du coté client, typiquement de 100 Mo à 1 Go), ses fonctionnalités de sécurité (basées sur Kerberos 4, listes de contrôle d'accès), sa simplicité d'adressage (vous n'avez qu'un seul système de fichiers), sa modulabilité (vous pouvez rajoute des serveurs à votre cellule en fonction de vos besoins), ses protocoles de communication. 

=== Où puis-je trouver plus d'information ? ===

Lisez les [http://www.angelfire.com/hi/plutonic/afs-faq.html FAQ AFS] . 

La page d'accueil de OpenAFS est à  [http://www.openafs.org www.openafs.org] . 

AFS a été développé à l'origine par Transarc qui est maintenant détenu par IBM. Depuis avril 2005, il a été retiré du catalogue des produits d'IBM. 

=== Comment résoudre les problèmes ? ===

OpenAFS possède des fonctions de journalisation importantes. Cependant, par défaut, il tient ses propres journaux au lieu d'utiliser les fonctions de journalisation de votre système. Pour que le serveur utilise les fonctions de journalisation du  système,  utilisez l'option <code>-syslog</code> pour toutes les commandes <code>bos</code>. 

== Mise à jour depuis une versions antérieure ==

=== Introduction ===

Cette section vous guide dans le processus de mise à jour d'une installation OpenAFS existante vers une version d'OpenAFS 1.4.0 ou postérieure (ou 1.2.x partant de 1.2.13. Cette dernière ne sera pas prise en compte spécifiquement car la majorité des gens désireront 1.4 pour disposer d'une prise en charge de a.o.linux-2.6, de fichiers de grande taille et de la résolution des bogues). 

Si vous disposez d'une installation fraîche de la version 1.4 d'OpenAFS, vous pouvez sans risque sauter ce chapitre. Néanmoins, si vous mettez à jour depuis une version antérieure, nous vous recommandons fortement de suivre les conseils des sections suivantes. Le script de transition dans l'ebuild est conçu pour vous assister dans la mise à jour rapide et le redémarrage. Notez que :il n'effacera pas (pour des raisons de sécurité) les fichiers de configuration et les scripts de démarrage aux anciens emplacements; ne changera pas automatiquement  votre configuration de démarrage pour utiliser les nouveaux scripts, etc. Si vous n'êtes pas encore convaincu, l'utilisation d'un ancien module du noyau OpenAFS avec le système de binaires à jour, peut rendre votre noyau imprévisible. C'est pourquoi vous devez continuer à lire pour une transition claire et facile. 


{{Note|Ce chapitre a été écrit en tenant compte de différentes configurations du système. Néanmoins, il est toujours possible que, en fonction des ajustements spécifiques qu'un utilisateur a pu apporter à sa configuration, sa situtation particulière ne soit pas décrite ici. Un utilisateur suffisamment assuré pour modifier sa configuration devrait être assez expérimenté pour mettre en œuvre les observations émises quand c'est approprié. À l'inverse, un utilisateur qui a très peu modifié son système en dehors de l'installation de l'ebuild précédent, pourra ignorer la plupart des avertissements donnés par la suite.}}


=== Différences avec les versions précédentes ===

Traditionnellement, OpenAFS a utilisé les mêmes conventions de chemin qu'IBM TransArc labs, jusqu'à ce que le code choisisse un embranchement différent. C'est compréhensible que la configuration de l'ancien AFS continue à utiliser ces conventions héritées. Des configuration plus récentes se conforment à FHS en utilisant des emplacements standards (comme dans beaucoup de distributions Linux). La table suivante est une compilation du script ''configure'' et du fichier README qui accompagne l'archive de distribution d'OpenAFS : 

{| class="wikitable" style="text-align: left;" 
|- 
! Répertoire
! But
! Mode Transarc
! Mode par défaut
! traduction pour  Gentoo
|- 
| viceetcdir
| Configuration du client
| /usr/vice/etc
| $(sysconfdir)/openafs
| /etc/openafs
|- 
| unnamed
| Binaires client 
| unspecified
| $(bindir)
| /usr/bin
|- 
| afsconfdir
| Configuration du serveur
| /usr/afs/etc
| $(sysconfdir)/openafs/server
| /etc/openafs/server
|- 
| afssrvdir
| Binaires serveur internes
| /usr/afs/bin (servers)
| $(libexecdir)/openafs
| /usr/libexec/openafs
|- 
| afslocaldir
| État du serveur
| /usr/afs/local
| $(localstatedir)/openafs
| /var/lib/openafs
|- 
| afsdbdir
| Aut/liste des serveurs/... bases de données
| /usr/afs/db
| $(localstatedir)/openafs/db
| /var/lib/openafs/db
|- 
| afslogdir
| Fichiers de journalisation
| /usr/afs/logs
| $(localstatedir)/openafs/logs
| /var/lib/openafs/logs
|- 
| afsbosconfig
| Configuration supervisatrice
| $(afslocaldir)/BosConfig
| $(afsconfdir)/BosConfig
| /etc/openafs/BosConfig
|-
|}

Il y a quelques autres curiosités, comme les binaires placés dans {{Path|/usr/vice/etc}}  pour le mode Transarc, mais cette liste ne prétend pas être exhaustive. Elle est plutôt destinée à servir de référence pour la transition créatrice de dysfonctionnements de ces fichiers de configuration. 

En conséquence des changements de chemin, l'emplacement par défaut du cache de  disque a également été changé de {{Path|/usr/vice/cache}} en {{Path|/var/cache/openafs}} . 

Plus encore, le script d'initialisation a été éclaté en une partie ''client'' et une partie ''serveur''. Vous aviez pour habitude d'avoir {{Path|/etc/init.d/afs}}, mais maintenant vous avez finalement {{Path|/etc/init.d/openafs-client}} et {{Path|/etc/init.d/openafs-server}}. Par conséquent, le fichier de configuration {{Path|/etc/conf.d/afs}} a été éclaté en {{Path|/etc/conf.d/openafs-client}} et {{Path|/etc/conf.d/openafs-server}}. Les options dans {{Path|/etc/conf.d/openafs}} pour arrêter ou démarrer le client ou le serveur sont également devenues obsolètes. 

Un autre changement au script d'initialisation, c'est qu'il ne vérifie plus la configuration de votre cache de disque. L'ancien code requérait qu'une partition ext2 séparée soit montée sur {{Path|/usr/vice/cache}}. Cela occasionnait quelques problèmes : 

* Bien qu'il s'agisse d'une configuration très logique, votre cache n'a pas besoin d'être sur une partition séparée. Tant que vous assurez que la quantité de place spécifiée dans {{Path|/etc/openafs/cacheinfo}} est réellement disponible pour le cache de disque, tout va bien. Ainsi, il n'y a réellement pas de problème à ce que le cache se trouve sur votre partition ''root''. 
* Quelques personnes utilisent des liens symboliques pour pointe vers le cache de disque réel. Le script d'initialisation n'aime pas ça, parce qu'alors, l'emplacement de ce cache n'apparaît pas dans {{Path|/proc/mounts}} .
* De nos jours, beaucoup préfèrent ext3 à ext2. Les deux systèmes de fichiers sont valides pour une utilisation en cache de disque. Tout autre système de fichiers n'est pas pris en charge (par exemple, n'essayez pas reiserfs, vous obtiendriez un énorme avertissement , et pourriez vous attendre à des plantages par la suite).

=== Transition vers les nouveaux chemins  ===

Tout d'abord, l'installation d'une nouvelle version d'OpenAFS n'écraserait pas vous anciens fichiers de configuration. Le script est conçu pour ne  remplacer aucun fichier déjà présent sur le système. C'est pourquoi, même si vous avez une configuration complètement anarchique, avec un mélange d'anciens et de nouveaux emplacements, le script ne créera pas de problèmes supplémentaires. Par ailleurs, si un serveur OpenAFS en service est détecté, l'installation avortera, empêchant ainsi une possible corruption de base de données. 

Une mise en garde cependant -- il y a eu des ebuilds traînant sur Internet qui désactivent partiellement la protection que Gentoo place sur {{Path|/etc}}. Ces ebuils n'ont jamais été distribués par Gentoo. Vous devriez vérifier la variable <code>CONFIG_PROTECT_MASK</code>  dans la sortie de la commande suivante : 

{{RootCmd|emerge info {{!}} grep "CONFIG_PROTECT_MASK"|output=<pre>
CONFIG_PROTECT_MASK="/etc/gconf /etc/terminfo /etc/texmf/web2c /etc/env.d"
</pre>
}}

Bien que rien dans cet ebuild ne toucherait les fichiers dans  {{Path|/etc/afs}} , la mise à jour causerait la suppression de votre ancienne installation d'OpenAFS. Les fichiers dans <code>CONFIG_PROTECT_MASK</code> qui appartiennent à l'ancienne installation seraient supprimés également. 

Il devrait être clair à l'utilisateur expérimenté que, s'il a mis au point son système en ajoutant les liens symboliques à la main (par exemple  {{Path|/usr/afs/etc}} vers {{Path|/etc/openafs}} ), la nouvelle installation peut continuer à bien fonctionner en continuant à utiliser les anciens fichiers de configuration. Dans ce cas, il n'y aurait pas eu de réelle transition, et le nettoyage de l'ancienne installation résulterait dans une configuration cassée d'OpenAFS. 

Maintenant que vous savez ce qui ne se produira pas, vous voulez savoir quoi faire : 

* {{Path|/usr/afs/etc}} is copied to {{Path|/etc/openafs/server}} 
* {{Path|/usr/vice/etc}} is copied to {{Path|/etc/openafs}} 
* {{Path|/usr/afs/local}} is copied to {{Path|/var/lib/openafs}} 
* {{Path|/usr/afs/local/BosConfig}} is copied to {{Path|/etc/openafs/BosConfig}} , while replacing occurrences of {{Path|/usr/afs/bin/}} with {{Path|/usr/libexec/openafs}} , {{Path|/usr/afs/etc}} with {{Path|/etc/openafs/server}} and {{Path|/usr/afs/bin}} (without the / as previously) with {{Path|/usr/bin}} 
* {{Path|/usr/afs/db}} is copied to {{Path|/var/lib/openafs/db}} 
* The configuration file {{Path|/etc/conf.d/afs}} is copied to {{Path|/etc/conf.d/openafs-client}} , as all known old options were destined for client usage only.

=== La mise à jour elle-même ===

So you haven't got an OpenAFS server setup? Or maybe you do, the previous sections have informed you about what is going to happen, and you're still ready for it? 

Let's go ahead with it then! 

If you do have a server running, you want to shut it down now. 

{{RootCmd|/etc/init.d/afs stop}}

And then the upgrade itself. 

{{Emerge|openafs}}

=== Restarting OpenAFS ===

If you had an OpenAFS server running, you would have not have been forced to shut it down. Now is the time to do that. 

{{RootCmd|/etc/init.d/afs stop}}

As you may want keep the downtime to a minimum, so you can restart your OpenAFS server right away. 

{{RootCmd|/etc/init.d/openafs-server start}}

You can check whether it's running properly with the following command: 

{{RootCmd|/usr/bin/bos status localhost -localauth}}

Before starting the OpenAFS client again, please take time to check your cache settings. They are determined by {{Path|/etc/openafs/cacheinfo}} . To restart your OpenAFS client installation, please type the following: 

{{RootCmd|/etc/init.d/openafs-client start}}

=== Cleaning up afterwards ===

Before cleaning up, please make really sure that everything runs smoothly and that you have restarted after the upgrade (otherwise, you may still be running your old installation). 

{{Important|Please make sure you're not using {{Path|/usr/vice/cache}} for disk cache if you are deleting {{Path|/usr/vice}} !!}}

The following directories may be safely removed from the system: 

*  {{Path|/etc/afs}} 
*  {{Path|/usr/vice}} 
*  {{Path|/usr/afs}} 
*  {{Path|/usr/afsws}} 

The following files are also unnecessary: 

*  {{Path|/etc/init.d/afs}} 
*  {{Path|/etc/conf.d/afs}} 

{{RootCmd|tar czf /root/oldafs-backup.tgz /etc/afs /usr/vice /usr/afs /usr/afsws
|rm -R /etc/afs /usr/vice /usr/afs /usr/afsws
|rm /etc/init.d/afs /etc/conf.d/afs}}

In case you've previously used ebuilds =openafs-1.2.13 or =openafs-1.3.85, you may also have some other unnecessary files: 

* {{Path|/etc/init.d/afs-client}} 
* {{Path|/etc/init.d/afs-server}} 
* {{Path|/etc/conf.d/afs-client}} 
* {{Path|/etc/conf.d/afs-server}} 

=== Init Script changes ===

Now most people would have their systems configured to automatically start the OpenAFS client and server on startup. Those who don't can safely skip this section. If you had your system configured to start them automatically, you will need to re-enable this, because the names of the init scripts have changed. 

{{RootCmd|rc-update del afs default
|rc-update add openafs-client default
|rc-update add openafs-server default}}

If you had <code>=openafs-1.2.13</code> or <code>=openafs-1.3.85</code> , you should remove {{Path|afs-client}} and {{Path|afs-server}} from the default runlevel, instead of {{Path|afs}} . 

=== Troubleshooting: what if the automatic upgrade fails ===

Don't panic. You shouldn't have lost any data or configuration files. So let's analyze the situation. Please file a bug at [http://bugs.gentoo.org bugs.gentoo.org] in any case, preferably with as much information as possible. 

If you're having problems starting the client, this should help you diagnosing the problem: 

* Run <code>dmesg</code> . The client normally sends error messages there.
* Check {{Path|/etc/openafs/cacheinfo}} . It should be of the form: /afs:{path to disk cache}:{number of blocks for disk cache}. Normally, your disk cache will be located at {{Path|/var/cache/openafs}} .
* Check the output of <code>lsmod</code> . You will want to see a line beginning with the word openafs.
* <code>pgrep afsd</code> will tell you whether afsd is running or not
* <code>cat /proc/mounts</code> should reveal whether {{Path|/afs}} has been mounted.

If you're having problems starting the server, then these hints may be useful: 

* <code>pgrep bosserver</code> tells you whether the overseer is running or not. If you have more than one overseer running, then something has gone wrong. In that case, you should try a graceful OpenAFS server shutdown with <code>bos shutdown localhost -localauth -wait</code> , check the result with <code>bos status localhost -localauth</code> , kill all remaining overseer processes and then finally check whether any server processes are still running ( <code>ls /usr/libexec/openafs</code> to get a list of them). Afterwards, do <code>/etc/init.d/openafs-server zap</code> to reset the status of the server and <code>/etc/init.d/openafs-server start</code> to try launching it again.
* If you're using OpenAFS' own logging system (which is the default setting), check out {{Path|/var/lib/openafs/logs/*}} . If you're using the syslog service, go check out its logs for any useful information.

== Documentation ==

=== Getting AFS Documentation ===

You can get the original IBM AFS Documentation. It is very well written and you really want read it if it is up to you to administer a AFS Server. 

{{Emerge|app-doc/afsdoc}}

You also have the option of using the documentation delivered with OpenAFS. It is installed when you have the USE flag <code>doc</code> enabled while emerging OpenAFS. It can be found in {{Path|/usr/share/doc/openafs-*/}} . At the time of writing, this documentation was a work in progress. It may however document newer features in OpenAFS that aren't described in the original IBM AFS Documentation. 

== Client Installation ==

=== Building the Client ===

{{Emerge|net-fs/openafs}}

After successful compilation you're ready to go. 

=== A simple global-browsing client installation ===

If you're not part of a specific OpenAFS-cell you want to access, and you just want to try browsing globally available OpenAFS-shares, then you can just install OpenAFS, not touch the configuration at all, and start {{Path|/etc/init.d/openafs-client}} .

=== Accessing a specific OpenAFS cell ===

If you need to access a specific cell, say your university's or company's own cell, then some adjustments to your configuration have to be made. 

Firstly, you need to update {{Path|/etc/openafs/CellServDB}} with the database servers for your cell. This information is normally provided by your administrator. 

Secondly, in order to be able to log onto the OpenAFS cell, you need to specify its name in {{Path|/etc/openafs/ThisCell}} . 

{{Code|Adjusting CellServDB and ThisCell|<pre>
CellServDB:
>netlabs        #Cell name
10.0.0.1        #storage
  
ThisCell:
netlabs
</pre>
}}


{{Warning|Only use spaces inside the {{Path|CellServDB}} file. The client will most likely fail if you use TABs.}}

CellServDB tells your client which server(s) it needs to contact for a specific cell. ThisCell should be quite obvious. Normally you use a name which is unique for your organisation. Your (official) domain might be a good choice. 

For a quick start, you can now start {{Path|/etc/init.d/openafs-client}} and use <code>klog</code> to authenticate yourself and start using your access to the cell. For automatic logons to you cell, you want to consult the appropriate section below. 

=== Adjusting the cache ===

{{Note|Unfortunately the AFS Client needs a ext2/3 filesystem for its cache to run correctly. There are some issues when using other filesystems (using e.g. reiserfs is not a good idea).}}

You can house your cache on an existing filesystem (if it's ext2/3), or you may want to have a separate partition for that. The default location of the cache is {{Path|/var/cache/openafs}} , but you can change that by editing {{Path|/etc/openafs/cacheinfo}} . A standard size for your cache is 200MB, but more won't hurt. 

=== Starting AFS on startup ===

The following command will create the appropriate links to start your afs client on system startup. 

{{Warning|You should always have a running afs server in your domain when trying to start the afs client. Your system won't boot until it gets some timeout if your AFS server is down (and this is quite a long long time.)}}

{{RootCmd|rc-update add openafs-client default}}

== Server Installation ==

=== Building the Server ===

{{Note|All commands should be written in one line!! In this document they are sometimes wrapped to two lines to make them easier to read.}}

If you haven't already done so, the following command will install all necessary binaries for setting up an AFS Server ''and'' Client. 

{{Emerge|net-fs/openafs}}

=== Starting AFS Server ===

You need to run the <code>bosserver</code> command to initialize the Basic OverSeer (BOS) Server, which monitors and controls other AFS server processes on its server machine. Think of it as init for the system. Include the <code>-noauth</code> flag to disable authorization checking, since you haven't added the admin user yet. 

{{Warning|Disabling authorization checking gravely compromises cell security. You must complete all subsequent steps in one uninterrupted pass and must not leave the machine unattended until you restart the BOS Server with authorization checking enabled. Well, this is what the AFS documentation says. :)}}

{{RootCmd|bosserver -noauth &}}

Verify that the BOS Server created {{Path|/etc/openafs/server/CellServDB}} and {{Path|/etc/openafs/server/ThisCell}}  

{{RootCmd|ls -al /etc/openafs/server/|output=<pre>
-rw-r--r--    1 root     root           41 Jun  4 22:21 CellServDB
-rw-r--r--    1 root     root            7 Jun  4 22:21 ThisCell
</pre>
}}

=== Defining Cell Name and Membership for Server Process ===

Now assign your cell's name. 

{{Important|There are some restrictions on the name format. Two of the most important restrictions are that the name cannot include uppercase letters or more than 64 characters. Remember that your cell name will show up under {{Path|/afs}} , so you might want to choose a short one.}}

{{Note|In the following and every instruction in this guide, for the SERVER_NAME argument substitute the full-qualified hostname (such as '''afs.gentoo.org''' ) of the machine you are installing. For the CELL_NAME argument substitute your cell's complete name (such as '''gentoo''' )}}

Run the <code>bos setcellname</code> command to set the cell name: 

{{RootCmd|bos setcellname SERVER_NAME CELL_NAME -noauth}}

=== Starting the Database Server Process ===

Next use the <code>bos create</code> command to create entries for the four database server processes in the {{Path|/etc/openafs/BosConfig}} file. The four processes run on database server machines only. 

{| class="wikitable" style="text-align: left;" 
|- 
| kaserver
| The Authentication Server maintains the Authentication Database. This can be replaced by a Kerberos 5 daemon. If anybody wants to try that feel free to update this document :)
|- 
| buserver
| The Backup Server maintains the Backup Database
|- 
| ptserver
| The Protection Server maintains the Protection Database
|- 
| vlserver
| The Volume Location Server maintains the Volume Location Database (VLDB). Very important :)
|-
|}

{{RootCmd|bos create SERVER_NAME kaserver simple /usr/libexec/openafs/kaserver -cell CELL_NAME -noauth
|bos create SERVER_NAME buserver simple /usr/libexec/openafs/buserver -cell CELL_NAME -noauth
|bos create SERVER_NAME ptserver simple /usr/libexec/openafs/ptserver -cell CELL_NAME -noauth
|bos create SERVER_NAME vlserver simple /usr/libexec/openafs/vlserver -cell CELL_NAME -noauth}}

You can verify that all servers are running with the <code>bos status</code> command: 

{{RootCmd|bos status SERVER_NAME -noauth|output=<pre>
Instance kaserver, currently running normally.
Instance buserver, currently running normally.
Instance ptserver, currently running normally.
Instance vlserver, currently running normally.
</pre>
}}

=== Initializing Cell Security ===

Now we'll initialize the cell's security mechanisms. We'll begin by creating the following two initial entries in the Authentication Database: The main administrative account, called '''admin''' by convention and an entry for the AFS server processes, called <code>afs</code> . No user logs in under the identity '''afs''' , but the Authentication Server's Ticket Granting Service (TGS) module uses the account to encrypt the server tickets that it grants to AFS clients. This sounds pretty much like Kerberos :) 

Enter <code>kas</code> interactive mode 

{{RootCmd|kas -cell CELL_NAME -noauth}}

{{RootCmd|create afs|prompt=ka&gt; |output=<pre>
initial_password:
Verifying, please re-enter initial_password:
</pre>}}

{{RootCmd|create admin|prompt=ka&gt; |output=<pre>
initial_password:
Verifying, please re-enter initial_password:
</pre>}}

{{RootCmd|examine afs|prompt=ka&gt; |output=<pre>
User data for afs
key (0) cksum is 2651715259, last cpw: Mon Jun  4 20:49:30 2001
password will never expire.
An unlimited number of unsuccessful authentications is permitted.
entry never expires.  Max ticket lifetime 100.00 hours.
last mod on Mon Jun  4 20:49:30 2001 by <none>
permit password reuse
</pre>}}

{{RootCmd|setfields admin -flags admin|prompt=ka&gt; }}
{{RootCmd|examine admin|prompt=ka&gt; |output=<pre>
User data for admin (ADMIN)
key (0) cksum is 2651715259, last cpw: Mon Jun  4 20:49:59 2001
password will never expire.
An unlimited number of unsuccessful authentications is permitted.
entry never expires.  Max ticket lifetime 25.00 hours.
last mod on Mon Jun  4 20:51:10 2001 by <none>
permit password reuse
</pre>
}}

Run the <code>bos adduser</code> command, to add the '''admin''' user to the {{Path|/etc/openafs/server/UserList}} . 

{{RootCmd|bos adduser SERVER_NAME admin -cell CELL_NAME -noauth}}

Issue the <code>bos addkey</code> command to define the AFS Server encryption key in {{Path|/etc/openafs/server/KeyFile}}  

{{Note|If asked for the input key, give the password you entered when creating the AFS entry with <code>kas</code>}}

{{RootCmd|bos addkey SERVER_NAME -kvno 0 -cell CELL_NAME -noauth|output=<pre>
input key:
Retype input key:
</pre>
}}

Issue the <code>pts createuser</code> command to create a Protection Database entry for the admin user. 

{{Note|By default, the Protection Server assigns AFS UID 1 to the '''admin''' user, because it is the first user entry you are creating. If the local password file ( {{Path|/etc/passwd}} or equivalent) already has an entry for '''admin''' that assigns a different UID use the <code>-id</code> argument to create matching UIDs.}}

{{RootCmd|pts createuser -name admin -cell CELL_NAME [-id AFS_UID] -noauth}}

Issue the <code>pts adduser</code> command to make the '''admin''' user a member of the system:administrators group, and the <code>pts membership</code> command to verify the new membership 

{{RootCmd|pts adduser admin system:administrators -cell CELL_NAME -noauth
|pts membership admin -cell CELL_NAME -noauth|output=<pre>
Groups admin (id: 1) is a member of:
system:administrators
</pre>
}}

=== Properly (re-)starting the AFS server ===

At this moment, proper authentication is possible, and the OpenAFS server can be started in a normal fashion. Note that authentication also requires a running OpenAFS client (setting it up is described in the previous chapter). 

{{RootCmd|bos shutdown SERVER_NAME -wait -noauth
|killall bosserver}}

{{RootCmd|/etc/init.d/openafs-server start
|/etc/init.d/openafs-client start}}

{{RootCmd|rc-update add openafs-server default}}

{{RootCmd|klog admin}}

=== Starting the File Server, Volume Server and Salvager ===

Start the <code>fs</code> process, which consists of the File Server, Volume Server and Salvager (fileserver, volserver and salvager processes). 

{{RootCmd|bos create SERVER_NAME fs fs /usr/libexec/openafs/fileserver /usr/libexec/openafs/volserver /usr/libexec/openafs/salvager -cell CELL_NAME -noauth}}

Verify that all processes are running: 

{{RootCmd|bos status SERVER_NAME -long -noauth|output=<pre>
Instance kaserver, (type is simple) currently running normally.
Process last started at Mon Jun  4 21:07:17 2001 (2 proc starts)
Last exit at Mon Jun  4 21:07:17 2001
Command 1 is '/usr/libexec/openafs/kaserver'
  
Instance buserver, (type is simple) currently running normally.
Process last started at Mon Jun  4 21:07:17 2001 (2 proc starts)
Last exit at Mon Jun  4 21:07:17 2001
Command 1 is '/usr/libexec/openafs/buserver'
  
Instance ptserver, (type is simple) currently running normally.
Process last started at Mon Jun  4 21:07:17 2001 (2 proc starts)
Last exit at Mon Jun  4 21:07:17 2001
Command 1 is '/usr/libexec/openafs/ptserver'
  
Instance vlserver, (type is simple) currently running normally.
Process last started at Mon Jun  4 21:07:17 2001 (2 proc starts)
Last exit at Mon Jun  4 21:07:17 2001
Command 1 is '/usr/libexec/openafs/vlserver'
  
Instance fs, (type is fs) currently running normally.
Auxiliary status is: file server running.
Process last started at Mon Jun  4 21:09:30 2001 (2 proc starts)
Command 1 is '/usr/libexec/openafs/fileserver'
Command 2 is '/usr/libexec/openafs/volserver'
Command 3 is '/usr/libexec/openafs/salvager'
</pre>
}}

Your next action depends on whether you have ever run AFS file server machines in the cell. 

If you are installing the first AFS Server ever in the cell, create the first AFS volume, '''root.afs''' 

{{Note|For the partition name argument, substitute the name of one of the machine's AFS Server partitions. Any filesystem mounted under a directory called {{Path|/vicepx}} , where x is in the range of a-z, will be considered and used as an AFS Server partition. Any unix filesystem will do (as opposed to the client's cache, which can only be ext2/3). Tip: the server checks for each {{Path|/vicepx}} mount point whether a filesystem is mounted there. If not, the server will not attempt to use it. This behaviour can be overridden by putting a file named {{Path|AlwaysAttach}} in this directory.}}

{{RootCmd|vos create SERVER_NAME PARTITION_NAME root.afs -cell CELL_NAME -noauth}}

If there are existing AFS file server machines and volumes in the cell issue the <code>vos sncvldb</code> and <code>vos syncserv</code> commands to synchronize the VLDB (Volume Location Database) with the actual state of volumes on the local machine. This will copy all necessary data to your new server. 

If the command fails with the message "partition /vicepa does not exist on the server", ensure that the partition is mounted before running OpenAFS servers, or mount the directory and restart the processes using <code>bos restart SERVER_NAME -all -cell CELL_NAME -noauth</code> . 

{{RootCmd|vos syncvldb SERVER_NAME -cell CELL_NAME -verbose -noauth
|vos syncserv SERVER_NAME -cell CELL_NAME -verbose -noauth}}

=== Starting the Server Portion of the Update Server ===

{{RootCmd|bos create SERVER_NAME upserver simple "/usr/libexec/openafs/upserver -crypt /etc/openafs/server -clear /usr/libexec/openafs" -cell CELL_NAME -noauth}}

=== Configuring the Top Level of the AFS filespace ===

First you need to set some ACLs, so that any user can lookup {{Path|/afs}} . 

{{Note|The default OpenAFS client configuration has '''dynroot''' enabled. This option turns {{Path|/afs}} into a virtual directory composed of the contents of your {{Path|/etc/openafs/CellServDB}} file. As such, the following command will not work, because it requires a real AFS directory. You can temporarily switch dynroot off by setting '''ENABLE_DYNROOT''' to '''no''' in {{Path|/etc/conf.d/openafs-client}} . Don't forget to issue a client restart after changing parameters.}}

{{RootCmd|fs setacl /afs system:anyuser rl}}

Then you need to create the root volume, mount it readonly on {{Path|/afs/<cell name>}} and read/write on {{Path|/afs/.<cell name>}} . 

{{RootCmd|vos create SERVER_NAME PARTITION_NAME root.cell
|fs mkmount /afs/CELL_NAME root.cell
|fs setacl /afs/CELL_NAME system:anyuser rl
|fs mkmount /afs/.CELL_NAME root.cell -rw}}

{{RootCmd|vos create SERVER_NAME PARTITION_NAME VOLUME_NAME
|fs mkmount /afs/CELL_NAME/MOUNT_POINT VOLUME_NAME
|fs mkmount /afs/CELL_NAME/.MOUNT_POINT VOLUME_NAME -rw
|fs setquota /afs/CELL_NAME/.MOUNT_POINT -max QUOTUM}}

Finally you're done!!! You should now have a working AFS file server on your local network. Time to get a big cup of coffee and print out the AFS documentation!!! 

{{Note|It is very important for the AFS server to function properly, that all system clocks are synchronized. This is best accomplished by installing a ntp server on one machine (e.g. the AFS server) and synchronize all client clocks with the ntp client. This can also be done by the AFS client.}}

== Basic Administration ==

=== Disclaimer ===

OpenAFS is an extensive technology. Please read the AFS documentation for more information. We only list a few administrative tasks in this chapter. 

=== Configuring PAM to Acquire an AFS Token on Login ===

To use AFS you need to authenticate against the KA Server if using an implementation AFS Kerberos 4, or against a Kerberos 5 KDC if using MIT, Heimdal, or ShiShi Kerberos 5. However in order to login to a machine you will also need a user account, this can be local in {{Path|/etc/passwd}} , NIS, LDAP (OpenLDAP), or a Hesiod database. PAM allows Gentoo to tie the authentication against AFS and login to the user account. 

You will need to update {{Path|/etc/pam.d/system-auth}} which is used by the other configurations. "use_first_pass" indicates it will be checked first against the user login, and "ignore_root" stops the local superuser being checked so as to order to allow login if AFS or the network fails. 

{{Code|/etc/pam.d/system-auth|<pre>
auth       required     pam_env.so
auth       sufficient   pam_unix.so likeauth nullok
auth       sufficient   pam_afs.so.1 use_first_pass ignore_root
auth       required     pam_deny.so
  
account    required     pam_unix.so
  
password   required     pam_cracklib.so retry=3
password   sufficient   pam_unix.so nullok md5 shadow use_authtok
password   required     pam_deny.so
  
session    required     pam_limits.so
session    required     pam_unix.so
</pre>
}}

In order for <code>sudo</code> to keep the real user's token and to prevent local users gaining AFS access change {{Path|/etc/pam.d/su}} as follows: 

{{Code|/etc/pam.d/su|<pre>
# Here, users with uid > 100 are considered to belong to AFS and users with
# uid <= 100 are ignored by pam_afs.
auth       sufficient   pam_afs.so.1 ignore_uid 100
  
auth       sufficient   pam_rootok.so
  
# If you want to restrict users begin allowed to su even more,
# create /etc/security/suauth.allow (or to that matter) that is only
# writable by root, and add users that are allowed to su to that
# file, one per line.
#auth       required     pam_listfile.so item=ruser \
#       sense=allow onerr=fail file=/etc/security/suauth.allow
  
# Uncomment this to allow users in the wheel group to su without
# entering a passwd.
#auth       sufficient   pam_wheel.so use_uid trust
  
# Alternatively to above, you can implement a list of users that do
# not need to supply a passwd with a list.
#auth       sufficient   pam_listfile.so item=ruser \
#       sense=allow onerr=fail file=/etc/security/suauth.nopass
  
# Comment this to allow any user, even those not in the 'wheel'
# group to su
auth       required     pam_wheel.so use_uid
  
auth       required     pam_stack.so service=system-auth
  
account    required     pam_stack.so service=system-auth
  
password   required     pam_stack.so service=system-auth
  
session    required     pam_stack.so service=system-auth
session    optional     pam_xauth.so
  
# Here we prevent the real user id's token from being dropped
session    optional     pam_afs.so.1 no_unlog
</pre>
}}

== Acknowledgements ==

We would like to thank the following authors and editors for their contributions to this guide:


* Stefaan De Roeck
* Holger Brueckner
* Benny Chuang
* Tiemo Kieft
* Steven McCoy
* Shyam Mani
