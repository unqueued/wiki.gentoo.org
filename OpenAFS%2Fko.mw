<languages />

{{Metadata|abstract=이 안내서는 젠투 리눅스에서 OpenAFS 서버 및 클라이언트를 설치하는 방법을 보여드립니다.}}

이 안내서는 젠투 리눅스에서 OpenAFS 서버 및 클라이언트를 설치하는 방법을 보여드립니다.

== 간단히 살펴보기 ==

=== 이 문서 정보 ===

이 문서는 젠투 리눅스에 OpenAFS 서버를 설치하는데 필요한 단계 정보를 제공합니다. 이 문서의 각 부분에는 AFS FAQ 및 IBM의 AFS  간편 초보자 안내서로부터 내용을 가져왔습니다. 글쎄요. 바퀴를 굳이 또 찍을 이유가 없죠 :&#41; 

=== AFS란게 뭔가요? ===

AFS는 로컬, 광역 네트워크를 통해 파일 시스템 자원을 효율적으로 공유하는 호스트(클라이언트 및 서버)간 협업을 가능케 하는 분산 파일 시스템입니다. 클라이언트는 주로 사용하는 객체(파일)을 빠르게 접근할 수 있도록 보관하고 있습니다. 

AFS는 본래 카네기 멜론 대학 정보 기술 센터에서 개발한 "앤드류 파일 시스템" 분산 파일 시스템을 기반으로 합니다. "앤드류"는 카네기 멜론 대학 의 연구 프로젝트 이름이었으며 대학교 설립자를 기리는 의미를 내포합니다. 트랜스아크를 설립하고 AFS를 상용화 하고 나서는 AFS가 앤드류 연구 프로젝트를 떠났고 제품 수준의 품질을 지닌 파일 시스템으로서 지원을 받았기에 "앤드류"라는 이름을 버렸습니다. 그러나 /afs 는 파일 시스템의 루트이며 셀이라는 개념을 가지고 있고, 이 셀은 파일 시스템 상에 여러 개가 존재했습니다. 그 때에, 파일 시스템의 루트를 바꾸는 일은 결코 간단한 일이 아니었습니다. 따라서, 파일 시스템 이름을 바꾸는데 있어 이전의 AFS 사이트를 유지하기 위해 AFS는 이름으로 그대로 남았으며 파일 시스템 루트가 되었습니다. 

=== AFS 셀은 뭔가요? ===

AFS 셀은 관리 관점상 하나의 집단으로 묶은 서버의 모음이며, 단일 복합 파일 시스템으로 보입니다.  보통 AFS 셀은 동일한 인터넷 도메인(예: gentoo.org)을 활용하는 호스트의 모음이며, 사용자는 사용자가 접속할 셀의 서버에서 정보와 파일을 요청하는 AFS 클라이언트 워크스테이션에 로그인합니다. 사용자는 찾아볼 어떤 파일이 어떤 서버에 있는지 알지 못합니다. 게다가 서버가 다른 방에 있겠다 하더라도 모든 볼륨을 복제하고 사용자에게 별 다른 내용을 알려주지 않고 다른 서버로 옮기기 때문에 서버가 어디있는지 또한 알지 못합니다. 파일은 언제든 접근할 수 있습니다. 스테로이드 시스템 상의 NFS랑 비슷해보이는군요 :&#41; 

=== AFS를 쓰면 뭐가 좋아요? ===

AFS는 캐싱 처리 요소(클라이언트, 보통 100M에서 1GB), 보안 기능(커베로스 5 기반, 접근 관리 목록), 주소 관리 단순화(오직 하나의 파일 시스템만 보유), 확장성(필요한만큼 셀에 서버를 더 추가함), 통신 프로토콜 의 주요 기능을 보유하고 있습니다. 

=== 더 많은 내용은 어디서 찾아볼 수 있나요? ===

[http://www.angelfire.com/hi/plutonic/afs-faq.html AFS FAQ]를 읽어보십시오. 

[http://www.openafs.org www.openafs.org]의 OpenAFS 메인 페이지 

AFS는 지금은 IBM이 소유한 트랜스아크에서 개발했습니다. 2005년 4월부터 IBM 제품 카탈로그에서 없어졌습니다. IBM은 AFS 제품의 소스 코드 브랜치를 분리했고, 2000년에 커뮤니티 개발용과 관리용 두가지 용도로 소스코드 복제본을 만들었습니다. 그들은 OpenAFS 릴리즈라 불렀습니다.

=== 문제를 어떻게 디버깅하죠? ===

OpenAFS에는 대단한 기록 설비를 갖추고 있습니다. 그러나 기본적으로 여러분의 시스템에 있는 로깅 체계로 기록하기보단 자체 기록에 바로 남겨둡니다. 서버 로그를 시스템 로거에서 처리하도록 하려면 <code>-syslog</code> 옵션을 모든 {{c|bos}} 명령에서 활용하십시오. 

== 이전 버전으로부터 업그레이드 ==

=== 도입부 ===

이 부분에서는 기존의 OpenAFS를 OpenAFS 1.4.0 이상으로 업그레이드하는 과정을 도와드립니다(또는 1.2.13부터 시작하는 1.2.x가 됩니다만, 후자는 대부분 사람들이 linux-2.6 지원, 대용량 파일 지원, 버그 수정판을 원하기 때문에 따로 다루지 않겠습니다). 

OpenAFS 1.4 버전을 아얘 처음부터 설치한다면 이 부분을 안전하게 건너뛸 수 있습니다. 허나, 이전 버전에서 업그레이드한다면, 다음 부분의 안내서를 따라가시는게 좋습니다. 이빌드의 트랜지션 스크립트는 간단한 업그레이드 및 재시작을 도와주도록 설계했습니다. (보안상 이유로) 설정 파일과 이전 위치의 시작 스크립트를 지우지 마시고 부팅 설정을 새 스크립트로 자동으로 바꾸지 않음을 참고하십시오. 더 확실한 방편이 필요하다면, 이전 OpenAFS 커널 모듈을 업데이트한 시스템 바이너리와 함께 사용할 때 커널이 더 잘 맛이 갈 수 있습니다. 따라서, 바닥부터 깔끔하고 쉽게 전환을 진해앟겠습니다. 그렇게 할거죠? 


{{Note|이 장은 다양한 시스템 설정을 염두에 두고 작성했습니다. 여전히 사용자가 만들어내는 꼼수 덕분에 여기서 설명하지 않을 별난 상황들이 일어날 수 있습니다. 시스템을 이리저리 설정할 충분한 자신감을 지닌 사용자라면 적재적소의 주어진 참고 사항을 충분히 적용하는 경험을 거쳐야합니다. 이 같은 경우가 아니라면 시스템에서 일부 과정만 진행하겠지만 이전 이빌드를 설치하면 앞으로 나타날 대부분의 경고 상황은 건너뛸 수 있습니다.}}


=== 이전 버전과의 차이점 ===

예전부터 OpenAFS는 코드를 포킹하기 전 IBM 트랜스타크 연구소에서 사용한 동일한 경로 이름 규칙을 사용합니다. 당연히도, 이전 AFS 설정은 이전 경로 이름 규칙을 계속 활용할 수 있습니다. 최근 설정에서는 (대부분의 리눅스 배포판에서 본 대로) 표준 경로를 활용한 FHS에 맞추었습니다. 다음 표에서는 OpenAFS 배포 타르볼의 README와 설정 스크립트의 내용을 비교합니다: 

{| class="table" style="text-align: left;" 
|- 
! 디렉터리
! 사용목적
! 트랜스아크 모드
! 기본모드
! 젠투용 변환
|- 
| viceetcdir
| 클라이언트 설정
| /usr/vice/etc
| $(sysconfdir)/openafs
| /etc/openafs
|- 
| unnamed
| 클라이언트 바이너리
| unspecified
| $(bindir)
| /usr/bin
|- 
| afsconfdir
| 서버 설정
| /usr/afs/etc
| $(sysconfdir)/openafs/server
| /etc/openafs/server
|- 
| afssrvdir
| 내부 서버 바이너리
| /usr/afs/bin (servers)
| $(libexecdir)/openafs
| /usr/libexec/openafs
|- 
| afslocaldir
| 서버 상태
| /usr/afs/local
| $(localstatedir)/openafs
| /var/lib/openafs
|- 
| afsdbdir
| Auth/serverlist/... databases
| /usr/afs/db
| $(localstatedir)/openafs/db
| /var/lib/openafs/db
|- 
| afslogdir
| 로그 파일
| /usr/afs/logs
| $(localstatedir)/openafs/logs
| /var/lib/openafs/logs
|- 
| afsbosconfig
| 감독관 설정
| $(afslocaldir)/BosConfig
| $(afsconfdir)/BosConfig
| /etc/openafs/BosConfig
|-
|}

트랜스아크 모드에서 바이너리 파일 같은걸 {{Path|/usr/vice/etc}}에 넣는 이상한 점이 보이지만, 전체적으로 정리하려는 의도에서 이 목록을 제시한건 아닙니다. 오히려 설정 파일을 변환하는데 있어 문제를 해결하기 위한 참고자료로 제시할 뿐입니다. 

또한 경로를 바꾸면 기본 디스크 캐시 위치는 {{Path|/usr/vice/cache}}에서 {{Path|/var/cache/openafs}}로 바뀝니다. 

게다가 초기화 스크립트는 클라이언트와 서버 부분으로 나뉘었습니다. 종종 {{Path|/etc/init.d/afs}} 를 보유하고 있지만 이제  {{Path|/etc/init.d/openafs-client}} 와 {{Path|/etc/init.d/openafs-server}}로 쪼갤 시간입니다. 따라서, {{Path|/etc/conf.d/afs}} 설정파일은  {{Path|/etc/conf.d/openafs-client}} 와 {{Path|/etc/conf.d/openafs-server}} 로 나눕니다. 또한 클라이언트 또는 서버를 가동하는 {{Path|/etc/conf.d/afs}}의 옵션은 오래되어 방치됩니다. 

Another change to the init script is that it doesn't check your disk cache setup anymore. The old code required that a separate ext2 partition be mounted at {{Path|/usr/vice/cache}} . There were some problems with that: 

* Though it's a very logical setup, your cache doesn't need to be on a separate partition. As long as you make sure that the amount of space specified in {{Path|/etc/openafs/cacheinfo}} really is available for disk cache usage, you're safe. So there is no real problem with having the cache on your root partition.
* Some people use soft-links to point to the real disk cache location. The init script didn't like this, because then this cache location didn't turn up in {{Path|/proc/mounts}} .
* Many prefer ext3 over ext2 nowadays. Both filesystems are valid for usage as a disk cache. Any other filesystem is unsupported (like: don't try reiserfs, you'll get a huge warning, expect failure afterwards).

=== 새 경로로 전환 ===

우선 최신 버전의 OpenAFS로 이전 설정 파일을 덮어쓰지 말아야합니다. 스크립트는 시스템에 이미 있는 파일을 바꾸지 않도록 했습니다. 그래서 이전의 설정과 새 위치의 설정으로 설정을 완전히 꼬아놓더라도 스크립트는 그 이상의 문제를 유발하지 않습니다. 또한 OpenAFS 가 동작중이라면, 데이터베이스가 깨질 가능성을 막기 위해 설치를 중단합니다. 

One caveat though -- there have been ebuilds floating around the internet that partially disable the protection that Gentoo puts on {{Path|/etc}} . These ebuilds have never been distributed by Gentoo. You might want to check the <code>CONFIG_PROTECT_MASK</code> variable in the output of the following command: 

{{RootCmd|emerge info {{!}} grep "CONFIG_PROTECT_MASK"|output=<pre>
CONFIG_PROTECT_MASK="/etc/gconf /etc/terminfo /etc/texmf/web2c /etc/env.d"
</pre>
}}

Though nothing in this ebuild would touch the files in {{Path|/etc/afs}} , upgrading will cause the removal of your older OpenAFS installation. Files in <code>CONFIG_PROTECT_MASK</code> that belong to the older installation will be removed as well. 

It should be clear to the experienced user that in the case he has tweaked his system by manually adding soft links (e.g. {{Path|/usr/afs/etc}} to {{Path|/etc/openafs}} ), the new installation may run fine while still using the old configuration files. In this case, there has been no real transition, and cleaning up the old installation will result in a broken OpenAFS config. 

이제 어떤 일도 일어나지 않는다는걸 확인했으니 어떤 동작을 하는지 알아볼 차례입니다: 

* {{Path|/usr/afs/etc}} 를 {{Path|/etc/openafs/server}} 에 복사
* {{Path|/usr/vice/etc}} 를 {{Path|/etc/openafs}} 에 복사
* {{Path|/usr/afs/local}} 를 {{Path|/var/lib/openafs}} 에 복사
* {{Path|/usr/afs/bin/}}를 {{Path|/usr/libexec/openafs}}로, {{Path|/usr/afs/etc}}를 {{Path|/etc/openafs/server}}로, {{Path|/usr/afs/bin}}(앞에서와 같이 / 제외)를 {{Path|/usr/bin}} 로 바꾸는 동안 {{Path|/usr/afs/local/BosConfig}} 를 {{Path|/etc/openafs/BosConfig}} 에 복사
* {{Path|/usr/afs/db}} 를 {{Path|/var/lib/openafs/db}} 에 복사
* 모든 이전 옵션을 클라이언트 전용으로 해둘 {{Path|/etc/conf.d/afs}} 설정 파일을 {{Path|/etc/conf.d/openafs-client}}에 복사

=== 자체 업그레이드 ===

OpenAFS 서버를 설정하지 않았다고요? 했으면 이전 어떤 상황인지 여전히 대기중인지 알아채셨죠? 

그럼 시작하지요! 

서버가 실행중이면 일단 끄십시오. 

{{RootCmd|/etc/init.d/afs stop}}

명령 실행이 끝나면 자체적으로 업그레이드를 실행하십시오. 

{{Emerge|openafs}}

=== OpenAFS 다시 시작하기 ===

If you had an OpenAFS server running, you would have not have been forced to shut it down. Now is the time to do that. 

{{RootCmd|/etc/init.d/afs stop}}

As you may want keep the downtime to a minimum, so you can restart your OpenAFS server right away. 

{{RootCmd|/etc/init.d/openafs-server start}}

다음 명령으로 제대로 동작중인지 확인할 수 있습니다: 

{{RootCmd|/usr/bin/bos status localhost -localauth}}

Before starting the OpenAFS client again, please take time to check your cache settings. They are determined by {{Path|/etc/openafs/cacheinfo}} . To restart your OpenAFS client installation, please type the following: 

{{RootCmd|/etc/init.d/openafs-client start}}

=== 과정 후 정리 ===

Before cleaning up, please make really sure that everything runs smoothly and that you have restarted after the upgrade (otherwise, you may still be running your old installation). 

{{Important|Please make sure you're not using {{Path|/usr/vice/cache}} for disk cache if you are deleting {{Path|/usr/vice}} !!}}

시스템에서 다음 디렉터리를 안전하게 제거하셔도 됩니다: 

*  {{Path|/etc/afs}} 
*  {{Path|/usr/vice}} 
*  {{Path|/usr/afs}} 
*  {{Path|/usr/afsws}} 

다음 파일은 필요하지 않습니다: 

*  {{Path|/etc/init.d/afs}} 
*  {{Path|/etc/conf.d/afs}} 

{{RootCmd|tar czf /root/oldafs-backup.tgz /etc/afs /usr/vice /usr/afs /usr/afsws
|rm -R /etc/afs /usr/vice /usr/afs /usr/afsws
|rm /etc/init.d/afs /etc/conf.d/afs}}

=openafs-1.2.13 또는 =openafs-1.3.85 이빌드를 앞서 호라용했다면 필요없는 파일도 있습니다: 

* {{Path|/etc/init.d/afs-client}} 
* {{Path|/etc/init.d/afs-server}} 
* {{Path|/etc/conf.d/afs-client}} 
* {{Path|/etc/conf.d/afs-server}} 

=== 초기화 스크립트 변경 ===

이제 대부분 사람들이 OpenAFS 클라이언트를 시스템을 시작할 때 자동으로 시작하도록 설정했습니다. 이 부분을 안전하게 넘어갈 수 없는 분이 있습니다. 자동으로 시작하도록 시스템을 설정했다면, 초기화 스크립트의 이름이 바뀌었으니ㅣ 다시 활성화해야합니다. 

{{RootCmd|rc-update del afs default
|rc-update add openafs-client default
|rc-update add openafs-server default}}

<code>=openafs-1.2.13</code> 또는 <code>=openafs-1.3.85</code>를 설치했다면 기본 런레벨에서 {{Path|afs-client}} 과 {{Path|afs-server}}를 제거하시고, 대신 {{Path|afs}}를 활용하십시오. 

=== 문제 해결: 자동 업그레이드 실패 ===

절망하지 마십시오. 어떤 데이터나 설정 파일도 날라가지 않았습니다. 그러니 상황을 다시 살펴보도록 하겠습니다. 이 경우 [http://bugs.gentoo.org bugs.gentoo.org]에 버그 보고서를 보내주시고 되도록이면 최대한 정보를 많게 자세히 적절하게 넣어주십시오. 

클라이언트 시작시 문제가 있다면 문제를 진단하는데 이 방법이 도움이 될 것입니다: 

* <code>dmesg</code>를 실행하십시오. 클라이언트에서 보통 오류 메시지를 보냅니다.
* {{Path|/etc/openafs/cacheinfo}}를 확인하십시오 . 다음 모양새를 갖춰야합니다: /afs:{path to disk cache}:{number of blocks for disk cache}. 보통 여러분의 디스크 캐시는 {{Path|/var/cache/openafs}} 에 있습니다.
* <code>lsmod</code> 출력 내용을 확인하십시오 . openafs로 시작하는 줄을 확인하십시오.
* <code>pgrep afsd</code> 명령은 afsd가 실행중인지 아닌지를 언급합니다.
* <code>cat /proc/mounts</code> 에서 {{Path|/afs}} 를 마운트했는지 여부가 나타나야합니다.

서버 시작에 누제가 있다면 다음 실마리가 도움이 될 수도 있습니다: 

* <code>pgrep bosserver</code> tells you whether the overseer is running or not. If you have more than one overseer running, then something has gone wrong. In that case, you should try a graceful OpenAFS server shutdown with <code>bos shutdown localhost -localauth -wait</code> , check the result with <code>bos status localhost -localauth</code> , kill all remaining overseer processes and then finally check whether any server processes are still running ( <code>ls /usr/libexec/openafs</code> to get a list of them). Afterwards, do <code>/etc/init.d/openafs-server zap</code> to reset the status of the server and <code>/etc/init.d/openafs-server start</code> to try launching it again.
* If you're using OpenAFS' own logging system (which is the default setting), check out {{Path|/var/lib/openafs/logs/*}} . If you're using the syslog service, go check out its logs for any useful information.

== 문서 ==

=== AFS 문서 가져오기 ===

원조 IBM AFS 문서를 구할 수 있습니다. 잘 쓰여졌고 AFS 서버를 관리하기에 여러분께 좋다면 읽고 싶어하실지도 모르겠습니다. 

{{Emerge|app-doc/afsdoc}}

You also have the option of using the documentation delivered with OpenAFS. It is installed when you have the USE flag <code>doc</code> enabled while emerging OpenAFS. It can be found in {{Path|/usr/share/doc/openafs-*/}} . At the time of writing, this documentation was a work in progress. It may however document newer features in OpenAFS that aren't described in the original IBM AFS Documentation. 

== 클라이언트 설치 ==

=== 클라이언트 빌드 ===

{{Emerge|net-fs/openafs}}

컴파일이 끝나면 계속 진행할 준비가 끝납니다. 

=== 간단한 전역 브라우징 클라이언트 설치 ===

If you're not part of a specific OpenAFS-cell you want to access, and you just want to try browsing globally available OpenAFS-shares, then you can just install OpenAFS, not touch the configuration at all, and start {{Path|/etc/init.d/openafs-client}} .

=== 지정 OpenAFS 셀에 접근 ===

지정 셀에 접근하려면 대학 또는 회사의 자체 셀에 요청한 후 일부 설정을 다루어야합니다. 

우선 데이터베이스 서버에서 여러분의 셀에 대해 {{Path|/etc/openafs/CellServDB}}를 업데이트해야합니다. 이 정보는 보통 관리자에게 제공합니다. 

두번째로, OpenAFS 셀에 기록할 수 있으려면, {{Path|/etc/openafs/ThisCell}} 과 같은 식의 이름을 지정해야합니다. 

{{CodeBox|title=CellServDB 및 ThisCell 설정 변경|1=
CellServDB:
>netlabs        #Cell name
10.0.0.1        #storage
  
ThisCell:
netlabs
}}


{{Warning| {{Path|CellServDB}} 파일에 있는 공간만 활용합니다. {{Key|TAB}} 키를 활용하면 클라이언트가 멈출 수도 있습니다.}}

CellServDB tells your client which server(s) it needs to contact for a specific cell. ThisCell should be quite obvious. Normally you use a name which is unique for your organisation. Your (official) domain might be a good choice. 

For a quick start, you can now start {{Path|/etc/init.d/openafs-client}} and use <code>kinit; aklog</code> to authenticate yourself and start using your access to the cell. For automatic logons to you cell, you want to consult the appropriate section below. 

=== 캐시 조정 ===

{{Note|Unfortunately the AFS Client needs a ext2/3 filesystem for its cache to run correctly. There are some issues when using other filesystems (using e.g. reiserfs is not a good idea).}}

You can house your cache on an existing filesystem (if it's ext2/3), or you may want to have a separate partition for that. The default location of the cache is {{Path|/var/cache/openafs}} , but you can change that by editing {{Path|/etc/openafs/cacheinfo}} . A standard size for your cache is 200MB, but more won't hurt. 

=== 시스템 시작시 AFS 시작하기 ===

다음 명령은 시스템 시작시 여러분의 afs 클라이언트에 적당한 링크를 만듭니다: 

{{Warning|Unless <code>afsd</code> is started with the <code>-dynroot</code> option, you should always have a running afs server in your domain when trying to start the afs client. Your system won't boot until it gets some timeout if your AFS server is down (and this is quite a long long time.)}}

{{RootCmd|rc-update add openafs-client default}}

== 서버 설치 ==

=== 커베로스 서버 설치 ===

OpenAFS는 커베로스 5 인증이 필요합니다. 다음에서는 MIT 커베로스 서버를 설치하는 방법을 보여드립니다. 대신 Heimdal 커베로스 구현체를 활용할 수도 있습니다.

{{Important|커베로스는 커베로스 서버와 클라이언트간 시간 동기화가 필요합니다. 서버에 ntpd를 설치했는지 확인하십시오.}}

다음 명령으로 MIT 커베로스 서버 바이너리를 설치하십시오:

{{Emerge|mit-krb5}}

Edit the {{Path|/etc/krb5.conf}} and {{Path|/etc/kdc.conf}} configuration files.
Replace the EXAMPLE.COM realm name with your realm name, and update the example
hostnames with your actual hostnames.

{{Note|By convention, your Kerberos realm name should match your internet
domain name, except the Kerberos realm name is in uppercase letters.}}

다음과 같이 커베로스 데이터베이스를 만드십시오:

{{RootCmd|mkdir /etc/krb5kdc}}
{{RootCmd|kdb5_util create -s}}

=== 서버 빌드 ===

{{Note|모든 명령은 한줄로 작성해야합니다. 이 문서에서는 보기 편하게 두 줄로 줄바꿈 처리를 하기도 합니다.}}

아직 끝내지 않았다면, 다음 명령으로 AFS 서버 ''와'' 클라이언트를 설정할 모든 필요한 바이너리를 설치하십시오. 

{{Emerge|net-fs/openafs}}

=== 서버 키 생성 ===

As of OpenAFS version 1.6.5, the OpenAFS servers support strong crypto
(AES, etc.) for the service key, and will read the Kerberos keytab file
directly.  Create the Kerberos service key for OpenAFS and export it to
a keytab for the OpenAFS server processes, before starting the OpenAFS
services.

{{RootCmd|kadmin.local -q "addprinc -randkey afs/<cellname>"}}
{{RootCmd|kadmin.local -q "ktadd -k /etc/openafs/server/rxkad.keytab afs/<cellname>"}}

{{Important|It is critical to keep the {{Path|rxkad.keytab}} file confidential. The security
of the files in your AFS cell depends on the service key it contains.}}

=== AFS 서버 시작하기 ===

기본 감독(BOS) 서버를 초기화하려면 서버 머신의 AFS 프로세스를 감시하고 관리하는 {{c|bosserver}} 명령을 실행해야 합니다. 시스템의 초기화 과정 처럼 생각하십시오 

{{Note|As of OpenAFS 1.6.0, it is no longer necessary to include the <code>-noauth</code> flag to disable authentication.  This makes the setup more secure, since there is not a window in which the servers are running with authentication disabled. This also has the nice side effect of greatly simplifying the server setup procedure.}}

OpenAFS  <code>bosserver</code>를 시작하십시오.

{{RootCmd|/etc/init.d/openafs-server start}}

다시 부팅할 때 OpenAFS 서버가 시작하는지 확인하십시오:

{{RootCmd|rc-update add openafs-server default}}

{{Path|/etc/openafs/server/CellServDB}} 와 {{Path|/etc/openafs/server/ThisCell}}로 만든 BOS 서버를 검증해보십시오:

{{RootCmd|ls -al /etc/openafs/server/|output=<pre>
-rw-r--r--    1 root     root           41 Jun  4 22:21 CellServDB
-rw-r--r--    1 root     root            7 Jun  4 22:21 ThisCell
</pre>
}}

=== 서버 프로세스의 셀 이름 정의 ===

이제 셀 이름을 할당하십시오. 

{{Important|There are some restrictions on the name format. Two of the most important restrictions are that the name cannot include uppercase letters or more than 64 characters. Remember that your cell name will show up under {{Path|/afs}} , so you might want to choose a short one. If your AFS service is to be accessible over the internet, you should use a registered internet domain name for your cell's name. This avoids conflicts in the global AFS namespace.}}

{{Note|In the following and every instruction in this guide, for the SERVER_NAME argument substitute the full-qualified hostname (such as '''afs.gentoo.org''' ) of the machine you are installing. For the CELL_NAME argument substitute your cell's complete name (such as '''gentoo''' )}}

{{c|bos setcellname}} 명령을 실행하여 셀 이름을 설정하십시오: 

{{RootCmd|bos setcellname localhost CELL_NAME -localauth}}

=== 데이터베이스 서버 프로세스 시작하기 ===

다음 {{c|bos create}} 명령을 활용하여 데이터베이스 서버 프로세스 항목을 {{Path|/etc/openafs/BosConfig}} 파일에 만드십시오. 이 세가지 프로세스는 데이터 서버 머신에서만 동작합니다. 

{| class="table" style="text-align: left;" 
! 프로세스
! 설명
|- 
| buserver
| 백업 데이터베이스를 관리하는 백업 서버
|- 
| ptserver
| 보호 데이터베이스를 관리하는 보호 서버
|- 
| vlserver
| 볼륨 위치 데이터베이스(VLDB)를 관리하는 볼륨 위치 서버. 상당히 중요합니다 :&#41;
|-
|}

{{Note|OpenAFS에 {{c|kaserver}}라고 하는 커베로스 서버가 있습니다. {{c|kaserver}}는 오래됐으니 새로 설치하고 나면 사용하면 안됩니다.}}

{{RootCmd|bos create localhost buserver simple /usr/libexec/openafs/buserver -cell CELL_NAME -localauth
|bos create localhost ptserver simple /usr/libexec/openafs/ptserver -cell CELL_NAME -localauth
|bos create localhost vlserver simple /usr/libexec/openafs/vlserver -cell CELL_NAME -localauth}}

{{c|bos status}} 명령을 실행하여 모든 서버를 확인할 수 있습니다: 

{{RootCmd|bos status localhost -localauth|output=<pre>
Instance buserver, currently running normally.
Instance ptserver, currently running normally.
Instance vlserver, currently running normally.
</pre>
}}

=== 첫 파일 서버, 볼륨 서버, 셀베이저 시작하기 ===

파일 서버, 볼륨 서버, 구조자(fileserver, volserver, salvager 프로세스)로 구성된 <code>fs</code> 프로세스를 시작하십시오. 

{{RootCmd|bos create localhost fs fs /usr/libexec/openafs/fileserver /usr/libexec/openafs/volserver /usr/libexec/openafs/salvager -localauth}}

모든 프로세스가 실행중인지 확인하십시오: 

{{RootCmd|bos status localhost -long -localauth|output=<pre>
  
Instance buserver, (type is simple) currently running normally.
Process last started at Mon Jun  4 21:07:17 2001 (2 proc starts)
Last exit at Mon Jun  4 21:07:17 2001
Command 1 is '/usr/libexec/openafs/buserver'
  
Instance ptserver, (type is simple) currently running normally.
Process last started at Mon Jun  4 21:07:17 2001 (2 proc starts)
Last exit at Mon Jun  4 21:07:17 2001
Command 1 is '/usr/libexec/openafs/ptserver'
  
Instance vlserver, (type is simple) currently running normally.
Process last started at Mon Jun  4 21:07:17 2001 (2 proc starts)
Last exit at Mon Jun  4 21:07:17 2001
Command 1 is '/usr/libexec/openafs/vlserver'
  
Instance fs, (type is fs) currently running normally.
Auxiliary status is: file server running.
Process last started at Mon Jun  4 21:09:30 2001 (2 proc starts)
Command 1 is '/usr/libexec/openafs/fileserver'
Command 2 is '/usr/libexec/openafs/volserver'
Command 3 is '/usr/libexec/openafs/salvager'
</pre>
}}

다음 동작은 셀에서 AFS 파일 서버를 가동하느냐에 따라 다릅니다. 

첫 AFS 서버를 셀에 설치한다면 첫 AFS 볼륨을 '''root.afs'''로 설정하십시오. 

{{Note|For the partition name argument, substitute the name of one of the machine's AFS Server partitions. Any filesystem mounted under a directory called {{Path|/vicepx}} , where x is in the range of a-z, will be considered and used as an AFS Server partition. Any unix filesystem will do (as opposed to the client's cache, which can only be ext2/3). Tip: the server checks for each {{Path|/vicepx}} mount point whether a filesystem is mounted there. If not, the server will not attempt to use it. This behaviour can be overridden by putting a file named {{Path|AlwaysAttach}} in this directory.}}

{{RootCmd|vos create localhost PARTITION_NAME root.afs -localauth}}

If there are existing AFS file server machines and volumes in the cell issue the <code>vos sncvldb</code> and <code>vos syncserv</code> commands to synchronize the VLDB (Volume Location Database) with the actual state of volumes on the local machine. This will copy all necessary data to your new server. 

If the command fails with the message "partition /vicepa does not exist on the server", ensure that the partition is mounted before running OpenAFS servers, or mount the directory and restart the processes using <code>bos restart localhost -all -cell CELL_NAME -localauth</code> . 

{{RootCmd|vos syncvldb localhost -verbose -localauth
|vos syncserv localhost -verbose -localauth}}

=== 업데이트 서버의 서버 기능 시작하기 ===

{{RootCmd|bos create localhost upserver simple "/usr/libexec/openafs/upserver -crypt /etc/openafs/server -clear /usr/libexec/openafs" -localauth}}

=== 첫 관리 계정 만들기 ===

An administrative account is needed to complete the cell setup and perform on going administration.  The first account must be created directly on the servers. Additional accounts may then be created without direct ssh access to the servers.

{{Note|In the following descriptions and commands, substitute all instances of USERNAME with your actual user name.}}

첫 관리 계정을 만들려면 네가지 작업 과정이 필요합니다.

* a Kerberos principal, by convention, in the form of USERNAME/admin
* an AFS user, by convention, the form of USERNAME.admin
* membership in the built-in AFS system::administrators group
* membership in the OpenAFS superuser list

{{Note|Any name may be used for the administrator principal, for example, "admin", or "afsadmin". If you create an admin principal that does not follow the USERNAME/admin pattern, be sure to update the kerberos KDC access control list in the {{Path|kadm5.acl}} configuration file.}}

{{Important|The Kerberos principal contains as slash "/" separator, but unfortunately, AFS uses a dot "." separator. Be sure to mind the difference.}}

커베로스 기본 인증 정보를 만드십시오. 이 명령을 커베로스 서버에서 루트 권한으로 실행하십시오:

{{RootCmd|kadmin.local -q "addprinc USERNAME/admin"}}

AFS admin 사용자를 만드십시오 이 명령을 OpenAFS 데이터베이스 서버에서 루트 권한으로 실행하십시오:

{{RootCmd|pts createuser USERNAME.admin -localauth}}

AFS admin 사용자를 내장 관리자 그룹에 추가하십시오. 이 명령을 OpenAFS 데이터베이스 서버에서 루트 권한으로 실행하십시오:

{{RootCmd|pts adduser USERNAME.admin system:administrators -localauth}}

AFS admin 사용자를 최고 사용자 목록에 추가하십시오. 이 명령을 각 OpenAFS 서버에서 루트 권한으로 실행하십시오:

{{RootCmd|bos adduser localhost USERNAME.admin -localauth}}

{{Note|부족한 권한과 관련이 있고, AFS 셀 이름이 커베로스 렘 이름과 달라 나중에 문제가 생기면, 이 문제는 {{Path|/etc/openafs/server/krb.conf}} 설정 파일에서 렘 이름을 설정하여 재조정할 수 있습니다.}}

=== AFS 파일 공간 최상위 레벨 설정 ===

서버 설정이 끝났습니다. AFS 클라이언트를 실행하여 AFS 최상위 디렉터리를 설정하고 올바른 접근 권한을 부여해야합니다. 이 클라이언트는 OpenAFS 서버에 설치할 필요가 없습니다. 단지 관리 권한만 가져오면 됩니다. 이 부분에서 명령 실행 목적의 루트 접근은 필요하지 않습니다.

우선 관리 권한을 가져오겠습니다:

{{Cmd|kinit USERNAME/admin|output=<pre>
Password for USERNAME/admin@REALM: ********
</pre>}}

{{Cmd|aklog
|tokens|output=<pre>
Tokens held by the Cache Manager:
 
User's (AFS ID 1) tokens for afs@mycellname.com [Expires Oct 21 20:26]
   --End of list--
</pre>}}

우선 일부 ACL을 설정하여 어떤 사용자든 {{Path|/afs}}을 탐색할 수 있게 해야합니다. 

{{Note|기본 OpenAFS 클라이언트 설정은 '''dynroot''' 활성화 상태입니다. 이 옵션은 {{Path|/afs}} 디렉터리를 {{Path|/etc/openafs/CellServDB}} 파일의 내용으로 구성한 가상 디렉터리로 바꿉니다. 다행스럽게도 '''dynroot'''에서는 "magic" {{Path|/afs/.:mount/}} 디렉터리를 통해 이름을 통핸 볼륨 접근 방식을 제공합니다. 이렇듯 설정에 따른 동작은 '''dynroot'''를 비활성 처리하고 클라이언트를 다시 시작할 필요가 없습니다.}}

{{Cmd|fs setacl /afs/.:mount/CELL_NAME:root.afs/. system:anyuser rl}}

다음에는 루트 볼륨을 만들고 {{Path|/afs/<cell name>}}를 읽기 전용으로, {{Path|/afs/.<cell name>}}를 읽기/쓰기용으로 마운트해야합니다. 

{{Cmd|vos create SERVER_NAME PARTITION_NAME root.cell
|fs mkmount /afs/.:mount/CELL_NAME:root.afs/CELL_NAME root.cell
|fs setacl /afs/.:mount/CELL_NAME:root.afs/CELL_NAME system:anyuser rl
|fs mkmount /afs/.:mount/CELL_NAME:root.afs/.CELL_NAME root.cell -rw}}


이 때, 새 AFS 위치에 볼륨을 만들고 파일 공간을 추가할 수 있습니다. 사용자 및 그룹을 만들고 디렉터리 접근 권한 설정을 통해 사용자가 파일과 디렉터리를 만들 수 있게 해야합니다. 볼륨을 만들고 마운트하려면:

{{Cmd|vos create SERVER_NAME PARTITION_NAME VOLUME_NAME
|fs mkmount /afs/CELL_NAME/MOUNT_POINT VOLUME_NAME
|fs mkmount /afs/CELL_NAME/.MOUNT_POINT VOLUME_NAME -rw
|fs setquota /afs/CELL_NAME/.MOUNT_POINT -max QUOTUM}}

결국은 해냈습니다!!! 로컬 네트워크에 동작하는 AFS 파일 서버를 두었습니다. 이제 왕컵에 커피를 부어드시고 AFS 문서를 쭉 뽑아두세요!!! 

{{Note|AFS 서버가 제기능을 하려면 모든 시스템의 시계를 동기화하는 것이 중요합니다. ntp 서버를 한대의 머신(예: AFS 서버)에 설치하고 ntp 클라이언트로 모든 클라이언트의 클록을 동기화하면 됩니다. 또한 AFS 클라이언트로도 처리할 수 있습니다.}}

== 기본 관리 ==

=== 면책 사항 ===

OpenAFS는 확장 기술입니다. 더 많은 정보를 확인하시려면 AFS 문서를 살펴보십시오. 이 부분에서는 지극히 일부의 관리 작업만 나열합니다. 

=== 로그인시 AFS 토큰을 획득하기 위한 PAM 설정 ===

AFS를 이용하려면 커베로스 5 KDC(MIT, Heimdal, ShiShi Kerberos 5 또는 Microsoft Active Directory)에 대해 인증해야합니다. 허나 머신에 로그인하려면  {{Path|/etc/passwd}},  NIS, LDAP(OpenLDAP), Hesiod 데이터베이스의 로컬 일부가 될 수 있는 사용자 계정이 있어야합니다. PAM은 젠투에서 AFS에 인증 체계를 붙일 수 있게 하고 사용자 계정으로 로그인할 수 있게합니다. 

{{Note| 이 섹션은 오래됐습니다. [http://docs.openafs.org/QuickStartUnix/HDRWQ41.html 리눅스 시스템에서 AFS 로그인 활성화] 문서를 보십시오.}}

다른 설정에서 활용할 {{Path|/etc/pam.d/system-auth}} 파일을 업데이트해야합니다. ''use_first_pass''는 사용자가 로그인하면 표시하는 것으로 보이며 ''ignore_root'' 는 AFS 또는 네트워크에 문제가 있어 로그인을 허용할 때 로컬 최고 사용자 확인 과정을 중단하게 합니다. 

{{FileBox|filename=/etc/pam.d/system-auth|1=
auth       required     pam_env.so
auth       sufficient   pam_unix.so likeauth nullok
auth       sufficient   pam_afs.so.1 use_first_pass ignore_root
auth       required     pam_deny.so
  
account    required     pam_unix.so
  
password   required     pam_cracklib.so retry=3
password   sufficient   pam_unix.so nullok md5 shadow use_authtok
password   required     pam_deny.so
  
session    required     pam_limits.so
session    required     pam_unix.so
}}

실제 사용자의 토큰을 유지하고 로컬 사용자가 {{Path|/etc/pam.d/su}}의 내용을 바꾸어 AFS 접근 권한을 취득하는 문제를 막으려면 다음 명령을 실행하십시오: 

{{FileBox|filename=/etc/pam.d/su|1=
# Here, users with uid > 100 are considered to belong to AFS and users with
# uid <= 100 are ignored by pam_afs.
auth       sufficient   pam_afs.so.1 ignore_uid 100
  
auth       sufficient   pam_rootok.so
  
# If you want to restrict users begin allowed to su even more,
# create /etc/security/suauth.allow (or to that matter) that is only
# writable by root, and add users that are allowed to su to that
# file, one per line.
#auth       required     pam_listfile.so item=ruser \
#       sense=allow onerr=fail file=/etc/security/suauth.allow
  
# Uncomment this to allow users in the wheel group to su without
# entering a passwd.
#auth       sufficient   pam_wheel.so use_uid trust
  
# Alternatively to above, you can implement a list of users that do
# not need to supply a passwd with a list.
#auth       sufficient   pam_listfile.so item=ruser \
#       sense=allow onerr=fail file=/etc/security/suauth.nopass
  
# Comment this to allow any user, even those not in the 'wheel'
# group to su
auth       required     pam_wheel.so use_uid
  
auth       required     pam_stack.so service=system-auth
  
account    required     pam_stack.so service=system-auth
  
password   required     pam_stack.so service=system-auth
  
session    required     pam_stack.so service=system-auth
session    optional     pam_xauth.so
  
# Here we prevent the real user id's token from being dropped
session    optional     pam_afs.so.1 no_unlog
}}

[[Category:Filesystems]] {{Migrated|originalauthors=Stefaan De Roeck, Holger Brueckner, Benny Chuang, Tiemo Kieft, Steven McCoy, Shyam Mani}}
