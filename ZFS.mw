[http://en.wikipedia.org/wiki/ZFS ZFS] was developed by SUN Microsystems and is an advanced file system that includes many features like:
* Manage storage hardware as vdev in ''zpools''
* Manage volumes in ''zpools'' (like [[LVM]])
* Redundancy with support for RAIDZ1(RAID5),RAIDZ2(RAID6) and MIRROR(RAID1)
* Resilvering file system
* [http://en.wikipedia.org/wiki/Data_deduplication Data Deduplication]
* [http://en.wikipedia.org/wiki/Lossy_data_compression Data Compression] with zle (fast) or gzip (higher compression)
* Snapshots (like differencial backups)
* NFS export of volumes

== Installation ==
The CDDL licence is incompatible with the GPL licence, therefore it is not allowed to build the modules into the Linux kernel. However, there is a package that allows us to load the modules in userspace.<br/>
{{Package|sys-fs/zfs-fuse}} is available on Gentoo Linux and supports the following useflags
* ''bash-completion'' = Enable bash-completion support
* ''debug'' = Enable extra debug codepaths
You can install zfs-fuse using the following command:
{{ShellRoot|emerge -av sys-fs/zfs-fuse}}
Start and add zfs-fuse to the boot runlevel:
{{ShellRoot|/etc/init.d/zfs-fuse start && rc-update add zfs-fuse boot}}
The configuration files are located in
{{Code|Configuration Files|<pre>
/etc/conf.d/zfs-fuse</pre>}}
{{Note|there is also a native port underway [http://zfsonlinux.org/ zfsonlinux.org]}}

== Usage ==
ZFS includes already all programs to manage the hardware and the file systems, there are no additional tools needed.

=== Preparation ===
To go through the different commands and scenarios we can create virtual hard drives using loopback devices.<br/>
First we need to make sure the loopback module is loaded. If you want to play around with partitions, use the following option:
{{ShellRoot|<nowiki>modprobe -r loop && modprobe loop max_part=63</nowiki>}}
{{Note|you cannot reload the module, if it is built into the kernel}}
The following commands create 2GB image files in /var/lib/zfs_img/ that we use as our hard drives (uses ~8GB disk space):

{{ShellRoot|mkdir /var/lib/zfs_img<br/>
<nowiki>dd if=/dev/zero of=/var/lib/zfs_img/zfs0.img bs=1024 count=2097152</nowiki><br/>
<nowiki>dd if=/dev/zero of=/var/lib/zfs_img/zfs1.img bs=1024 count=2097152</nowiki><br/>
<nowiki>dd if=/dev/zero of=/var/lib/zfs_img/zfs2.img bs=1024 count=2097152</nowiki><br/>
<nowiki>dd if=/dev/zero of=/var/lib/zfs_img/zfs3.img bs=1024 count=2097152</nowiki>}}
Now we check which loopback devices are in use:
{{ShellRoot|losetup -a}}
We assume that all loopback devices are available and create our hard drives:
{{ShellRoot|losetup /dev/loop0 /var/lib/zfs_img/zfs0.img<br/>
losetup /dev/loop1 /var/lib/zfs_img/zfs1.img<br/>
losetup /dev/loop2 /var/lib/zfs_img/zfs2.img<br/>
losetup /dev/loop3 /var/lib/zfs_img/zfs3.img}}
We have now /dev/loop[0-3] as four hard drives available
{{Note|On the next reboot, all the loopback devices will be released and the folder /var/lib/zfs_img can be deleted}}

=== Zpools ===
The program /usr/sbin/zpool is used with any operation regarding ''zpools''.

==== import/export Zpool ====
To export (unmount) an existing zpool named ''zfs_test'' into the file system, you can use the following command:
{{ShellRoot|zpool export zfs_test}}
{{ShellRoot|zpool status}}
To import (mount) the zpool named ''zfs_test'' use this command:
{{ShellRoot|zpool import zfs_test}}
{{ShellRoot|zpool status}}
{{Note|ZFS will automatically search on the hard drives for the zpool named ''zfs_test''}}

==== One Hard Drive ====
Create a new zpool named ''zfs_test'' with one hard drive:
{{ShellRoot|zpool create zfs_test /dev/loop0}}
The zpool will automatically be mounted, default is the root file system aka /zfs_test
{{ShellRoot|zpool status}}
To delete a zpool use this command:
{{ShellRoot|zpool destroy zfs_test}}
{{Important|ZFS will not asked twice if you really want to}}

==== MIRROR Two Hard Drives ====
In ZFS you can have several harddrives in a MIRROR, where equal copies exist on each storage. This increases the performance and redundancy. To create a new zpool named ''zfs_test'' with two hard drives as MIRROR:
{{ShellRoot|zpool create zfs_test mirror /dev/loop0 /dev/loop1}}
{{Note|of the two hard drives only 2GB are affective useable so ''total_space * 1/n''}}
{{ShellRoot|zpool status}}
To delete the zpool:
{{ShellRoot|zpool destroy zfs_test}}

==== RAIDZ1 Three Hard Drives ====
RAIDZ1 is the equivalent to RAID5, where data is written to the first two drives and a parity onto the third. You need at least three hard drives, one can fail and the zpool is still ONLINE but the faulty drive should be replaced as soon as possible.<br/>
To create a pool with RAIDZ1 and three hard drives:
{{ShellRoot|zpool create zfs_test raidz1 /dev/loop0 /dev/loop1 /dev/loop2}}
{{Note|of the three hard drives only 4GB are affective useable so ''total_space * (1-1/n)''}}
{{ShellRoot|zpool status}}
To delete the zpool:
{{ShellRoot|zpool destroy zfs_test}}

==== RAIDZ2 Four Hard Drives ====
RAIDZ2 is the equivalent to RAID6, where data is written to the first two drives and a parity onto the next two. You need at least four hard drives, two can fail and the zpool is still ONLINE but the faulty drives should be replaced as soon as possible.<br/>
To create a pool with RAIDZ2 and four hard drives:
{{ShellRoot|zpool create zfs_test raidz2 /dev/loop0 /dev/loop1 /dev/loop2 /dev/loop3}}
{{Note|of the four hard drives only 4GB are affective useable so ''total_space * (1-2/n)''}}
{{ShellRoot|zpool status}}
To delete the zpool:
{{ShellRoot|zpool destroy zfs_test}}

== Links ==
* [http://zfs-fuse.net/ zfs-fuse.net]
