[http://en.wikipedia.org/wiki/ZFS ZFS] was developed by SUN Microsystems and is an advanced file system that includes many features like:
* Manage storage hardware as vdev in ''zpools''
* Manage volumes in ''zpools'' (like [[LVM]])
* Redundancy with support for RAIDZ1(RAID5),RAIDZ2(RAID6) and MIRROR(RAID1)
* Resilvering file system
* [http://en.wikipedia.org/wiki/Data_deduplication Data Deduplication]
* [http://en.wikipedia.org/wiki/Lossy_data_compression Data Compression] with zle (fast) or gzip (higher compression)
* Snapshots (like differencial backups)
* NFS export of volumes

== Installation ==
The CDDL licence is incompatible with the GPL licence, therefore it is not allowed to build the modules into the Linux kernel. However, there is a package that allows us to load the modules in userspace.<br/>
{{Package|sys-fs/zfs-fuse}} is available on Gentoo Linux and supports the following useflags
* ''bash-completion'' = Enable bash-completion support
* ''debug'' = Enable extra debug codepaths
You can install zfs-fuse using the following command:
{{ShellRoot|emerge -av sys-fs/zfs-fuse}}
Start and add zfs-fuse to the boot runlevel:
{{ShellRoot|/etc/init.d/zfs-fuse start && rc-update add zfs-fuse boot}}
The configuration files are located in
{{Code|Configuration Files|<pre>
/etc/conf.d/zfs-fuse</pre>}}
{{Note|there is also a native port underway [http://zfsonlinux.org/ zfsonlinux.org]}}

== Usage ==
ZFS includes already all programs to manage the hardware and the file systems, there are no additional tools needed.

=== Preparation ===
To go through the different commands and scenarios we can create virtual hard drives using loopback devices.<br/>
First we need to make sure the loopback module is loaded. If you want to play around with partitions, use the following option:
{{ShellRoot|<nowiki>modprobe -r loop && modprobe loop max_part=63</nowiki>}}
{{Note|you cannot reload the module, if it is built into the kernel}}
The following commands create 2GB image files in /var/lib/zfs_img/ that we use as our hard drives (uses ~8GB disk space):

{{ShellRoot|mkdir /var/lib/zfs_img<br/>
<nowiki>dd if=/dev/zero of=/var/lib/zfs_img/zfs0.img bs=1024 count=2097152</nowiki><br/>
<nowiki>dd if=/dev/zero of=/var/lib/zfs_img/zfs1.img bs=1024 count=2097152</nowiki><br/>
<nowiki>dd if=/dev/zero of=/var/lib/zfs_img/zfs2.img bs=1024 count=2097152</nowiki><br/>
<nowiki>dd if=/dev/zero of=/var/lib/zfs_img/zfs3.img bs=1024 count=2097152</nowiki>}}
Now we check which loopback devices are in use:
{{ShellRoot|losetup -a}}
We assume that all loopback devices are available and create our hard drives:
{{ShellRoot|losetup /dev/loop0 /var/lib/zfs_img/zfs0.img<br/>
losetup /dev/loop1 /var/lib/zfs_img/zfs1.img<br/>
losetup /dev/loop2 /var/lib/zfs_img/zfs2.img<br/>
losetup /dev/loop3 /var/lib/zfs_img/zfs3.img}}
We have now /dev/loop[0-3] as four hard drives available
{{Note|On the next reboot, all the loopback devices will be released and the folder /var/lib/zfs_img can be deleted}}

=== Zpools ===
The program /usr/sbin/zpool is used with any operation regarding ''zpools''.

==== import/export Zpool ====
To export (unmount) an existing zpool named ''zfs_test'' into the file system, you can use the following command:
{{ShellRoot|zpool export zfs_test}}
{{ShellRoot|zpool status}}
To import (mount) the zpool named ''zfs_test'' use this command:
{{ShellRoot|zpool import zfs_test}}
{{ShellRoot|zpool status}}
{{Note|ZFS will automatically search on the hard drives for the zpool named ''zfs_test''}}

==== One Hard Drive ====
Create a new zpool named ''zfs_test'' with one hard drive:
{{ShellRoot|zpool create zfs_test /dev/loop0}}
The zpool will automatically be mounted, default is the root file system aka /zfs_test
{{ShellRoot|zpool status}}
To delete a zpool use this command:
{{ShellRoot|zpool destroy zfs_test}}
{{Important|ZFS will not asked twice if you really want to}}

==== MIRROR Two Hard Drives ====
In ZFS you can have several harddrives in a MIRROR, where equal copies exist on each storage. This increases the performance and redundancy. To create a new zpool named ''zfs_test'' with two hard drives as MIRROR:
{{ShellRoot|zpool create zfs_test mirror /dev/loop0 /dev/loop1}}
{{Note|of the two hard drives only 2GB are affective useable so ''total_space * 1/n''}}
{{ShellRoot|zpool status}}
To delete the zpool:
{{ShellRoot|zpool destroy zfs_test}}

==== RAIDZ1 Three Hard Drives ====
RAIDZ1 is the equivalent to RAID5, where data is written to the first two drives and a parity onto the third. You need at least three hard drives, one can fail and the zpool is still ONLINE but the faulty drive should be replaced as soon as possible.<br/>
To create a pool with RAIDZ1 and three hard drives:
{{ShellRoot|zpool create zfs_test raidz1 /dev/loop0 /dev/loop1 /dev/loop2}}
{{Note|of the three hard drives only 4GB are affective useable so ''total_space * (1-1/n)''}}
{{ShellRoot|zpool status}}
To delete the zpool:
{{ShellRoot|zpool destroy zfs_test}}

==== RAIDZ2 Four Hard Drives ====
RAIDZ2 is the equivalent to RAID6, where data is written to the first two drives and a parity onto the next two. You need at least four hard drives, two can fail and the zpool is still ONLINE but the faulty drives should be replaced as soon as possible.<br/>
To create a pool with RAIDZ2 and four hard drives:
{{ShellRoot|zpool create zfs_test raidz2 /dev/loop0 /dev/loop1 /dev/loop2 /dev/loop3}}
{{Note|of the four hard drives only 4GB are affective useable so ''total_space * (1-2/n)''}}
{{ShellRoot|zpool status}}
To delete the zpool:
{{ShellRoot|zpool destroy zfs_test}}

==== Spares/Replace vdev ====
You can add ''hot-spares'' into your zpool. In case a failure, those are already installed and available to replace faulty vdevs.
In this example, we use RAIDZ1 with three hard drives and a zpool named ''zfs_test'':
{{ShellRoot|zpool add zfs_test spare /dev/loop3}}
{{ShellRoot|zpool status}}
The status of /dev/loop3 will stay AVAIL until it is set to be ''online'', now we let /dev/loop0 fail:
{{ShellRoot|zpool offline zfs_test /dev/loop0}}
{{Code|zpool status|<pre>
NAME        STATE     READ WRITE CKSUM
zfs_test    DEGRADED     0     0     0
  raidz1-0  DEGRADED     0     0     0
    loop0   OFFLINE      0     0     0
    loop1   ONLINE       0     0     0
    loop2   ONLINE       0     0     0
spares
  loop3     AVAIL</pre>}}
We replace /dev/loop0 with our spare /dev/loop3:
{{ShellRoot|zpool replace zfs_test /dev/loop0 /dev/loop3}}
{{Code|zpool status|<pre>
  pool: zfs_test
 state: DEGRADED
status: One or more devices has been taken offline by the administrator.
        Sufficient replicas exist for the pool to continue functioning in a
        degraded state.
action: Online the device using 'zpool online' or replace the device with
        'zpool replace'.
 scrub: resilver completed after 0h0m with 0 errors on Sun Aug 21 22:29:22 2011
config:

        NAME         STATE     READ WRITE CKSUM
        zfs_test     DEGRADED     0     0     0
          raidz1-0   DEGRADED     0     0     0
            spare-0  DEGRADED     0     0     0
              loop0  OFFLINE      0     0     0
              loop3  ONLINE       0     0     0  46.5K resilvered
            loop1    ONLINE       0     0     0
            loop2    ONLINE       0     0     0
        spares
          loop3      INUSE     currently in use

errors: No known data errors</pre>}}
{{Note|the file system got automatically resilvered onto /dev/loop3 and the zpool was all the time online}}
Now we remove the failed vdev /dev/loop0 and start a manual scrubbing:
{{ShellRoot|zpool detach zfs_test /dev/loop0 && zpool scrub}}
{{Code|zpool status|<pre>
  pool: zfs_test
 state: ONLINE
 scrub: scrub completed after 0h0m with 0 errors on Sun Aug 21 22:37:52 2011
config:

        NAME        STATE     READ WRITE CKSUM
        zfs_test    ONLINE       0     0     0
          raidz1-0  ONLINE       0     0     0
            loop3   ONLINE       0     0     0
            loop1   ONLINE       0     0     0
            loop2   ONLINE       0     0     0

errors: No known data errors</pre>}}

==== Zpool Version Update ====
With every update of {{Package|sys-fs/zfs-fuse}}, you are likely to also get a more recent ZFS version. Also the status of your zpools will indicate a warning that a new version is available and the zpools could be upgraded.<br/>
To display the current version on a zpool:
{{Code|zpool upgrade -v|<pre>
This system is currently running ZFS pool version 23.

The following versions are supported:

VER  DESCRIPTION
---  --------------------------------------------------------
 1   Initial ZFS version
 2   Ditto blocks (replicated metadata)
 3   Hot spares and double parity RAID-Z
 4   zpool history
 5   Compression using the gzip algorithm
 6   bootfs pool property
 7   Separate intent log devices
 8   Delegated administration
 9   refquota and refreservation properties
 10  Cache devices
 11  Improved scrub performance
 12  Snapshot properties
 13  snapused property
 14  passthrough-x aclinherit
 15  user/group space accounting
 16  stmf property support
 17  Triple-parity RAID-Z
 18  Snapshot user holds
 19  Log device removal
 20  Compression using zle (zero-length encoding)
 21  Deduplication
 22  Received properties
 23  Slim ZIL</pre>}}
{{Warning|systems with a lower version installed will not be able to import a zpool of a higher version}}
To upgrade the version of zpool ''zfs_test'':
{{ShellRoot|zpool upgrade zfs_test}}
To upgrade the version of all zpools in the system:
{{ShellRoot|zpool upgrade -a}}

== Links ==
* [http://zfs-fuse.net/ zfs-fuse.net]
