{{InfoBox stack
|{{InfoBox homepage|http://www.xenproject.org|header=true}}
|{{InfoBox wikipedia|Xen}}
}}

[http://wiki.xenproject.org/wiki/Xen_Project_Software_Overview Xen] is a native, or bare-metal, [http://en.wikipedia.org/wiki/Hypervisor hypervisor]  that allows multiple distinct virtual machines (referred to as domains) to share a single physical machine. As the highest privilege process on the system, Xen is responsible for the distribution of processor and memory resources between guest domains on the host. Other hardware resources such as network interfaces, disks, or direct PCI/USB devices are controlled by a privileged domain known as domain-0 (dom0).

From it's inception Xen has focused on the [https://en.wikipedia.org/wiki/Paravirtualization para-virtualization] approach to hypervisor design. As a result, Xen guests or unprivileged domains (domU) are typically aware of the hypervisor and their status as guests. The base system, Domain-0, must have inherent Xen support, however, unmodified domU guests are supported on hardware which implements Intel (VT-x) or AMD (SVM) virtualization technology.

== Installation (domain-0) ==

=== BIOS / UEFI Firmware ===

While paravirtualized (PV) guests can run on a host without any form of hardware accelerated virtualization extensions this severely restricts the set of available guests and prevents the use of new and exciting modes such as [http://wiki.xen.org/wiki/Xen_Project_Software_Overview#PVH PVH]. 

The following command should provide a highlighted row for each processor if the corresponding feature is present:

{{cmd|grep --color -E "vmx{{!}}svm" /proc/cpuinfo}}

Unfortunately motherboard manufacturers often explicitly disable these virtualization extensions via BIOS or UEFI settings. If you don't see support listed check your BIOS/UEFI configuration for flags related to virtualization.

=== Rebuilding the Gentoo Installation? ===

A dramatic change that might be necessary on 32-bit systems is to rebuild the entire Gentoo installation with a different <code>CFLAGS</code> setting. Guest operating systems running under Xen might otherwise see major performance degradation. If you, however, are planning on checking out Xen rather than installing it for production use and are not terribly fond of rebuilding all programs, you can skip this step. In this case you will notice performance degradation but you will still be able to use Xen. 

{{Important|It is advised that, if you change your <code>CFLAGS</code> and build your system with a gcc lower than version 4, you do not have <code>-Os</code> set as it has been reported to produce broken code.}}

Add -mno-tls-direct-seg-refs ONLY if you have a 32-bit dom0. You don't need this flag if you have a 64-bit dom0.

{{FileBox|filename=/etc/portage/make.conf|title=CFLAGS change for mno-tls-direct-seg-refs|lang=bash|1=
CFLAGS="... -mno-tls-direct-seg-refs"
}}

{{RootCmd|emerge -e world}}

If you boot your system using an initial ramdisk (initrd) you need to rebuild the initrd as well (which is best done by running all steps you would do when you rebuild your kernel). 

=== Kernel ===

Next we'll build the Linux kernel with Xen support. This kernel, whose sources are available at {{Path|/usr/src/linux}} , will be our main running kernel (i.e. the one running domain 0). In the <code>XEN</code> section you'll find drivers for all kinds of input/output, each driver having a ''backend'' and ''frontend'' implementation available. For the domain 0 kernel you need to select the ''backend'' implementation: these are used by the other domains (who use the ''frontend'' drivers) to communicate directly with the hardware. However, you should be able to configure the kernel to provide support for both frontend (guest) and backend (host) drivers. 

If you're wondering about networking: each interface in a domain has a point-to-point link to an interface on domain 0 (called {{Path|vifX.Y}} where X is the domain number and Y the Yth interface of that domain), so you can configure your network the way you want (bridging, NAT, etc.) 

Enable general Xen support:
{{KernelBox|title=Xen Base|
Processor type and features  --->
     [*] Linux guest support  --->
          [*]   Enable paravirtualization code
          [*]     Xen guest support
          [*]       Support for running as a PVH guest
          [*]     Paravirtualization layer for spinlocks          
}}

Add support for paravirtualized console connections:
{{KernelBox|title=PV Console|
Device Drivers  --->
     Character devices  --->
          [*] Xen Hypervisor Console support
          [*]   Xen Hypervisor Multiple Consoles support
}}

Facilitates guest access to block and network devices via dom0:
{{KernelBox|title=Disk and Network|
Device Drivers  --->
     [*] Block devices  --->
          <*>   Xen virtual block device support
     [*] Network device support  --->
          <*>   Xen network device frontend driver
}}

In some configurations it can be desirable to provide a guest with direct access to a PCI device. This is known as [http://wiki.xenproject.org/wiki/XenPCIpassthrough Xen PCI Passthrough ]:
{{KernelBox|title=Guest PCI Passthrough|
Bus options (PCI etc.)  --->
     <*> Xen PCI Frontend
}}

Keyboard, mouse, and display support via dom0 backend:
{{KernelBox|title=Guest Human Interface|
Device Drivers  --->
     Input device support  --->
          [*]   Miscellaneous devices  --->
               <*>   Xen virtual keyboard and mouse support
     Graphics support  --->
          Frame buffer Devices  --->
               <*> Xen virtual frame buffer support
}}

Xen dom0 support depends on APCI; without it dom0 related options will be hidden:
{{KernelBox|title=APCI Support|
Power management and ACPI options  --->
     [*] ACPI (Advanced Configuration and Power Interface) Support  --->
}}

Typical network configurations depend on linux bridge functionality:
{{KernelBox|title=Linux Bridge|
[*] Networking support --->
     Networking options  --->
          <*> 802.1d Ethernet Bridging
	  [*] Network packet filtering framework (Netfilter) --->
	       [*] Advanced netfilter configuration
	       [*]   Bridged IP/ARP packets filtering
}}

[http://wiki.xenproject.org/wiki/Xen_Project_Software_Overview#HVM Fully virtualised (HVM)] guests depend on Universal TUN/TAP device driver support for their network interfaces. This option is required if you plan to create fully [http://wiki.xenproject.org/wiki/Xen_Networking#Emulated_Network_Devices Emulated Network Devices] within Dom0/DomU configuration.
{{KernelBox|title=TUN and TAP virtual network kernel devices|
[*] Device Drivers  --->
     Network device support --->
     <M> Universal TUN/TAP device driver support
}}

The remaining drivers flesh out memory management, domain-to-domain communication, and communication to Xen via sysfs interfaces:
{{KernelBox|title=Xen Drivers|
Device Drivers  --->
     [*] Block devices  --->
          <*>   Xen block-device backend driver
     [*] Network device support --->
          <*>   Xen backend network device
     Xen driver support  --->
          [*] Xen memory balloon driver
          [*]   Scrub pages before returning them to system
          <*> Xen /dev/xen/evtchn device
          [*] Backend driver support
          <*> Xen filesystem
          [*]   Create compatibility mount point /proc/xen
          [*] Create xen entries under /sys/hypervisor
          <*> userspace grant access device driver
          <*> User-space grant reference allocator driver
          <M> Xen PCI-device backend driver
          <*> Xen ACPI processor
          [*] Xen platform mcelog
}}

With all of the above configuration enabled, this kernel image should be able to boot as the dom0 host or as another domU guest. Note that the domU kernel can be slimmed down significantly if desired.

=== USE Flags ===

The first set of use flags correspond directly to the Xen hypervisor. Of these, ensure that EFI support is selected if you plan to configure EFI boot.
{{USEflag|package=app-emulation/xen|desc=USE flags for {{Package|app-emulation/xen}}:
|efi
|flask
|xsm
}}

In addition to the core hypervisor Xen depends on a set of supporting libraries and management tools. 
{{USEflag|package=app-emulation/xen-tools|desc=USE flags for {{Package|app-emulation/xen-tools}}:
|api
|flask
|hvm++yes
|ocaml
|pam
|pygrub
|python
|qemu
|screen++yes
}}

{{Note|In case you need hardware virtualization, the 'hvm' USE flag must be enabled for the xen-tools package.  This flag can not be enabled on no-multilib installations.  You need a multilib installation (and have a multilib profile selected) to enable the 'hvm' USE flag.  (You also need a CPU which supports hardware virtualization.)

In case 'equery uses xen-tools' does not list the 'hvm' flag at all, or when 'emerge -a xen-tools' shows this flag in brackets, the flag cannot be enabled (unless you figure out how to unmask it).  Simply selecting a multilib profile on hosts that do not have a multilib installation is likely to give you error messages like about a missing stub-32.h when the xen-tools package is compiled, and emerging the package fails.  You may need to re-install the host to a multilib installation then. (32bit support in the kernel is not required.)
}}

=== Emerge ===

Xen will automatically install it's boot files in /boot. If you have a separate boot file system ensure that it is mounted at this point.

{{RootCmd|mount /boot}}

After you have reviewed and updated the relevant USE flags, emerge xen and xen-tools:

{{RootCmd|emerge --ask app-emulation/xen app-emulation/xen-tools}}

Once the installation process completes {{Path|/boot/xen.gz}} should be present.

=== Bootloader ===

In order for Xen to be the highest privilege process it must be the initial boot target, however, it must also subsequently boot the dom0 kernel.

==== Legacy GRUB ====
The following configuration handles basic [[GRUB]] settings:

{{FileBox|filename=/boot/grub/grub.conf|title=GRUB Configuration for Xen|1=
title Xen Gentoo Linux 4.0
root (hd0,0)
kernel /boot/xen.gz
module /boot/kernel-4.0.x.y-xen0 root=/dev/sda3
}}

In the above configuration GRUB will load xen.gz as the kernel with the actual dom0 kernel provided as a module.

==== GRUB2 ====

[[GRUB2]] provides auto-configuration scripts through grub2-mkconfig which will generate the appropriate settings based on your kernel {{Path|.config}}. If the scripts detect Xen dom0 options they will append Xen hypervisor boot lines to the grub menu. Note that for this to function correctly the kernel config file must be located in one of the following directories with a suffix matching the desired kernel:

{| class="table"
! Directory !! File Prefix !! Example
|-
| /etc/kernels/ || kernel-config-* || /etc/kernels/kernel-config-4.0.5-gentoo
|-
| /boot || config-* || /boot/config-4.0.5-gentoo
|}

The example column above assumes a kernel named /boot/kernel-4.0.5-gentoo.

The following variables in {{Path|/etc/default/grub}} are the most common ones to set to control how GRUB2 will function:

{| class="table"
! Variable
! Explanation
|-
| <var>GRUB_CMDLINE_XEN</var>
| Extra parameters to be passed on the Xen command line for all Xen menu entries. For instance, to have a timestamped Xen dmesg, users will need to add <code>GRUB_CMDLINE_XEN="console_timestamps=boot"</code>.
|-
| <var>GRUB_CMDLINE_XEN_DEFAULT</var>
| Parameters to be passed on the Xen command line for non-recovery Xen menu entries.
|}

For a more complete list, please refer to the [[GRUB2/Config_Variables|GRUB2 configuration variables]] sub-page.

GRUB2 support is also included in [[genkernel]] and can be triggered during the build process:

{{RootCmd|genkernel --oldconfig --menuconfig --install --bootloader{{=}}grub --symlink --disklabel --lvm --mdadm --makeopts{{=}}-j9 all}}

Once you are satisfied with your configuration reboot the system into Xen. If everything worked correctly Xen should transfer display control to the dom0 environment at the end of the boot process which should operate just like your original native system. At this point you may want to set Xen as the default boot option via updating {{path|/etc/default/grub}}.

=== Runlevel ===

In order for your new Xen environment to function correctly a number of daemons must be added to your default runlevel. 

{| class="table"
! Init Script !! Required !! Description
|-
| xencommons
| yes
| This daemon ensures all Xen related kernel modules are loaded in a dom0 system. Additionally, in dom0 and domU pv_ops systems it ensures /proc/xen is mounted as type xenfs.
|-
| xenconsoled
| yes
| Manages xl console sessions between dom0 and domU guests.
|-
| xendomains
| no
| Automatically starts all domU configuration files found in {{Path|/etc/xen/auto/}}.
|-
| xenstored
| yes
| Manages both disk and network I/O configuration and operations between the dom0 environment and other domU guests.
|-
| xen-watchdog
| ?
| Probably kills the dom0 environment or Xen in general if it becomes unresponsive for too long.
|}

{{Note|The xen-watchdog init script currently has not been migrated from sysvinit to the runscript/openRC format.}}

=== Performance Tuning ===
When configuring a host with a significant amount of memory it is beneficial to fix the allocated to dom0 and disable ballooning functionality. This ensures Xen and the dom0 operating system are able to build working structures based on the memory available at boot-time and have their assumptions hold throughout operation. Additionally, in the event that a large number of guests are created, dom0 is explicitly protected from external memory pressure.

In a GRUB2 environment the following directive assigns 1 GiB of memory to the dom0 guest.
{{FileBox|filename=/etc/default/grub|title=GRUB2 Limit dom0 Memory|1=
GRUB_CMDLINE_XEN_DEFAULT="dom0_mem=1024M,max:1024M"
}}

Ballooning should also be disabled by setting autoballoon to either "off" or "auto" in {{Path|/etx/xen/xl.conf}}.

Further detail on this topic and other related tuning suggestions can be found on the [http://wiki.xen.org/wiki/Tuning_Xen_for_Performance Tuning Xen for Performance] article on the Xen wiki.

== Creating an Unpriviledged Domain (domU) ==

=== Building the Kernel ===

Go to the Xen-powered Linux kernel source and, if necessary, update the configuration. It is wise to keep as many topics as possible similar to the main kernel. Then build the kernel and place the resulting {{Path|vmlinuz}} file where you want (we assume this is {{Path|/mnt/data/xen/kernel}} ): 

{{RootCmd|make O{{=}}~/build/domU
|cp ~/build/domU/vmlinuz /mnt/data/xen/kernel/kernel-3.5.x.y-xen}}

{{Note|On modern systems (e.g. xen-4.3.2, kernel-3.12) it is possible to use only one kernel for both: dom0 and domU. Just configure all options needed for a guest system in the main kernel. Don't forget to copy the modules {{Path|/lib/modules/<your kernel>}} to the guest system. }}

If you'd like to trim down the domU kernel the following flags are necessary.

Enable general Xen support:
{{KernelBox|title=Xen Base|
Processor type and features  --->
     [*] Linux guest support  --->
          [*]   Enable paravirtualization code
          [*]     Xen guest support
          [*]       Support for running as a PVH guest
          [*]     Paravirtualization layer for spinlocks          
}}

Facilitates guest access to block and network devices via dom0:
{{KernelBox|title=Disk and Network|
Device Drivers  --->
     [*] Block devices  --->
          <*>   Xen virtual block device support
     [*] Network device support  --->
          <*>   Xen network device frontend driver
}}

In some configurations it can be desirable to provide a guest with direct access to a PCI device. This is known as [http://wiki.xenproject.org/wiki/XenPCIpassthrough Xen PCI Passthrough ]:
{{KernelBox|title=Guest PCI Passthrough|
Bus options (PCI etc.)  --->
     [*] Xen PCI Frontend
}}

Keyboard, mouse, and display support via dom0 backend:
{{KernelBox|title=Guest Human Interface|
Device Drivers  --->
     Input device support  --->
          [*]   Miscellaneous devices  --->
               <*>   Xen virtual keyboard and mouse support
     Graphics support  --->
          Frame buffer Devices  --->
               <*> Xen virtual frame buffer support
}}

The remaining drivers flesh out memory management, domain-to-domain communication, and communication to Xen via sysfs interfaces:
{{KernelBox|title=Xen Drivers|
Device Drivers  --->
     Xen driver support  --->
          [*] Xen memory balloon driver
          [*]   Scrub pages before returning them to system
          <*> Xen /dev/xen/evtchn device
          <*> Xen filesystem
          [*]   Create compatibility mount point /proc/xen
          [*] Create xen entries under /sys/hypervisor
          <*> userspace grant access device driver
          <*> User-space grant reference allocator driver
          <*> Xen ACPI processor
          [*] Xen platform mcelog
}}

=== Creating the Domain Disks ===

For best performance, it is best to dedicate a partition (or logical volume) to a domain rather than a file based filesystem. However, if you are going to use Xen primarily for tests using a file based filesystem does have its advantages (especially regarding maintenance). 

You can create a file based filesystem using <code>dd</code> and <code>mke2fs</code> (or any other file system creation tool). For instance, to create a 4 Gbyte ext4 filesystem: 

{{RootCmd|dd if{{=}}/dev/zero of{{=}}/mnt/data/xen/disks/ext4root.img bs{{=}}1M count{{=}}4096
|mkfs.ext4 /mnt/data/xen/disks/ext4root.img}}

=== Configuring a Domain ===

Next we create a Xen configuration file for a domain. You can store these configuration files where you want, for instance at {{Path|/mnt/data/xen/configs}} . As an example, we create a configuration file for a small Gentoo environment which uses the disk image we created previously: 

{{FileBox|filename=/mnt/data/xen/configs/gentoo|lang=python|1=
kernel = "/mnt/data/xen/kernel/kernel-3.5.x.y-xen"
memory = 512
name   = "gentoo"
(Map the disk image to the virtual /dev/sda1)
disk   = ['file:/mnt/data/xen/disks/ext4root.img,sda1,w']
root   = "/dev/sda1 ro"
}}

If you are using a block device (such as an lvm volume or partition) for the disk use 'phy:' instead of 'file:' and leave off /dev. For example: 

{{CodeBox|title=Using a block device|lang=python|1=
(LVM Volume)
disk = [ 'phy:lvm/xen-guest-root,sda1,w' ]
  
(Physical Partition)
disk = [ 'phy:sdb6,sda1,w' ]
}}

You can find example configuration files in {{Path|/etc/xen}}, which is also the default location for domU config files.

=== Launching the New Domain ===

Now we're all set and we can launch the new domain. If the disk image contained an operating system, we could just create and attach the domain using the <code>xl</code> command: 

{{RootCmd|xl create /mnt/data/xen/gentoo -c}}

The domain would be booted inside the terminal in which you executed the command. However, in our case, the disk image is empty so the domain won't boot up in anything useful. To fix this, you can loop-mount the image and install Gentoo as you're used to. 

If you want to disconnect from the domain, press {{Path|Ctrl+]}} . You can always reconnect to the domains' console using <code>xl console gentoo</code> . However, there is only one console per domain, so only use it when you can't access the domain otherwise (for instance, through SSH).

If you are missing login prompt on the console, make sure you have entries like this in your inittab files on dom0 and domU pointing to {{Path|/dev/hvc0}}:

{{FileBox|filename=dom0:/etc/inittab|1=
c0:2345:respawn:/sbin/agetty 38400 hvc0 linux
}}

{{FileBox|filename=domU:/etc/inittab|1=
c0:2345:respawn:/sbin/agetty 38400 hvc0 linux
}}

To apply the changes in {{Path|/etc/inittab}} without a reboot issue the following command:
{{RootCmd|telinit q}}

If it still does not work, check the kernel config for the entries: {{Path|DEVTMPFS}} and {{Path|DEVTMPFS_MOUNT}}. They should be set.

{{KernelBox|title=Kernel Config DEVTMPFS|
Device Drivers --->
    Generic Driver Options  --->
        -*- Maintain a devtmpfs filesystem to mount at /dev
        [*]   Automount devtmpfs at /dev, after the kernel mounted the rootfs
}}

== Networking on Domains ==

=== Introduction ===

Xen works best when using a bridged mode network configuration. This means that your default network interface on the administrative domain becomes a bridge which accepts connections to the virtual domains as well as to the IP address your administrative domain has. 

=== Bridged Interfaces ===

Create a bridge interface by creating a new link to the networking init script as provided by Gentoo: 

{{RootCmd|cd /etc/init.d
|ln -s net.lo net.br0}}

Next, edit {{Path|/etc/conf.d/net}} and setup the bridge: 

{{FileBox|filename=dom0:/etc/conf.d/net|lang=bash|1=
# eth0 should NOT have an ip configured
config_eth0="null"

# configure bridge to replace eth0 on dom0. Make sure the netmask for the bridge includes ip addresses of all your domU's!
bridge_br0="eth0"
config_br0="192.168.XX.XX netmask 255.255.0.0 brd 192.168.255.255"
routes_br0="default via 192.168.XX.XX"
mac_br0="00:16:3e:5b:XX:XX"

# bridge options to make interface coming up immediately
brctl_br0="stp off
        setfd 0
        sethello 10"

rc_net_br0_need="net.eth0"
rc_net_br0_provide="!net"
}}

{{FileBox|filename=domU:/etc/conf.d/net|lang=bash|1=
config_eth0="192.168.1.200 netmask 255.255.255.0 brd 192.168.1.255"
# route all trafic through dom0 bridge address
routes_eth0="default via 192.168.XX.XX"
# make sure all your domU's have different mac addresses! Set them if needed in xen domU 
# config files in /etc/xen/<domU_name> with "vif = [ "ip=192.68.XX.XX,mac=XX:XX:XX:XX:XX:XX,bridge=br0" ];" !
}}

Finally, install the {{Package|net-misc/bridge-utils}} package, and make sure the {{Path|net.br0}} init script is loaded at boot. 

{{RootCmd|emerge net-misc/bridge-utils
|rc-update add net.br0 default}}

If you use bridged networks with real internet IP's in hosted environments, it may be necessary to add one or all of the following lines (depending on your environment) in your {{Path|/etc/sysctl.conf}} file to prevent redirects, that can cause intermittent network interruptions:

{{FileBox|filename=/etc/sysctl.conf|lang=ini|1=
net.ipv4.conf.all.send_redirects=0
net.ipv4.conf.eth0.send_redirects=0
net.ipv4.conf.br0.send_redirects=0
net.ipv4.conf.default.send_redirects=0
}}

To get the changes in {{Path|/etc/sysctl.conf}} to work, use:
{{RootCmd|sysctl -p /etc/sysctl.conf}}

if you encounter a poor network performance or if your domU network permanently stops working under heavy load (backup jobs, etc) (from outside it looks, like the instance would crash, but deactivating and activating the interface e.g. form the <code>xl console <domU name></code> with <code>/etc/init.d/net.eth0 stop/start</code>, restores normal operation) , use <code>ethtool</code> to improve/prevent it on all interfaces connected to the bridge (don't forget the bridge himself):

{{RootCmd|ethtool --offload <network device> gso off tso off sg off gro off}}

{{Note|You have to do it after each reboot, so use e.g. {{Path|/etc/crontab}} to make it permanent.}}

== Further Resources ==

=== Xen Documentation ===

*  [http://www.xenproject.org/help/documentation.html Official Xen documentation] 

*  [http://wiki.xen.org/ Xen Wiki] 

=== Xen Tools ===

*  [http://virt-manager.org/ app-emulation/virt-manager] is a graphical tool for administering virtual machines

=== Xen Tuning ===

*  [http://cloudnull.io/2012/07/xenserver-network-tuning Xen network tuning]

[[Category:Hypervisor]] [[Category:virtualization]] {{Migrated|originalauthors=Sven Vermeulen, nightmorph}}
