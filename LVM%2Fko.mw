<languages />

{{Metadata|abstract=LVM은 관리자가 파일 시스템과 사용중인 물리 저장소간의 추상 레이어를 제공하여 메타 장치를 만들 수 있도록 합니다.}}

{{InfoBox stack
|{{InfoBox wikipedia|Logical Volume Manager (Linux)|header=true}}
}}

'''LVM'''('''L'''ogical '''V'''olume '''M'''anager)은 관리자가 파일 시스템과 사용 중인 물리 저장소 사이의 추상 레이어를 제공하여 메타 장치를 만들 수 있게 합니다. (파일 시스템이 있는)메타 장치는 ''논리 볼륨''이며, 저장소 풀에서 사용하는 저장소를 ''볼륨 그룹'' 이라고 합니다. 볼륨 그룹은 데이터를 저장하는 하나 이상의 ''실제 장치''로 구성되어있습니다.

물리 볼륨은 파티션으로 구성되어 있고, 전체 SATA 하드 디스크 드라이브는 JBOD('''J'''ust a '''B'''unch '''O'''f '''D'''isks), RAID 체계, iSCSI, 광 채널, eSATA 등으로 묶습니다.

== 설치 ==

LVM은 설정을 관리하기 위해 커널 수준의 드라이버와 사용자 영역 프로그램으로 다룹니다.

=== 커널 ===

다음 커널 옵션을 활성화하십시오:

{{Kernel||<pre>
Device Drivers  --->
   Multiple devices driver support (RAID and LVM)  --->
       <*> Device mapper support
           <*> Crypt target support
           <*> Snapshot target
           <*> Mirror target
       <*> Multipath target
           <*> I/O Path Selector based on the number of in-flight I/Os
           <*> I/O Path Selector based on the service time
</pre>}}

{{Note/ko|모두 활성화 할 필요는 없습니다. 어떤 옵션은 [[#LVM2_Snapshots_and_LVM2_Thin_Snapshots|LVM2 스냅샷, LVM2 씬 스냅샷]], [[#LVM2_Mirrors|LVM2 미러]], [[#LVM2_RAID_0.2FStripeset|LVM2 RAID 0/스트라이프 집합]], 암호화에 필요합니다.}}

=== 프로그램 ===

{{Package|sys-fs/lvm2}}를 설치하십시오:

{{USEflag|package=sys-fs/lvm2
|clvm
|cman
|lvm1+yes
|readline+yes
|selinux++no
|static
|static-libs
|thin+yes
|udev+yes
}}

{{Emerge|lvm2}}

== 설정 ==

LVM 설정은 다음 몇가지 단계에서 처리할 수 있습니다:
# 관리 유틸리티를 통한 LV, PV, VG 관리
# 설정 파일을 통한 LVM 하위시스템 세부 설정
# 분산 차원 서비스 관리
# 초기화 RAM 파일 시스템 설정

논리, 물리 볼륨 및 볼륨 그룹의 관리는 [[#Usage|사용법]] 장에서 다룹니다.

=== LVM 설정 파일 ===

LVM은 {{Path|/etc/lvm/lvm.conf}}에 더 많이 설정할 수 있는 파일이 있습니다. 대부분의 사용자는 LVM 사용을 시작하는데 이 파일의 설정을 수정할 필요가 없습니다.�

=== 서비스 관리 ===

젠투에서는 볼륨 그룹과 논리 볼륨을 자동으로 감지하고 활성화 하는 LVM 서비스를 제공합니다.

서비스는 init 시스템에서 관리할 수 있습니다.

==== openrc ====

LVM을 별도로 시작하려면:

{{RootCmd|/etc/init.d/lvm start}}

부팅시 LVM을 시작하려면:

{{RootCmd|rc-update add lvm boot}}

==== systemd ====

lvm을 별도로 시작하려면:

{{RootCmd|systemctl start lvm2-monitor.service}}

부팅할 때 LVM을 시작하려면:

{{RootCmd|systemctl enable lvm2-monitor.service}}

=== initramfs에서 LVM 사용하기 ===

대부분의 부트 로더는 LVM에서 직접 부팅할 수 있습니다. 기존의 GRUB은 물론이고 LILO에서도 안됩니다. Grub2는 LVM 선형 논리 볼륨, 미러링한 논리 볼륨, 몇가지 RAID 논리 볼륨으로 부팅할 수 있습니다. 씬 논리 볼륨을 지원하는 부트로더는 없습니다. 

이런 이유로, /boot 파티션은 비 LVM 파티션으로 사용하고 initramfs에서 LVM 루트를 마운트하는 방식을 권장합니다. initramfs 같은요소는 [[Genkernel|genkernel]], genkernel-next, [[dracut]]에서 자동으로 만들 수 있습니다:

* '''genkernel''' can boot from all types except thin volumes (as it neither builds nor copies the thin-provisioning-tools binaries from the build host) and maybe RAID10 (RAID10 support requires LVM2 2.02.98, but genkernel builds 2.02.89, however if static binaries are available it can copy those) 
* '''genkernel-next''' can boot from all types volumes. but needs a new enough app-misc/pax-utils or the resulting thin binaries will be broken (See {{Bug|482504}}) 
* '''dracut''' should boot all types, but only includes thin support in the initramfs if the host being run on has a thin root.

==== Genkernel/Genkernel-next ====

Emerge either {{Package|sys-kernel/genkernel}} or {{Package|sys-kernel/genkernel-next}}. The static USE flag may also be enabled on the package {{Package|sys-fs/lvm2}} so that genkernel will use the system binaries (otherwise it will build its own private copy). The following example will build only an initramfs (not an entire kernel) and enable support for lvm.

{{RootCmd|genkernel --lvm initramfs}}

The genkernel manpage outlines other options depending on system requirements.

The initrd will require parameters to tell it how to start lvm, and they are supplied the same way as other kernel parameters.  For example:

{{File|/etc/default/grub|dolvm을 커널 부팅 매개변수로 넣기|<pre>
GRUB_CMDLINE_LINUX="dolvm"
</pre>}}

==== Dracut ====

{{Package|sys-kernel/dracut}} 꾸러미는 레드햇 프로젝트에서 이식했으며, initramfs를 만드는 유사한 도구를 제공합니다. 시험 목적으로 ~arch 상태에 있기 때문에 이머지를 하려면 사용자 여러분은( {{Path|/etc/portage/package.accept_keywords}} 에서)[[Knowledge_Base:Accepting_a_keyword_for_a_single_package|이 글대로 하셔야합니다]]. 이 과정을 수행하기 전에 {{Path|/etc/portage/make.conf}}에 <code>DRACUT_MODULES="lvm"</code>를 추가해야 합니다. 다른 모듈을 사용한다면, [[Dracut]]을 참고하십시오. 보통 다음 명령을 통해, 활용 가능한 기본 initramfs를 만듭니다.

{{RootCmd|dracut -a lvm}}

The initrd will require parameters to tell it how to start lvm, and they are supplied the same way as other kernel parameters.  For example:

{{File|/etc/default/grub|LVM 지원을 커널 부팅 매개변수로 넣기|<pre>
GRUB_CMDLINE_LINUX="rd.lvm.vg=vol00"
</pre>}}

For a comprehensive list of lvm options within dracut please see the section in the [https://www.kernel.org/pub/linux/utils/boot/dracut/dracut.html#_lvm Dracut Manual].

== 사용법 ==

LVM organizes storage in three different levels as follows:
* hard drives, partitions, RAID systems or other means of storage are initialized as physical volumes (PVs)
* Physical Volumes (PV) are grouped together in Volume Groups (VG)
* Logical Volumes (LV) are managed in Volume Groups (VG)

=== PV(물리 볼륨) ===
물리 볼륨은 LVM을 구축할 실제 하드웨어 또는 저장장치 입니다.

==== 공간 분할 ====

{{Note/ko|단일 LVM 볼륨 그룹에 대해 전체 디스크를 활용하고 싶지 않을 경우에만 볼륨 그룹으로 저장소를 관리하려는 목적의 각 분할 공간 활용이 필요합니다. 전체 디스크를 활용할 가능성이 있다면 이 단계를 건너뛰고 전체 하드디스크 드라이브를 물리 볼륨으로 초기화 하십시오.}}

''LVM''의 분할 공간 형식은 ''8e''(Linux LVM)입니다.

가령,  <code>fdisk</code>로 {{Path|/dev/sda}}의 파티션을 설정한다면:

{{RootCmd|fdisk /dev/sda}}

<code>fdisk</code>에서, 분할 공간을 만들려면 '''n'''키를 사용하고, '''t'''키를 사용하여 분할 공간 형식을 ''8e''로 바꾸십시오.

=== PV 만들기 ===

물리 볼륨은 <code>pvcreate</code>명령으로 만들고 초기화 할 수 있습니다.

가령, 다음 명령을 사용하면 {{Path|/dev/sda}}와 {{Path|/dev/sdb}}의 첫번째 처음 파티션의 물리 볼륨을 만듭니다:

{{RootCmd|pvcreate /dev/sd[ab]1}}

==== PV 목록 표시 ====

<code>pvdisplay</code> 명령으로 시스템에 활성화 된 모든 물리 볼륨의 개요를 볼 수 있습니다.

{{RootCmd|pvdisplay|output=<pre>
 --- Physical volume ---
  PV Name               /dev/sda1
  VG Name               volgrp
  PV Size               160.01 GiB / not usable 2.31 MiB
  Allocatable           yes 
  PE Size               4.00 MiB
  Total PE              40962
  Free PE               4098
  Allocated PE          36864
  PV UUID               3WHAz3-dh4r-RJ0E-5o6T-9Dbs-4xLe-inVwcV
  
 --- Physical volume ---
  PV Name               /dev/sdb1
  VG Name               volgrp
  PV Size               160.01 GiB / not usable 2.31 MiB
  Allocatable           yes 
  PE Size               4.00 MiB
  Total PE              40962
  Free PE               40962
  Allocated PE          0
  PV UUID               b031x0-6rej-BcBu-bE2C-eCXG-jObu-0Boo0x
</pre>}}

더 많은 물리 볼륨을 표시해야 한다면, <code>pvscan</code> 명령으로 비활성화된 물리 볼륨을 감지하고, 활성화 할 수 있습니다.

{{RootCmd|pvscan|output=<pre>
  PV /dev/sda1  VG volgrp        lvm2 [160.01 GiB / 16.01 GiB free]
  PV /dev/sdb1  VG volgrp        lvm2 [160.01 GiB / 160.01 GiB free]
  Total: 2 [320.02 GB] / in use: 2 [320.02 GiB] / in no VG: 0 [0]
</pre>}}

==== PV 제거 ====

LVM은 (다른 방식을 언급하지 않는 한)존재하는 모든 물리 볼륨에 데이터를 넣는데, 연속적인 접근 방식으로 넣습니다. (볼륨 그룹 내)논리 볼륨 논리 볼륨이 단일 물리 볼륨의 남은 공간보다 적다면, 논리 볼륨의 모든 공간을 (단일) 물리 볼륨에 연속적인 형태로 위치하도록 합니다. 이러한 조치는 성능상 이유로 처리됩니다.

If a physical volume needs to be removed from a volume group, the data first needs to be moved away from the physical volume. With the <code>pvmove</code> command, all data on a physical volume is moved to other physical volumes within the same volume group.

{{RootCmd|pvmove -v /dev/sda1}}

Such an operation can take a while depending on the amount of data that needs to be moved. Once finished, there should be no data left on the device. Verify with <code>pvdisplay</code> that the physical volume is no longer used by any logical volume.

The next step is to remove the physical volume from the volume group using <code>vgreduce</code> after which the device can be "deselected" as a physical volume using <code>pvremove</code>:

{{RootCmd|vgreduce vg0 /dev/sda1 && pvremove /dev/sda1}}

=== VG(볼륨 그룹) ===

볼륨 그룹(VG)은 수많은 물리 볼륨을 묶고 장치 파일 시스템에 {{Path|/dev/VG_NAME}}와 같은 식으로 보여줍니다. 볼륨 그룹의 이름은 관리자가 선택합니다.

==== VG 만들기 ====

다음 명령은 {{Path|/dev/sda1}}과 {{Path|/dev/sdb1}} 물리 볼륨을 묶어 ''vg0''이라 하는 볼륨 그룹을 만듭니다.

{{RootCmd|vgcreate vg0 /dev/sd[ab]1}}

==== VG 목록 표시 ====

모든 활성화 볼륨 그룹의 목록을 표시하려면, <code>vgdisplay</code> 명령을 사용하십시오:

{{RootCmd|vgdisplay|output=<pre>
  --- Volume group ---
  VG Name               vg0
  System ID             
  Format                lvm2
  Metadata Areas        1
  Metadata Sequence No  8
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                6
  Open LV               6
  Max PV                0
  Cur PV                1
  Act PV                1
  VG Size               320.02 GiB
  PE Size               4.00 MiB
  Total PE              81924
  Alloc PE / Size       36864 / 144.00 GiB
  Free  PE / Size       45056 /176.01 GiB
  VG UUID               mFPXj3-DdPi-7YJ5-9WKy-KA5Y-Vd4S-Lycxq3
</pre>}}

볼륨 그룹이 빠져있으면 볼륨 그룹을 찾아 가리키는 <code>vgscan</code> 명령을 사용하십시오:

{{RootCmd|vgscan|output=<pre>
  Reading all physical volumes.  This may take a while...
  Found volume group "vg0" using metadata type lvm2
</pre>}}

==== VG 확장 ====

볼륨 그룹은 관리자가 파일 시스템에 할당하는 저장소 자원 풀을 사용하도록 하며, 물리 볼륨을 묶습니다. 볼륨 그룹이 충분한 저장소 자원을 가지지 않았다면, 추가 물리 볼륨에 대해 볼륨 그룹을 확장할 필요가 있습니다.�

다음 예제 에서는 ''vg0'' 그룹을  {{Path|/dev/sdc1}} 물리 볼륨으로 확장합니다:

{{RootCmd|vgextend vg0 /dev/sdc1}}

물리 볼륨을 각각 준비해야 함을 기억하십시오!

==== VG 줄이기 ====

If physical volumes need to be removed from the volume group, all data still in use on the physical volume needs to be moved to other physical volumes in the volume group. As seen before, this is handled through the <code>pvmove</code> command, after which the physical volume can be removed from the volume group using <code>vgreduce</code>:

{{RootCmd|pvmove -v /dev/sdc1
|vgreduce vg0 /dev/sdc1}}

==== VG 제거 ====

If a volume group is no longer necessary (or, in other words, the storage pool that it represents is no longer used and the physical volumes in it need to be freed for other purposes) then the volume group can be removed with <code>vgremove</code>. This only works if no logical volume is defined for the volume group, and all but one physical volume have already been removed from the pool.

{{RootCmd|vgremove vg0}}

=== LV(논리 볼륨) ===

Logical volumes are the final meta devices which are made available to the system, usually to create file systems on. They are created and managed in volume groups and show up as {{Path|/dev/VG_NAME/LV_NAME}}. Like with volume groups, the name used for a logical volume is decided by the administrator.

==== LV 만들기 ===

To create a logical volume, the <code>lvcreate</code> command is used. The parameters to the command consist out of the requested size for the logical volume (which cannot be larger than the amount of free space in the volume group), the volume group from which the space is to be claimed and the name of the logical volume to be created.

In the next example, a logical volume named ''lvol1'' is created from the volume group named ''vg0'' and with a size of 150MB:

{{RootCmd|lvcreate -L 150M -n lvol1 vg0}}

It is possible to tell <code>lvcreate</code> to use all free space inside a volume group. This is done through the ''-l'' parameter which selects the amount of ''extents'' rather than a (human readable) size. Logical volumes are split into ''logical extents'' which are data chunks inside a volume group. All extents in a volume group have the same size. With the ''-l'' parameter <code>lvcreate</code> can be asked to allocate all free extents:

{{RootCmd|lvcreate -l 100%FREE -n lvol1 vg0}}

Next to ''FREE'' the ''VG'' key can be used to denote the entire size of a volume group.

==== LV 목록 표시 ====

모든 논리 볼륨 내용을 표시하려면 <code>lvdisplay</code> 명령을 사용하십시오:

{{RootCmd|lvdisplay}}

논리 볼륨이 빠져있다면, <code>lvscan</code> 명령으로 존재하는 모든 볼륨 그룹의 논리 볼륨을 검색할 수 있습니다.

{{RootCmd|lvscan}}

==== LV 확장 ====

논리 볼륨을 확장하려면, <code>lvextend</code> 명령으로 논리 볼륨에 할당한 공간을 늘릴 수 있습니다.

가령, 논리 볼륨 ''lvol1''의 전체 용량을 500MB로 확장하려면:

{{RootCmd|lvextend -L500M /dev/vg0/lvol1}}

전체 용량보다 더 큰 용량을 추가할 수도 있습니다:

{{RootCmd|lvextend -L+350MB /dev/vg0/lvol1}}

확장 볼륨 그룹은 최종 사용자에게 추가 저장소로서 바로 제공할 수 없습니다. 그렇기에, 볼륨 그룹의 최상단에 있는 파일 시스템을 마찬가지로 늘려야 합니다. 마운트한 상태에서 모든 파일 시스템의 크기를 조정할 수 있는 것은 아니기에, 파일 시스템의 궁금증에 대해 더 많은 내용을 살펴보려면 문서를 확인하십시오.

가령, ext4 파일 시스템ㅁ의 크기를 500MB 크기로 조절하려면:

{{RootCmd|resize2fs /mnt/data 500M}}

==== LV 줄이기 ====

If a logical volume needs to be reduced in size, first shrink the file system itself. Not all file systems support online shrinking.

For instance, ext4 does not support online shrinking so the file system needs to be unmounted first. It is also recommended to do a file system check to make sure there are no inconsistencies:

{{RootCmd|umount /mnt/data
|e2fsck -f /dev/vg0/lvol1
|resize2fs /dev/vg0/lvol1 150M}}

With a reduced file system, it is now possible to reduce the logical volume as well:

{{RootCmd|lvreduce -L150M /dev/vg0/lvol1}}

==== LV 사용 권한 ====

LVM에서는 논리 볼륨에 대해 권한 상태 조정 기능을 지원합니다.

가령, <code>lvchange</code> 명령으로 논리 볼륨을 ''읽기 전용'' 상태로 설정할 수 있습니다:

{{RootCmd|lvchange -p r /dev/vg0/lvol1
|mount -o remount /dev/vg0/lvol1}}

바뀐 내용을 즉시 강제로 적용하지 않았기 때문에 다시 마운트해야 합니다.

논리 볼륨을 쓰기 가능상태로 다시 처리하려면 ''rw'' 퍼미션 비트를 사용하십시오:

{{RootCmd|lvchange -p rw /dev/vg0/lvol1 && mount -o remount /dev/vg0/lvol1}}

==== LV 제거 ====

논리 볼륨을 제거하기 전에 더이상 마운트한 볼륨이 없는지 확인하십시오:

{{RootCmd|umount /dev/vg0/lvol1}}

논리 볼륨을 비활성화하여 더이상 쓰기 동작을 할 수 없게 하십시오:

{{RootCmd|lvchange -a n /dev/vg0/lvol1}}

볼륨 마운트를 해제하여 비활성화 하면, 이제 제거하고, 볼륨 그룹에서 다른 논리 볼륨이 사용하도록 확장 할당을 해제할 수 있습니다.

{{RootCmd|lvremove /dev/vg0/lvol1}}

== 기능 ==

LVM provides quite a few interesting features for storage administrators, including (but not limited to)
* thin provisioning (over-committing storage)
* snapshot support
* volume types with different storage allocation methods

=== 씬 관리 ===

LVM2 최근 버전(2.02.89)에서는 ''씬'' 볼륨을 지원합니다. 씬 볼륨은 파일 시스템에 파일이 드문드문 분산해서 들어가는 블록 장치입니다. 따라서, 풀에 있는 씬 논리 볼륨은 ''오버 커밋'' 이 가능합니다. 즉, 할당한 크기보다 나타난 크기가 클 수는 있습니다. 풀 자체 크기보다 클 수도 있습니다. 드문드문 분산된 파일 처럼, 확장체는 블록 장치로 할당됩니다. 파일을 제거했을 때 다시 빈 공간이 늘어나 남은 영역을 파일 시스템에서 ''버리는'' 기능을 지원한다면 풀의 공간 가용성이 줄어듭니다.

LVM에서 각각의 씬 풀은 특별한 형식의 논리 볼륨이며, 논리 볼륨을 자체적으로 제공할 수 있습니다.

=== 씬 풀 만들기 ===

{{Warning/ko|씬 풀 메타데이터에서 오버플로우가 발생하면 풀이 깨집니다. '''LVM에서는 이러한 현상을 복구할 수 없습니다'''.}} 

{{Note/ko|씬 풀의 용량이 고갈되면 씬 풀을 확장하거나 프로세스에서 SIGKILL 시그널을 받기 전까지는, 어떤 프로세스든 (존재하지 않는)더 많은 공간을 할당하려는 씬 풀의 상태가 ''죽일 수 있는 대기'' 상태에 봉착합니다.}}

각각의 씬 풀에는 관련 메타데이터가 있으며, 씬 풀 용량에 추가됩니다. LVM에서는 크기가 어떻게 되든지간에 최소한 <tt>pool_chunks * 64 bytes</tt> 또는 2MiB 만큼 씬 풀 크기를 기반으로 메타데이터 크기를 계산합니다. 관리자는 마찬가지로 메타데이터 크기를 달리 선택할 수 있습니다.

씬 풀을 만들려면 <code>lvcreate</code> 명령에 ''--type thin-pool --thinpool thin_pool'' 매개변수를 추가하십시오:

{{RootCmd|lvcreate -L 150M --type thin-pool --thinpool thin_pool vg0}}

위의 예제에서는 ''thin_pool''을 총 150MB의 용량으로 만듭니다. 150MB는 씬 풀에 실제 할당한 크기입니다(그렇기 때문에 실제 저장소에서 활용할 수 있는 전체 공간이 됩니다).

각각의 메타데이터 크기를 분명하게 요청하려면 ''--metadatasize'' 매개변수를 사용하십시오:

{{RootCmd|lvcreate -L 150M --metadatasize 2M --type thin-pool --thinpool thin_pool vg0}}

씬 풀에 추가한 메타데이터 때문에, 볼륨 그룹에서 논리 볼륨에 대해 사용 가능한 전체 공간을 활용하는 직관적인 방법이 먹혀들지 않습니다(LVM 버그 [https://bugzilla.redhat.com/show_bug.cgi?id=812726|812726] 참고).

{{RootCmd|lvcreate -l 100%FREE --type thin-pool --thinpool thin_pool vg0|output=<pre>
Insufficient suitable allocatable extents for logical volume thin_pool: 549 more required
</pre>}}

씬 풀에는 다른 LV의 노드처럼 관련 장치 노드가 있는것이 아닙니다.

==== 씬 논리 볼륨 만들기 ====

''씬 논리 볼륨''은 씬 풀에 있는 논리 볼륨입니다(씬 풀 자체는 논리 볼륨입니다). 씬 논리 볼륨이 ''듬성듬성'' 하기에, ''-V'' 매개 변수로 물리 크기 대신 가상 크기를 설정합니다:

{{RootCmd|lvcreate -T vg0/thin_pool -V 300M -n lvol1}}

이 예제에서, 실제 할당한 저장소의 크기가 150MB라 하더라도, (씬) 논리 볼륨 ''lvol1''은 300MB 크기의 장치로 나타납니다.

씬 풀의 논리 볼륨처럼 각각의 씬 풀을 하나의 명령으로 만들 수 있습니다:

{{RootCmd|lvcreate -T vg0/thin_pool -V 300M -L150M -n lvol1}}

==== 씬 풀과 씬 논리 볼륨 목록 표시 ====

Thin pools and thin logical volumes are special types of logical volumes, and as such as displayed through the <code>lvdisplay</code> command. The <code>lvscan</code> command will also detect these logical volumes.

==== 씬 풀 확장 ====

{{Warning|As of LVM2 2.02.89, the metadata size of the thin pool cannot be expanded, it is fixed at creation}}

The thin pool is expanded like a non-thin logical volume using <code>lvextend</code>. For instance:

{{RootCmd|lvextend -L500M vg0/thin_pool}}

==== 씬 논리 볼륨 확장 ====

씬 논리 볼륨은 보통 논리 볼륨처럼 확장됩니다:

{{RootCmd|lvextend -L1G vg0/lvol1}}

Note that the <code>lvextend</code> command uses the ''-L'' option (or ''-l'' if extent counts are used) and not a "virtual size" option as was used during the creation. 

==== 씬 풀 줄이기 ====

Currently, LVM cannot reduce the size of the thin pool. See LVM bug [https://bugzilla.redhat.com/show_bug.cgi?id=812731|812731].

==== Reducing a thin logical volume ====

Thin logical volumes are reduced just like regular logical volumes.

For instance:
{{RootCmd|lvreduce -L300M vg0/lvol1l}}

<code>lvreduce</code> 명령은 ''-L'' 옵션(또는 확장 카운트를 사용할 경우 ''-l'')을 사용하며, 만들기 과정에 사용한 ''가상 크기'' 옵션은 아님에 유의하십시오.

==== 씬 풀 제거 ====

씬 풀은 내부에 있는 모든 씬 논리 볼륨을 제거하기 전까지는 제거할 수 없습니다.

씬 풀에서 더 이상 어떤 씬 논리 볼륨을 제공하지 않는다면, <code>lvremove</code>명령으로 제거할 수 있습니다:

{{RootCmd|lvremove vg0/thin_pool}}

=== LVM2 스냅샷과 씬 스냅샷 ===

스냅샷은 다른 논리 볼륨의 사본처럼 동작하는 논리 볼륨입니다. 스냅샷을 만든 횟수만큼 원래 논리 볼륨의 상태를 나타냅니다.

=== 스냅샷 논리 볼륨 만들기 ===

A snapshot logical volume is created using the ''-s'' option to <code>lvcreate</code>. Snapshot logical volumes are still given allocated storage as LVM "registers" all changes made to the original logical volume and stores these changes in the allocated storage for the snapshot. When querying the snapshot state, LVM will start from the original logical volume and then check all changes registered, "undoing" the changes before showing the result to the user.

A snapshot logical volume henceforth "growths" at the rate that changes are made on the original logical volume. When the allocated storage for the snapshot is completely used, then the snapshot will be removed automatically from the system.

{{RootCmd|lvcreate -l 10%VG -s -n 20140412_lvol1 /dev/vg0/lvol1}}

The above example creates a snapshot logical volume called ''20140412_lvol1'', based on the logical volume ''lvol1'' in volume group ''vg0''. It uses 10% of the space (extents actually) allocated to the volume group.

==== Accessing a snapshot logical volume ====

Snapshot logical volumes can be mounted like regular logical volumes. They are even not restricted to read-only operations - it is possible to modify snapshots and thus use it for things such as testing changes before doing these on a "production" file system.

As long as snapshot logical volumes exist, the regular/original logical volume cannot be reduced in size or removed.

==== LVM thin snapshots ====

{{Note|A thin snapshot can only be taken on a thin pool for a thin logical volume. The thin device mapper target supports thin snapshots of read-only non-thin logical volumes, but the LVM2 tooling does not support this. However, it is possible to create a regular (non-thin) snapshot logical volume of a thin logical volume.}}

To create a thin snapshot, the <code>lvcreate</code> command is used with the <code>-s</code> option. No size declaration needs to be passed on:

{{RootCmd|lvcreate -s -n 20140413_lvol1 /dev/vg0/lvol1}}

Thin logical volume snapshots have the same size as their original thin logical volume, and use a physical allocation of 0 just like all other thin logical volumes. 

{{Important|If ''-l'' or ''-L'' is specified, a snapshot will still be created, but the resulting snapshot will be a regular snapshot, not a thin snapshot.}}

It is also possible to take snapshots of snapshots:

{{RootCmd|lvcreate -s -n 1_20140413_lvol1 /dev/vg0/20140413_lvol1}}

Thin snapshots have several advantages over regular snapshots. First, thin snapshots are independent of their original logical volume once created. The original logical volume can be shrunk or deleted without affecting the snapshot. Second, thin snapshots can be efficiently created recursively (snapshots of snapshots) without the "chaining" overhead of regular recursive LVM snapshots.

==== Rolling back to snapshot state ====

To rollback the logical volume to the version of the snapshot, use the following command:

{{RootCmd|lvconvert --merge /dev/vg0/20140413_lvol1}}

This might take a couple of minutes, depending on the size of the volume.

{{Important|The snapshot will disappear and this change is not revertible}}

==== Rolling back thin snapshots ====

For thin volumes, <code>lvconvert --merge</code> does not work. Instead, delete the original logical volume and rename the snapshot:

{{RootCmd|umount /dev/vg0/lvol1
|lvremove /dev/vg0/lvol1
|lvrename vg0/20140413_lvol1 lvol1}}

=== Different storage allocation methods ===

LVM supports different allocation methods for storage:
* linear volumes (which is the default)
* mirrored volumes (in a more-or-less active/standby setup)
* striping (RAID0)
* mirrored volumes (RAID1 - which is more an active/active setup)
* striping with parity (RAID4 and RAID5)
* striping with double parity (RAID6)
* striping and mirroring (RAID10)

==== Linear volumes ====

Linear volumes are the most common kind of LVM volumes. LVM will attempt to allocate the logical volume to be as physically contiguous as possible. If there is a physical volume large enough to hold the entire logical volume, then LVM will allocate it there, otherwise it will split it up into as few pieces as possible.

The commands introduced earlier on to create volume groups and logical volumes create linear volumes.

Because linear volumes have no special requirements, they are the easiest to manipulate and can be resized and relocated at will. If a logical volume is allocated across multiple physical volumes, and any of the physical volumes become unavailable, then that logical volume cannot be started anymore and will be unusable.

==== Mirrored volumes ====

LVM supports ''mirrored'' volumes, which provide fault tolerance in the event of drive failure. Unlike RAID1, there is no performance benefit - all reads and writes are delivered to a single side of the mirror.

To keep track of the mirror state, LVM requires a ''log'' to be kept. It is recommended (and often even mandatory) to position this log on a physical volume that does not contain any of the mirrored logical volumes. There are three kind of logs that can be used for mirrors:

# '''Disk''' is the default log type. All changes made are logged into extra metadata extents, which LVM manages. If a device fails, then the changes are kept in the log until the mirror can be restored again.
# '''Mirror''' logs are '''disk''' logs that are themselves mirrored. 
# '''Core''' mirror logs record the state of the mirror in memory only. LVM will have to rebuild the mirror every time it is activated. This type is useful for temporary mirrors.

To create a logical volume with a single mirror, pass the ''-m 1'' argument (to select standard mirroring) with optionally ''--mirrorlog'' to select a particular log type:

{{RootCmd|lvcreate -m 1 --mirrorlog mirror -l 40%VG --nosync -n lvol1 vg0}}

The <tt>-m 1</tt> tells LVM to create one (additional) mirror, so requiring 2 physical volumes. The <tt>--nosync</tt> option is an optimization - without it LVM will try synchronize the mirror by copying empty sectors from one logical volume to another.

It is possible to create a mirror of an existing logical volume:

{{RootCmd|lvconvert -m 1 -b vg0/lvol1}}

The ''-b'' option does the conversion in the background as this can take quite a while.

To remove a mirror, set the number of mirrors (back) to 0:

{{RootCmd|lvconvert -m0 vg0/lvol1}}

If part of the mirror is unavailable (usually because the disk containing the physical volume has failed), the volume group will need to be brought up in degraded mode:

{{RootCmd|vgchange -ay --partial vg0}}

On the first write, LVM will notice the mirror is broken. The default policy ("remove") is to automatically reduce/break the mirror according to the number of pieces available. A 3-way mirror with a missing physical volume will be reduced to 2-way mirror; a 2-way mirror will be reduced to a regular linear volume. If the failure is only transient, and the missing physical volume returns after LVM has broken the mirror, the mirrored logical volume will need to be recreated on it. 

To recover the mirror, the failed physical volume needs to be removed from the volume group, and a replacement physical volume needs to be added (or if the volume group has a free physical volume, it can be created on that one). Then the mirror can be recreated with <tt>lvconvert</tt> at which point the old physical volume can be removed from the volume group:

{{RootCmd|vgextend vg0 /dev/sdc1
|lvconvert -b -m 1 --mirrorlog disk vg0/lvol1
|vgreduce --removemissing vg0}}

It is possible to have LVM recreate the mirror with free extents on a different physical volume if one side fails. To accomplish that, set <code>mirror_image_fault_policy</code> to ''allocate'' in {{Path|lvm.conf}}.

==== Thin mirrors ====

It is not (yet) possible to create a mirrored thin pool or thin volume. It is possible to create a mirrored thin pool my creating a normal mirrored logical volume and then converting the logical volume to a thin pool with <tt>lvconvert</tt>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will merge them into a single logical volume.

{{Warning|LVM 2.02.98 or above is required for this to work properly. Prior versions are either not capable or will segfault and corrupt the volume group. Also, conversion of a mirror into a thin pool '''destroys''' all existing data in the mirror!}}

{{RootCmd|lvcreate -m 1 --mirrorlog mirrored -l40%VG -n thin_pool vg0
|lvcreate -m 1 --mirrorlog mirrored -L4MB -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg0/thin_meta}}

==== Striping (RAID0) ====

Instead of a linear volume, where multiple contiguous physical volumes are appended, it possible to create a ''striped'' or ''RAID 0'' volume for better performance. This will alternate storage allocations across the available physical volumes.

To create a striped volume over three physical volumes:

{{RootCmd|lvcreate -i 3 -l 20%VG -n lvol1_stripe vg0|output=<pre>
Using default stripesize 64.00 KiB
</pre>}}

The -i option indicates over how many physical volumes the striping should be done.

It is possible to mirror a stripe set. The -i and -m options can be combined to create a striped mirror:

{{RootCmd|lvcreate -i 2 -m 1 -l 10%VG vg0}}

This creates a 2 physical volume stripe set and mirrors it on 2 different physical volumes, for a total of 4 physical volumes. An existing stripe set can be mirrored with <tt>lvconvert</tt>.

A thin pool can be striped like any other logical volume. All the thin volumes created from the pool inherit that settings - do not specify it manually when creating a thin volume.

It is not possible to stripe an existing volume, nor reshape the stripes across more/less physical volumes, nor to convert to a different RAID level/linear volume. A stripe set can be mirrored. It is possible to extend a stripe set across additional physical volumes, but they must be added in multiples of the original stripe set (which will effectively linearly append a new stripe set).

==== Mirroring (RAID1) ====

Unlike RAID 0, which is striping, RAID 1 is mirroring, but implemented differently than the original LVM mirror. Under RAID1, reads are spread out across physical volumes, improving performance. RAID1 mirror failures do not cause I/O to block because LVM does not need to break it on write.

Any place where an LVM mirror could be used, a RAID 1 mirror can be used in its place. It is possible to have LVM create RAID1 mirrors instead of regular mirrors implicitly by setting <tt>mirror_segtype_default</tt> to ''raid1'' in {{Path|lvm.conf}}.

To create a logical volume with a single mirror:

{{RootCmd|lvcreate -m 1 --type raid1 -l 40%VG --nosync -n lvm_raid1 vg0}}

Note the difference for creating a mirror: There is no ''mirrorlog'' specified, because RAID1 logical volumes do not have an explicit mirror log - it built-in to the logical volume.

It is possible to convert an existing logical volume to RAID 1:

{{RootCmd|lvconvert -m 1 --type raid1 -b vg0/lvol1}}

To remove a RAID 1 mirror, set the number of mirrors to 0:

{{RootCmd|lvconvert -m0 vg0/lvm_raid1}}

If part of the RAID1 is unavailable (usually because the disk containing the physical volume has failed), the volume group will need to be brought up in degraded mode:

{{RootCmd|vgchange -ay --partial vg0}}

Unlike an LVM mirror, writing does NOT breaking the mirroring. If the failure is only transient, and the missing physical volume returns, LVM will resync the mirror by copying cover the out-of-date segments instead of the entire logical volume. If the failure is permanent, then the failed physical volume needs to be removed from the volume group, and a replacement physical volume needs to be added (or if the volume group has a free physical volume, it can be created on a different PV). The mirror can then be repaired with ''lvconvert'', and the old physical volume can be removed from the volume group:

{{RootCmd|vgextend vg0 /dev/sdc1
|lvconvert --repair -b vg0/lvm_raid1
|vgreduce --removemissing vg0}}

==== Thin RAID1 ====

It is not (yet) possible to create a RAID 1 thin pool or thin volume. It is possible to create a RAID 1 thin pool by creating a normal mirrored logical volume and then converting the logical volume to a thin pool with <tt>lvconvert</tt>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will then merge them into a single logical volume.

{{Warning|LVM 2.02.98 or above is required for this to work properly. Prior versions are either not capable or will segfault and corrupt the VG. Also, conversion of a RAID 1 into a thin pool '''destroys''' all existing data in the mirror!}}

{{RootCmd|lvcreate -m 1 --type raid1 -l40%VG -n thin_pool vg0
|lvcreate -m 1 --type raid1 -L4MB -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg00/thin_meta}}

==== Striping with parity (RAID4 and RAID5) ====

{{Note|Striping with parity requires at least 3 physical volumes.}}

RAID 0 is not fault-tolerant - if any of the physical volumes fail then the logical volume is unusable. By adding a parity stripe to RAID 0 the logical volume can still function if a physical volume is missing. A new physical volume can then be added to restore fault tolerance.

Stripsets with parity come in 2 flavors: RAID 4 and RAID 5. Under RAID 4, all the parity stripes are stored on the same physical volume. This can become a bottleneck because all writes hit that physical volume, and it gets worse the more physical volumes are in the array. With RAID 5, the parity data is distributed evenly across the physical volumes so none of them become a bottleneck. For that reason, RAID 4 is rare and is considered obsolete/historical. In practice, all stripesets with parity are RAID 5.

{{RootCmd|lvcreate --type raid5 -l 20%VG -i 2 -n lvm_raid5 vg0}}

Only the data physical volumes are specified with -i, LVM adds one to it automatically for the parity. So for a 3 physical volume RAID5, ''-i 2'' is passed on and not ''-i 3''.

When a physical volume fails, then the volume group will need to be brought up in degraded mode:

{{RootCmd|vgchange -ay --partial vg0}}

The volume will work normally at this point, however this degrades the array to RAID 0 until a replacement physical volume is added. Performance is unlikely to be affected while the array is degraded - although it does need to recompute its missing data via parity, it only requires simple XOR for the parity block with the remaining data. The overhead is negligible compared to the disk I/O.

To repair the RAID5:

{{RootCmd|lvconvert --repair vg0/lvm_raid5
|vgreduce --removemissing vg0}}

It is possible to replace a still working physical volume in RAID5 as well:

{{RootCmd|lvconvert --replace /dev/sdb1 vg0/lvm_raid5
|vgreduce vg0 /dev/sdb1}}

The same restrictions of stripe sets apply to stripe sets with parity as well: it is not possible to enable striping with parity on an existing volume, nor reshape the stripes with parity across more/less physical volumes, nor to convert to a different RAID level/linear volume. A stripe set with parity can be mirrored. It is possible to extend a stripe set with parity across additional physical volumes, but they must be added in multiples of the original stripe set with parity (which will effectively linearly append a new stripe set with parity).

==== Thin RAID5 logical volumes ====

It is not (yet) possible to create stripe set with parity (RAID5) thin pools or thin logical volumes. It is possible to create a RAID5 thin pool by creating a normal RAID5 logical volume and then converting the logical volume into a thin pool with <tt>lvconvert</tt>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will merge them into a single logical volume.

{{Warning|LVM 2.02.98 or above is required for this to work properly. Prior versions are either not capable or will segfault and corrupt the VG. Also, coversion of a RAID5 LV into a thin pool '''destroys''' all existing data in the LV!}}

{{RootCmd|lvcreate --type raid5 -i 2  -l20%VG -n thin_pool vg0
|lvcreate --type raid5 -i 2 -L4MB -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg00/thin_meta}}

==== Striping with double parity (RAID6) ====

{{Note|RAID6 requires at least 5 physical volumes.}}

RAID 6 is similar to RAID 5, however RAID 6 can survive up to '''two''' physical volume failures, thus offering more fault tolerance than RAID5 at the expense of extra physical volumes. 

{{RootCmd|lvcreate --type raid6 -l 20%VG -i 3 -n lvm_raid6 vg00}}

Like raid5, the -i option is used to specify the number of physical volumes to stripe, excluding the 2 physical volumes for parity. So for a 5 physical volume RAID6, pass on ''-i 3'' and not ''-i 5''.

Recovery for RAID6 is the same as RAID5.

{{Note|Unlike RAID5 where parity block is cheap to recompute vs disk I/O, this is only half true in RAID6. RAID6 uses 2 parity stripes: One stripe is computed the same way as RAID5 (simple XOR). The second parity stripe is much harder to compute - see [https://www.kernel.org/pub/linux/kernel/people/hpa/raid6.pdf|raid6 (pdf)] for more information.}}

==== Thin RAID6 logical volumes ====

It is not (yet) possible to create a RAID6 thin pool or thin volumes. It is possible to create a RAID6 thin pool by creating a normal RAID6 logical volume and then converting the logical volume into a thin pool with <tt>lvconvert</tt>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will merge them into a single logical volume.

{{Warning|LVM 2.02.98 or above is required for this to work properly. Prior versions are either not capable or will segfault and corrupt the VG. Also, conversion of a RAID6 LV into a thin pool '''destroys''' all existing data in the LV!}}

{{RootCmd|lvcreate --type raid6 -i 2  -l20%VG -n thin_pool vg0
|lvcreate --type raid6 -i 2 -L4MB -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg0/thin_meta}}

==== LVM RAID10 ====

{{Note|RAID10 requires at least 4 physical volumes. Also LVM syntax requires the number of physical volumes be multiple of the numbers stripes and mirror, even though RAID10 format does not}}

RAID10 is a combination of RAID0 and RAID1. It is more powerful than RAID0+RAID1 as the mirroring is done at the stripe level instead of the logical volume level, and therefore the layout doesn't need to be symmetric. A RAID10 volume can tolerate at least a single missing physical volume, and possibly more.

{{Note|LVM currently limits RAID10 to a single mirror.}}

{{RootCmd|lvcreate --type raid10 -l 1020 -i 2 -m 1 --nosync -n lvm_raid10 vg0}}

Both the ''-i and -m'' options are specified: ''-i'' is the number of stripes and ''-m'' is the number of mirrors. Two stripes and 1 mirror requires 4 physical volumes.

==== Thin RAID10 ====

It is not (yet) possible to create a RAID10 thin pool or thin volumes. It is possible to create a RAID10 thin pool by creating a normal RAID10 logical volume and then converting the logical volume into a thin pool with <code>lvconvert</code>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will merge them into a single logical volume.
 
{{Warning|Conversion of a RAID10 logical volume into a thin pool '''destroys''' all existing data in the logical volume!}}

{{RootCmd|lvcreate -i 2 -m 1 --type raid10 -l 1012 -n thin_pool vg0
|lvcreate -i 2 -m 1 --type raid10 -l 6 -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg0/thin_meta}}

== Experimenting with LVM ==

It is possible to experiment with LVM without using real storage devices. To accomplish this, loopback devices are created.

First make sure to have the loopback module loaded. 

{{RootCmd|modprobe -r loop && modprobe loop max_part{{=}}63}}

{{Note|If loopback support is built into the kernel, then use <code>loop.max_part{{=}}63</code> as boot option.}}

Next configure LVM to not use [[udev]] to scan for devices:

{{File|/etc/lvm/lvm.conf|Disabling udev in LVM config|<pre>
obtain_device_list_from_udev = 0
</pre>}}

{{Important|This is for testing only, make sure to change the setting back when dealing with real devices since it is much faster to use udev!}}

Create some image files which will become the storage devices. The next example uses five files for a total of about ~10GB of real hard drive space:

{{RootCmd|mkdir /var/lib/lvm_img
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm0.img bs{{=}}1024 seek{{=}}2097152
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm1.img bs{{=}}1024 seek{{=}}2097152
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm2.img bs{{=}}1024 seek{{=}}2097152
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm3.img bs{{=}}1024 seek{{=}}2097152
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm4.img bs{{=}}1024 seek{{=}}2097152}}

Check which loopback devices are available:

{{RootCmd|losetup -a}}

Assuming all loopback devices are available, next create the devices:

{{RootCmd|losetup /dev/loop0 /var/lib/lvm_img/lvm0.img
|losetup /dev/loop1 /var/lib/lvm_img/lvm1.img
|losetup /dev/loop2 /var/lib/lvm_img/lvm2.img
|losetup /dev/loop3 /var/lib/lvm_img/lvm3.img
|losetup /dev/loop4 /var/lib/lvm_img/lvm4.img}}

The {{Path|/dev/loop[0-4]}} devices are now available to use as any other hard drive in the system (and thus be perfect for physical volumes).

{{Note|On the next reboot, all the loopback devices will be released and the folder {{Path|/var/lib/lvm_img}} can be deleted.}}

== Troubleshooting ==

LVM has a few features that already provide some level of redundancy. However, there are situations where it is possible to restore lost physical volumes or logical volumes.

=== vgcfgrestore utility ===

By default, on any change to a LVM physical volume, volume group, or logical volume, LVM2 create a backup file of the metadata in {{Path|/etc/lvm/archive}}. These files can be used to recover from an accidental change (like deleting the wrong logical volume). LVM also keeps a backup copy of the most recent metadata in {{Path|/etc/lvm/backup}}. These can be used to restore metadata to a replacement disk, or repair corrupted metadata.

To see what states of the volume group are available to be restored (partial output to improve readability):

{{RootCmd|vgcfgrestore --list vg00|output=<pre>
  File:		/etc/lvm/archive/vg0_00042-302371184.vg
  VG name:    	vg0
  Description:	Created *before* executing 'lvremove vg0/lvm_raid1'
  Backup Time:	Sat Jul 13 01:41:32 201
</pre>}}

==== Recovering an accidentally deleted logical volume ====

Assuming the logical volume ''lvm_raid1'' was accidentally removed from volume group ''vg0'', it is possible to recover it as follows:

{{RootCmd|vgcfgrestore -f /etc/lvm/archive/vg0_00042-302371184.vg vg0}}

{{Important|<tt>vgcfgrestore</tt> only restores LVM metadata, ''not'' the data inside the logical volume. However <code>pvremove</code>, <code>vgremove</code>, and <code>lvremove</code> only wipe metadata, leaving any data intact. If <code>issue_discards</code> is set in {{Path|/etc/lvm/lvm.conf}} though, then these command ''are'' destructive to data.}}

==== Replacing a failed physical volume ====

It possible to do a true "replace" and recreate the metadata on the new physical volume to be the same as the old physical volume:

{{RootCmd|vgdisplay --partial --verbose|output=<pre>
  --- Physical volumes ---
  PV Name               /dev/loop0     
  PV UUID               iLdp2U-GX3X-W2PY-aSlX-AVE9-7zVC-Cjr5VU
  PV Status             allocatable
  Total PE / Free PE    511 / 102
  
  PV Name               unknown device     
  PV UUID               T7bUjc-PYoO-bMqI-53vh-uxOV-xHYv-0VejBY
  PV Status             allocatable
  Total PE / Free PE    511 / 102
</pre>}}

The important line here is the UUID "unknown device". 

{{RootCmd|pvcreate --uuid T7bUjc-PYoO-bMqI-53vh-uxOV-xHYv-0VejBY --restorefile /etc/lvm/backup/vg0 /dev/loop1|output=<pre>
  Couldn't find device with uuid T7bUjc-PYoO-bMqI-53vh-uxOV-xHYv-0VejBY.
  Physical volume "/dev/loop1" successfully created</pre>}}

This recreates the physical volume metadata, but not the missing logical volume or volume group data on the physical volume.

{{RootCmd|vgcfgrestore -f /etc/lvm/backup/vg0 vg0|output=<pre>
  Restored volume group vg0
</pre>}}

This now reconstructs all the missing metadata on the physical volume, including the logical volume and volume group data. However it doesn't restore the data, so the mirror is out of sync.

{{RootCmd|vgchange -ay vg0|output=<pre>
  device-mapper: reload ioctl on  failed: Invalid argument
  1 logical volume(s) in volume group "vg0" now active
</pre>}}

{{RootCmd|lvchange --resync vg0/lvm_raid1|output=<pre>
Do you really want to deactivate logical volume lvm_raid1 to resync it? [y/n]: y
</pre>}}

This will resync the mirror. This works with RAID 4,5 and 6 as well. 

=== Deactivating a logical volume ===

It is possible to deactivate a logical volume with the following command:

{{RootCmd|umount /dev/vg0/lvol1
|lvchange -a n /dev/vg0/lvol1}}

It is not possible to mount the logical volume anywhere before it gets reactivated:

{{RootCmd|lvchange -a y /dev/vg0/lvol1}}

== External resources ==

* [http://sourceware.org/lvm2/ LVM2 sourceware.org]
* [http://tldp.org/HOWTO/LVM-HOWTO/ LVM tldp.org]
* [http://sources.redhat.com/lvm2/wiki/ LVM2 Wiki redhat.com]


[[Category:Core system]]
