<languages />

{{Metadata|abstract=LVM은 관리자가 파일 시스템과 사용중인 물리 저장소간의 추상 레이어를 제공하여 메타 장치를 만들 수 있도록 합니다.}}

{{InfoBox stack
|{{InfoBox wikipedia|Logical Volume Manager (Linux)|header=true}}
}}

'''LVM'''('''L'''ogical '''V'''olume '''M'''anager)은 관리자가 파일 시스템과 사용 중인 물리 저장소 사이의 추상 레이어를 제공하여 메타 장치를 만들 수 있게 합니다. (파일 시스템이 있는)메타 장치는 ''논리 볼륨''이며, 저장소 풀에서 사용하는 저장소를 ''볼륨 그룹'' 이라고 합니다. 볼륨 그룹은 데이터를 저장하는 하나 이상의 ''실제 장치''로 구성되어있습니다.

물리 볼륨은 파티션으로 구성되어 있고, 전체 SATA 하드 디스크 드라이브는 JBOD('''J'''ust a '''B'''unch '''O'''f '''D'''isks), RAID 체계, iSCSI, 광 채널, eSATA 등으로 묶습니다.

== 설치 ==

LVM은 설정을 관리하기 위해 커널 수준의 드라이버와 사용자 영역 프로그램으로 다룹니다.

=== 커널 ===

다음 커널 옵션을 활성화하십시오:

{{KernelBox|<pre>
Device Drivers  --->
   Multiple devices driver support (RAID and LVM)  --->
       <*> Device mapper support
           <*> Crypt target support
           <*> Snapshot target
           <*> Mirror target
       <*> Multipath target
           <*> I/O Path Selector based on the number of in-flight I/Os
           <*> I/O Path Selector based on the service time
</pre>}}

{{Note|Not everything needs to be enabled; some of the options are only needed for [[LVM#LVM2_snapshots_and_thin_snapshots|LVM2 Snapshots and LVM2 Thin Snapshots]], [[LVM#Mirrored_volumes|LVM2 Mirrors]], [[LVM#Striping_.28RAID0.29|LVM2 RAID 0/Stripeset]] and encryption.}}

=== 프로그램 ===

{{Package|sys-fs/lvm2}}를 설치하십시오:

{{USEflag|package=sys-fs/lvm2
|clvm
|cman
|lvm1+yes
|readline+yes
|selinux++no
|static
|static-libs
|thin+yes
|udev+yes
}}

{{Emerge|lvm2}}

== 설정 ==

Configuring LVM is done on several levels:
# LV, PV and VG management through the management utilities;
# LVM subsystem fine-tuning through the configuration file;
# Service management at the distribution level;
# Setup through an initial ram file system (initramfs).

논리, 물리 볼륨 및 볼륨 그룹의 관리는 [[#Usage|사용법]] 장에서 다룹니다.

=== LVM 설정 파일 ===

LVM은 {{Path|/etc/lvm/lvm.conf}}에 더 많이 설정할 수 있는 파일이 있습니다. 대부분의 사용자는 LVM 사용을 시작하는데 이 파일의 설정을 수정할 필요가 없습니다.�

=== 서비스 관리 ===

젠투에서는 볼륨 그룹과 논리 볼륨을 자동으로 감지하고 활성화 하는 LVM 서비스를 제공합니다.

서비스는 init 시스템에서 관리할 수 있습니다.

==== openrc ====

LVM을 별도로 시작하려면:

{{RootCmd|/etc/init.d/lvm start}}

부팅시 LVM을 시작하려면:

{{RootCmd|rc-update add lvm boot}}

==== systemd ====

lvm을 별도로 시작하려면:

{{RootCmd|systemctl start lvm2-monitor.service}}

부팅할 때 LVM을 시작하려면:

{{RootCmd|systemctl enable lvm2-monitor.service}}

=== initramfs에서 LVM 사용하기 ===

대부분의 부트 로더는 LVM에서 직접 부팅할 수 있습니다. 기존의 GRUB은 물론이고 LILO에서도 안됩니다. Grub2는 LVM 선형 논리 볼륨, 미러링한 논리 볼륨, 몇가지 RAID 논리 볼륨으로 부팅할 수 있습니다. 씬 논리 볼륨을 지원하는 부트로더는 없습니다. 

이런 이유로, /boot 파티션은 비 LVM 파티션으로 사용하고 initramfs에서 LVM 루트를 마운트하는 방식을 권장합니다. initramfs 같은 요소는 [[Genkernel|genkernel]], {{Package|sys-kernel/genkernel-next}}, [[dracut]]에서 자동으로 만들 수 있습니다:

* <tt>genkernel</tt> can boot from all types except thin volumes (as it neither builds nor copies the {{Package|thin-provisioning-tools}} binaries from the build host) and maybe RAID10 (RAID10 support requires LVM2 2.02.98, but genkernel builds 2.02.89, however if static binaries are available it can copy those);
* <tt>genkernel-next</tt> can boot from all types volumes, but needs a new enough {{Package|app-misc/pax-utils}} or the resulting thin binaries will be broken (See {{Bug|482504}});
* <tt>dracut</tt> should boot all types, but only includes thin support in the initramfs if the host being run on has a thin root.

==== Genkernel/Genkernel-next ====

{{Package|sys-kernel/genkernel}} 또는 {{Package|sys-kernel/genkernel-next}} 둘 중 하나를 이머지하십시오. {{Package|sys-fs/lvm2}} 패키지를 이머지할 때는 static USE 플래그를 활성화하여 genkernel이 시스템 바이너리를 사용할 수 있게 하십시오(그렇지 않으면 자체 사본을 만듭니다). 다음 예제에서는 initramfs(전체 커널 아님)만 빌드하며 LVM 지원을 활성화합니다.

{{RootCmd|genkernel --lvm initramfs}}

genkernel 맨페이지에서는 시스템 요구사항에 따라 다른 옵션을 설명합니다.

initrd는 LVM 시작 방법을 알리는 매개 변수가 필요하며, 다른 커널 매개 변수를 넣는 동일한 방식으로 처리합니다. 예를 들면:

{{FileBox|filename=/etc/default/grub|title=dolvm을 커널 부팅 매개변수로 넣기|lang=bash|1=
GRUB_CMDLINE_LINUX="dolvm"
}}

==== Dracut ====

{{Package|sys-kernel/dracut}} 꾸러미는 레드햇 프로젝트에서 이식했으며, initramfs를 만드는 유사한 도구를 제공합니다. 시험 목적으로 ~arch 상태에 있기 때문에 이머지를 하려면 사용자 여러분은( {{Path|/etc/portage/package.accept_keywords}} 에서)[[Knowledge_Base:Accepting_a_keyword_for_a_single_package|이 글대로 하셔야합니다]]. 이 과정을 수행하기 전에 {{Path|/etc/portage/make.conf}}에 <code>DRACUT_MODULES="lvm"</code>를 추가해야 합니다. 다른 모듈을 사용한다면, [[Dracut]]을 참고하십시오. 보통 다음 명령을 통해, 활용 가능한 기본 initramfs를 만듭니다.

{{RootCmd|dracut -a lvm}}

initrd는 lvm 시작 방법을 알리는 매개 변수가 필요하며, 다른 커널 매개 변수를 넣는 동일한 방식으로 처리합니다. 예를 들면:

{{FileBox|filename=/etc/default/grub|title=LVM 지원을 커널 부팅 매개변수로 넣기|lang=bash|1=
GRUB_CMDLINE_LINUX="rd.lvm.vg=vol00"
}}

For a comprehensive list of LVM options within <tt>dracut</tt> please see the section in the [https://www.kernel.org/pub/linux/utils/boot/dracut/dracut.html#_lvm Dracut Manual].

== 사용법 ==

LVM은 다음 세가지 레벨의 저장소로 구성되어 있습니다:
* 하드 디스크 드라이브, 파티션, RAID 시스템 또는 저장소의 다른 개념을 물리 볼륨(PV)으로 초기화합니다
* 물리 볼륨(PV)은 볼륨 그룹(VG)으로 묶습니다
* 논리 볼륨(LV)은 볼륨 그룹(VG)에서 관리합니다

=== PV(물리 볼륨) ===
물리 볼륨은 LVM을 구축할 실제 하드웨어 또는 저장장치 입니다.

==== 공간 분할 ====

{{Note/ko|단일 LVM 볼륨 그룹에 대해 전체 디스크를 활용하고 싶지 않을 경우에만 볼륨 그룹으로 저장소를 관리하려는 목적의 각 분할 공간 활용이 필요합니다. 전체 디스크를 활용할 가능성이 있다면 이 단계를 건너뛰고 전체 하드디스크 드라이브를 물리 볼륨으로 초기화 하십시오.}}

''LVM''의 분할 공간 형식은 ''8e''(Linux LVM)입니다.

For instance, to set the type through <tt>fdisk</tt> for a partition on {{Path|/dev/sda}}:

{{RootCmd|fdisk /dev/sda}}

In <tt>fdisk</tt>, create partitions using the {{Key|n}} key and then change the partition type with the {{Key|t}} key to ''8e''.

=== PV 만들기 ===

Physical volumes can be created / initialized with the <tt>pvcreate</tt> command.

가령, 다음 명령을 사용하면 {{Path|/dev/sda}}와 {{Path|/dev/sdb}}의 첫번째 처음 파티션의 물리 볼륨을 만듭니다:

{{RootCmd|pvcreate /dev/sd[ab]1}}

==== PV 목록 표시 ====

With the <tt>pvdisplay</tt> command, an overview of all active physical volumes on the system can be obtained.

{{RootCmd|pvdisplay|output=<pre>
 --- Physical volume ---
  PV Name               /dev/sda1
  VG Name               volgrp
  PV Size               160.01 GiB / not usable 2.31 MiB
  Allocatable           yes 
  PE Size               4.00 MiB
  Total PE              40962
  Free PE               4098
  Allocated PE          36864
  PV UUID               3WHAz3-dh4r-RJ0E-5o6T-9Dbs-4xLe-inVwcV
  
 --- Physical volume ---
  PV Name               /dev/sdb1
  VG Name               volgrp
  PV Size               160.01 GiB / not usable 2.31 MiB
  Allocatable           yes 
  PE Size               4.00 MiB
  Total PE              40962
  Free PE               40962
  Allocated PE          0
  PV UUID               b031x0-6rej-BcBu-bE2C-eCXG-jObu-0Boo0x
</pre>}}

If more physical volumes should be displayed, then <tt>pvscan</tt> can detect inactive physical volumes and activate those.

{{RootCmd|pvscan|output=<pre>
  PV /dev/sda1  VG volgrp        lvm2 [160.01 GiB / 16.01 GiB free]
  PV /dev/sdb1  VG volgrp        lvm2 [160.01 GiB / 160.01 GiB free]
  Total: 2 [320.02 GB] / in use: 2 [320.02 GiB] / in no VG: 0 [0]
</pre>}}

==== PV 제거 ====

LVM은 (다른 방식을 언급하지 않는 한)존재하는 모든 물리 볼륨에 데이터를 넣는데, 연속적인 접근 방식으로 넣습니다. (볼륨 그룹 내)논리 볼륨 논리 볼륨이 단일 물리 볼륨의 남은 공간보다 적다면, 논리 볼륨의 모든 공간을 (단일) 물리 볼륨에 연속적인 형태로 위치하도록 합니다. 이러한 조치는 성능상 이유로 처리됩니다.

If a physical volume needs to be removed from a volume group, the data first needs to be moved away from the physical volume. With the <tt>pvmove</tt> command, all data on a physical volume is moved to other physical volumes within the same volume group.

{{RootCmd|pvmove -v /dev/sda1}}

Such an operation can take a while depending on the amount of data that needs to be moved. Once finished, there should be no data left on the device. Verify with pvdisplay that the physical volume is no longer used by any logical volume.

The next step is to remove the physical volume from the volume group using <tt>vgreduce</tt> after which the device can be "deselected" as a physical volume using pvremove:

{{RootCmd|vgreduce vg0 /dev/sda1 && pvremove /dev/sda1}}

=== VG(볼륨 그룹) ===

볼륨 그룹(VG)은 수많은 물리 볼륨을 묶고 장치 파일 시스템에 {{Path|/dev/VG_NAME}}와 같은 식으로 보여줍니다. 볼륨 그룹의 이름은 관리자가 선택합니다.

==== VG 만들기 ====

다음 명령은 {{Path|/dev/sda1}}과 {{Path|/dev/sdb1}} 물리 볼륨을 묶어 ''vg0''이라 하는 볼륨 그룹을 만듭니다.

{{RootCmd|vgcreate vg0 /dev/sd[ab]1}}

==== VG 목록 표시 ====

To list all active volume groups, use the <tt>vgdisplay</tt> command:

{{RootCmd|vgdisplay|output=<pre>
  --- Volume group ---
  VG Name               vg0
  System ID             
  Format                lvm2
  Metadata Areas        1
  Metadata Sequence No  8
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                6
  Open LV               6
  Max PV                0
  Cur PV                1
  Act PV                1
  VG Size               320.02 GiB
  PE Size               4.00 MiB
  Total PE              81924
  Alloc PE / Size       36864 / 144.00 GiB
  Free  PE / Size       45056 /176.01 GiB
  VG UUID               mFPXj3-DdPi-7YJ5-9WKy-KA5Y-Vd4S-Lycxq3
</pre>}}

If volume groups are missing, use the <tt>vgscan</tt> command to locate volume groups:

{{RootCmd|vgscan|output=<pre>
  Reading all physical volumes.  This may take a while...
  Found volume group "vg0" using metadata type lvm2
</pre>}}

==== VG 확장 ====

볼륨 그룹은 관리자가 파일 시스템에 할당하는 저장소 자원 풀을 사용하도록 하며, 물리 볼륨을 묶습니다. 볼륨 그룹이 충분한 저장소 자원을 가지지 않았다면, 추가 물리 볼륨에 대해 볼륨 그룹을 확장할 필요가 있습니다.

다음 예제 에서는 ''vg0'' 그룹을  {{Path|/dev/sdc1}} 물리 볼륨으로 확장합니다:

{{RootCmd|vgextend vg0 /dev/sdc1}}

물리 볼륨을 각각 준비해야 함을 기억하십시오!

==== VG 줄이기 ====

If physical volumes need to be removed from the volume group, all data still in use on the physical volume needs to be moved to other physical volumes in the volume group. As seen before, this is handled through the <tt>pvmove</tt> command, after which the physical volume can be removed from the volume group using vgreduce:

{{RootCmd|pvmove -v /dev/sdc1
|vgreduce vg0 /dev/sdc1}}

==== VG 제거 ====

If a volume group is no longer necessary (or, in other words, the storage pool that it represents is no longer used and the physical volumes in it need to be freed for other purposes) then the volume group can be removed with vgremove. This only works if no logical volume is defined for the volume group, and all but one physical volume have already been removed from the pool.

{{RootCmd|vgremove vg0}}

=== LV(논리 볼륨) ===

논리 볼륨은 시스템에 존재하는 최종 메타 장치이며, 보통 파일 시스템에 만듭니다. 볼륨 그룹에 만들고 관리하며 {{Path|/dev/VG_NAME/LV_NAME}} 형식으로 나타납니다. 볼륨 그룹처럼, 논리 볼륨에 사용하는 이름은 관리자가 결정합니다.

==== LV 만들기 ===

To create a logical volume, the <tt>lvcreate</tt> command is used. The parameters to the command consist out of the requested size for the logical volume (which cannot be larger than the amount of free space in the volume group), the volume group from which the space is to be claimed and the name of the logical volume to be created.

다음 예제에서는 ''vg0'' 볼륨 그룹에 150MB 용량의 ''lvol1'' 논리 볼륨을 만듭니다:

{{RootCmd|lvcreate -L 150M -n lvol1 vg0}}

It is possible to tell <tt>lvcreate</tt> to use all free space inside a volume group. This is done through the <code>-l</code> option which selects the amount of ''extents'' rather than a (human readable) size. Logical volumes are split into ''logical extents'' which are data chunks inside a volume group. All extents in a volume group have the same size. With the <code>-l</code> option <tt>lvcreate</tt> can be asked to allocate all free extents:

{{RootCmd|lvcreate -l 100%FREE -n lvol1 vg0}}

''FREE'' 다음에 ''VG'' 키를 사용하여 볼륨 그룹의 전체 크기를 표시할 수 있습니다.

==== LV 목록 표시 ====

To list all logical volumes, use the <tt>lvdisplay</tt> command:

{{RootCmd|lvdisplay}}

If logical volumes are missing, then the <tt>lvscan</tt> command can be used to scan for logical volumes on all available volume groups.

{{RootCmd|lvscan}}

==== LV 확장 ====

When a logical volume needs to be expanded, then the <tt>lvextend</tt> command can be used to grow the allocated space for the logical volume.

가령, 논리 볼륨 ''lvol1''의 전체 용량을 500MB로 확장하려면:

{{RootCmd|lvextend -L500M /dev/vg0/lvol1}}

전체 용량보다 더 큰 용량을 추가할 수도 있습니다:

{{RootCmd|lvextend -L+350MB /dev/vg0/lvol1}}

확장 볼륨 그룹은 최종 사용자에게 추가 저장소로서 바로 제공할 수 없습니다. 그렇기에, 볼륨 그룹의 최상단에 있는 파일 시스템을 마찬가지로 늘려야 합니다. 마운트한 상태에서 모든 파일 시스템의 크기를 조정할 수 있는 것은 아니기에, 파일 시스템의 궁금증에 대해 더 많은 내용을 살펴보려면 문서를 확인하십시오.

가령, ext4 파일 시스템ㅁ의 크기를 500MB 크기로 조절하려면:

{{RootCmd|resize2fs /mnt/data 500M}}

==== LV 줄이기 ====

논리 볼륨의 크기를 줄여야 한다면, 먼저 파일 시스템 자체의 크기를 줄이십시오. 모든 파일 시스템이 마운트한 상태에서 용량 줄이기 기능을 지원하는 것은 아닙니다.

가령, ext4는 마운트한 상태에서 용량 줄이는 기능을 지우너하지 않으므로, 먼저 파일 시스템의 마운트를 해제해야합니다. 그리고 파일 시스템을 검사하여 파일 시스템의 무결성 확인을 추천합니다:

{{RootCmd|umount /mnt/data
|e2fsck -f /dev/vg0/lvol1
|resize2fs /dev/vg0/lvol1 150M}}

용량이 줄어든 파일 시스템을 통해, 마찬가지로, 이제 논리 볼륨의 용량을 줄일 수 있습니다:

{{RootCmd|lvreduce -L150M /dev/vg0/lvol1}}

==== LV 사용 권한 ====

LVM에서는 논리 볼륨에 대해 권한 상태 조정 기능을 지원합니다.

For instance, a logical volume can be set to ''read only'' using the <tt>lvchange</tt> command:

{{RootCmd|lvchange -p r /dev/vg0/lvol1
|mount -o remount /dev/vg0/lvol1}}

바뀐 내용을 즉시 강제로 적용하지 않았기 때문에 다시 마운트해야 합니다.

논리 볼륨을 쓰기 가능상태로 다시 처리하려면 ''rw'' 퍼미션 비트를 사용하십시오:

{{RootCmd|lvchange -p rw /dev/vg0/lvol1 && mount -o remount /dev/vg0/lvol1}}

==== LV 제거 ====

논리 볼륨을 제거하기 전에 더이상 마운트한 볼륨이 없는지 확인하십시오:

{{RootCmd|umount /dev/vg0/lvol1}}

논리 볼륨을 비활성화하여 더이상 쓰기 동작을 할 수 없게 하십시오:

{{RootCmd|lvchange -a n /dev/vg0/lvol1}}

볼륨 마운트를 해제하여 비활성화 하면, 이제 제거하고, 볼륨 그룹에서 다른 논리 볼륨이 사용하도록 확장 할당을 해제할 수 있습니다.

{{RootCmd|lvremove /dev/vg0/lvol1}}

== 기능 ==

LVM에서는 저장소 관리자에게 다음과 같은 몇가지(라지만 약간 제한이 있는) 괜찮은 기능을 제공합니다
* 씬 관리(오버 커밋 저장소)
* 스냅샷 지원
* 다른 저장소 할당 방식에 따른 볼륨 형식

=== 씬 관리 ===

LVM2 최근 버전(2.02.89)에서는 ''씬'' 볼륨을 지원합니다. 씬 볼륨은 파일 시스템에 파일이 드문드문 분산해서 들어가는 블록 장치입니다. 따라서, 풀에 있는 씬 논리 볼륨은 ''오버 커밋'' 이 가능합니다. 즉, 할당한 크기보다 나타난 크기가 클 수는 있습니다. 풀 자체 크기보다 클 수도 있습니다. 드문드문 분산된 파일 처럼, 확장체는 블록 장치로 할당됩니다. 파일을 제거했을 때 다시 빈 공간이 늘어나 남은 영역을 파일 시스템에서 ''버리는'' 기능을 지원한다면 풀의 공간 가용성이 줄어듭니다.

LVM에서 각각의 씬 풀은 특별한 형식의 논리 볼륨이며, 논리 볼륨을 자체적으로 제공할 수 있습니다.

=== 씬 풀 만들기 ===

{{Warning/ko|씬 풀 메타데이터에서 오버플로우가 발생하면 풀이 깨집니다. '''LVM에서는 이러한 현상을 복구할 수 없습니다'''.}} 

{{Note/ko|씬 풀의 용량이 고갈되면 씬 풀을 확장하거나 프로세스에서 SIGKILL 시그널을 받기 전까지는, 어떤 프로세스든 (존재하지 않는)더 많은 공간을 할당하려는 씬 풀의 상태가 ''죽일 수 있는 대기'' 상태에 봉착합니다.}}

각각의 씬 풀에는 관련 메타데이터가 있으며, 씬 풀 용량에 추가됩니다. LVM에서는 크기가 어떻게 되든지간에 최소한 <tt>pool_chunks * 64 bytes</tt> 또는 2MiB 만큼 씬 풀 크기를 기반으로 메타데이터 크기를 계산합니다. 관리자는 마찬가지로 메타데이터 크기를 달리 선택할 수 있습니다.

To create a thin pool, add the <code>--type thin-pool --thinpool thin_pool</code> options to <tt>lvcreate</tt>:

{{RootCmd|lvcreate -L 150M --type thin-pool --thinpool thin_pool vg0}}

위의 예제에서는 ''thin_pool''을 총 150MB의 용량으로 만듭니다. 150MB는 씬 풀에 실제 할당한 크기입니다(그렇기 때문에 실제 저장소에서 활용할 수 있는 전체 공간이 됩니다).

To explicitly ask for a certain metadata size, use the <code>--metadatasize</code> option:

{{RootCmd|lvcreate -L 150M --metadatasize 2M --type thin-pool --thinpool thin_pool vg0}}

씬 풀에 추가한 메타데이터 때문에, 볼륨 그룹에서 논리 볼륨에 대해 사용 가능한 전체 공간을 활용하는 직관적인 방법이 먹혀들지 않습니다(LVM 버그 [https://bugzilla.redhat.com/show_bug.cgi?id=812726|812726] 참고).

{{RootCmd|lvcreate -l 100%FREE --type thin-pool --thinpool thin_pool vg0|output=<pre>
Insufficient suitable allocatable extents for logical volume thin_pool: 549 more required
</pre>}}

씬 풀에는 다른 LV의 노드처럼 관련 장치 노드가 있는것이 아닙니다.

==== 씬 논리 볼륨 만들기 ====

A ''thin logical volume'' is a logical volume inside the thin pool (which itself is a logical volume). As thin logical volumes are ''sparse'', a virtual size instead of a physical size is specified using the <code>-V</code> option:

{{RootCmd|lvcreate -T vg0/thin_pool -V 300M -n lvol1}}

이 예제에서, 실제 할당한 저장소의 크기가 150MB라 하더라도, (씬) 논리 볼륨 ''lvol1''은 300MB 크기의 장치로 나타납니다.

씬 풀의 논리 볼륨처럼 각각의 씬 풀을 하나의 명령으로 만들 수 있습니다:

{{RootCmd|lvcreate -T vg0/thin_pool -V 300M -L150M -n lvol1}}

==== 씬 풀과 씬 논리 볼륨 목록 표시 ====

Thin pools and thin logical volumes are special types of logical volumes, and as such as displayed through the <tt>lvdisplay</tt> command. The <tt>lvscan</tt> command will also detect these logical volumes.

==== 씬 풀 확장 ====

{{Warning/ko|LVM2 2.02.89에서는 씬 풀의 메타데이터 크기가 만들 때부터 정해져 있어 늘릴 수 없습니다}}

The thin pool is expanded like a non-thin logical volume using <tt>lvextend</tt>. For instance:

{{RootCmd|lvextend -L500M vg0/thin_pool}}

==== 씬 논리 볼륨 확장 ====

씬 논리 볼륨은 보통 논리 볼륨처럼 확장됩니다:

{{RootCmd|lvextend -L1G vg0/lvol1}}

Note that the <tt>lvextend</tt> command uses the <code>-L</code> option (or <code>-l</code> if extent counts are used) and not a "virtual size" option as was used during the creation. 

==== 씬 풀 줄이기 ====

현재 LVM에서는 씬 풀의 크기를 줄일 수 없습니다. LVM 버그 [https://bugzilla.redhat.com/show_bug.cgi?id=812731|812731]을 참고하십시오.

==== 씬 논리 볼륨 줄이기 ====

씬 논리 볼륨은 보통 논리 볼륨처럼 줄입니다.

가령:
{{RootCmd|lvreduce -L300M vg0/lvol1l}}

Note that the <tt>lvreduce</tt> command uses the <code>-L</code> option (or <code>-l</code> if extent counts are used) and not a "virtual size" option as was used during the creation.

==== 씬 풀 제거 ====

씬 풀은 내부에 있는 모든 씬 논리 볼륨을 제거하기 전까지는 제거할 수 없습니다.

When a thin pool no longer services any thin logical volume, it can be removed through the <tt>lvremove</tt> command:

{{RootCmd|lvremove vg0/thin_pool}}

=== LVM2 스냅샷과 씬 스냅샷 ===

스냅샷은 다른 논리 볼륨의 사본처럼 동작하는 논리 볼륨입니다. 스냅샷을 만든 횟수만큼 원래 논리 볼륨의 상태를 나타냅니다.

{{Warning|Since the logical snapshot volume also gets the same filesystem ''LABEL'' and ''UUID'', be sure the {{Path|/etc/fstab}} file or initramfs '''does not''' contain entries for these filesystems using the <code>LABEL{{=}}</code> or <code>UUID{{=}}</code> syntax. Otherwise you might end up with the snapshot being mounted instead of the (intended) original logical volume.}}

=== 스냅샷 논리 볼륨 만들기 ===

A snapshot logical volume is created using the <code>-s</code> option to <tt>lvcreate</tt>. Snapshot logical volumes are still given allocated storage as LVM "registers" all changes made to the original logical volume and stores these changes in the allocated storage for the snapshot. When querying the snapshot state, LVM will start from the original logical volume and then check all changes registered, "undoing" the changes before showing the result to the user.

이후로 스냅샷 논리 볼륨은 초기 논리 볼륨에서 바뀐 내용의 비율이 ''늘어납니다''. 스냅샷에 할당한 저장소를 다 사용하면 시스템에서 스냅샷을 자동으로 제거합니다.

{{RootCmd|lvcreate -l 10%VG -s -n 20140412_lvol1 /dev/vg0/lvol1}}

위 예제에서 ''vg0'' 볼륨 그룹의 ''lvol1'' 논리 볼륨을 기반으로 ''20140412_lvol1''라고 하는 스냅샷 논리 볼륨을 만듭니다. 볼륨 그룹에 할당한 공간의 10%(실제로 더 많이)를 사용합니다.

==== 스냅샷 논리 볼륨 접근 ====

스냅샷 논리 볼륨은 보통 논리 볼륨처럼 마운트 할 수 있습니다. 심지어 읽기 전용 처리에 구애받지 않습니다. 스냅샷을 수정할 수도 있기 때문에 ''프로덕션'' 파일 시스템에서 뭔가를 수행하기 전에 바뀐 내용을 시험하는 등의 용도로 활용합니다.

스냅샷 논리 볼륨이 존재하는 동안, 보통의 원래 논리 볼륨의 크기를 줄이거나 제거할 수 없습니다.

==== LVM 씬 스냅샷 ====

{{Note/ko|씬 스냅샷은 씬 논리 볼륨의 씬 풀에서만 취할 수 있습니다. 씬 장치 매퍼 타겟은 읽기 전용 비 씬 논리 볼륨의 씬 스냅샷을 지원하지만 LVM2 도구에선 지원하지 않습니다. 다만, 씬 논리 볼륨의 일반(비 씬) 스냅샷 논리 볼륨을 만들 수는 있습니다.}}

To create a thin snapshot, the <tt>lvcreate</tt> command is used with the <code>-s</code> option. No size declaration needs to be passed on:

{{RootCmd|lvcreate -s -n 20140413_lvol1 /dev/vg0/lvol1}}

씬 논리 볼륨 스냅샷은 초기 씬 논리 볼륨과 동일한 크기를 지니며, 물리 할당 0번은 다른 모든 씬 논리 볼륨과 동일하게 활용합니다. 

{{Important/ko|''-l''또는 ''-L''을 지정해도 스냅샷을 만듯지만, 결과 스냅샷은 씬 스냅샷이 아닌 일반 스냅샷이 됩니다.}}

스냅샷의 스냅샷을 취할 수도 있습니다:

{{RootCmd|lvcreate -s -n 1_20140413_lvol1 /dev/vg0/20140413_lvol1}}

씬 스냅샷을 기존 스냅샷에 비해 여러가지 장점이 있습니다. 먼저, 씬 스냅샷은 원래 만들어놓은 논리 볼륨에 독립적입니다. 초기 논리 볼륨은 스냅샷에 영향을 주지 않고 크기를 줄이거나 삭제할 수 있습니다. 두번째로, 씬 스냅샷은 일반 재귀적 LVM 스냅샷의 ''체이닝'' 오버헤드 없이 효율적으로 재귀적으로(스냅샷의 스냅샷) 만들 수 있습니다.

==== 스냅샷 상태로 되돌리기 ====

스냅샷의 버전으로 논리 볼륨을 되돌리려면 다음 명령을 사용하십시오:

{{RootCmd|lvconvert --merge /dev/vg0/20140413_lvol1}}

This might take a couple of minutes, depending on the size of the volume. Please note that the rollback will only happen once the parent logical volume is offline. Hence a reboot might be required.

{{Important/ko|스냅샷이 사라지며, 바뀐 내용은 되돌릴 수 없습니다}}

==== 씬 스냅샷으로 되돌리기 ====

For thin volumes, <tt>lvconvert --merge</tt> does not work. Instead, delete the original logical volume and rename the snapshot:

{{RootCmd|umount /dev/vg0/lvol1
|lvremove /dev/vg0/lvol1
|lvrename vg0/20140413_lvol1 lvol1}}

=== 다른 저장소 할당 방법 ===

LVM supports different allocation methods for storage:
* Linear volumes (which is the default);
* Mirrored volumes (in a more-or-less active/standby setup);
* Striping (RAID0);
* Mirrored volumes (RAID1 - which is more an active/active setup);
* Striping with parity (RAID4 and RAID5);
* Striping with double parity (RAID6);
* Striping and mirroring (RAID10).

==== 선형 볼륨 ====

리니어 볼륨은 대부분의 LVM 볼륨 중 한 종류입니다. LVM은 가능한한 물리적으로 연속된 공간으로 논리 볼륨을 할당합니다. 충분히 큰 용량을 지닌 전체 논리 볼륨이 되도록 하는 물리 볼륨이 있다면, LVM이 해당 물리 볼륨을 할당할 것이며, 그렇지 않으면 가능한한 최소한으로 볼륨을 쪼갭니다.

볼륨 그룹과 논리 볼륨을 만드는데 앞에서 언급한 명령으로 선형 볼륨을 만드십시오.

선형 볼륨에는 특별한 요구사항이 없기 때문에 조작하기 쉬우며, 크기를 조절하고 위치를 다시 잡을 수 있습니다. 논리 볼륨을 다중 물리 볼륨에 걸쳐 할당했는데, 어떤 물리 볼륨이 존재하지 않게 되면, 논리 볼륨을 더이상 시작할 수 없고 사용할 수 없게 됩니다.

==== 미러링 볼륨 ====

LVM 에서는 ''미러링''(복제) 볼륨을 지원하며, 드라이브가 망가진 상황에서 내고장성을 보장합니다. RAID1과는 달리 성능 이득은 없습니다. 모든 읽기 쓰기 동작은 단일 미러로 처리합니다.

미러 상태를 유지하려, LVM 에서는 유지 ''로그''가 필요합니다. 그 어떤 미러 논리 볼륨이 들어있지 않은 물리 볼륨의 로그에 이 로그를 넣으시는 것이 좋(으며 심지어는 필수로 권장)습니다. 미러에 활용할 로그에는 세가지 종류가 있습니다:

# '''Disk'''는 기본 로그 형식입니다. 모든 바뀐 내용은 LVM이 관리하는 추가 메타데이터 확장에 기록합니다. 장치가 고장나면, 미러를 복원할 수 있기 전까지는 로그에 바뀐 내용을 유지합니다.
# '''Mirror'''는 자체적으로 미러링한 '''disk''' 로그입니다.
# '''Core''' 미러 로그는 메모리의 미러링 상태를 기록합니다. LVM에서는 미러를 활성화 할 때마다 미러를 다시 구성합니다. 이 형식은 임시 미러에 적합합니다.

To create a logical volume with a single mirror, pass the ''-m 1'' argument (to select standard mirroring) with optionally <code>--mirrorlog</code> to select a particular log type:

{{RootCmd|lvcreate -m 1 --mirrorlog mirror -l 40%VG --nosync -n lvol1 vg0}}

The ''-m 1'' tells LVM to create one (additional) mirror, so requiring 2 physical volumes. The <code>--nosync</code> option is an optimization - without it LVM will try synchronize the mirror by copying empty sectors from one logical volume to another.

현재 논리 볼륨의 미러를 만들 수 있습니다:

{{RootCmd|lvconvert -m 1 -b vg0/lvol1}}

The <code>-b</code> option does the conversion in the background as this can take quite a while.

미러를 제거하려면, 미러 번호를 0으로 (되돌려) 설정하십시오:

{{RootCmd|lvconvert -m0 vg0/lvol1}}

미러 부분을 사용할 수 없다면(보통 물리 볼륨이 들어간 디스크가 망가졌기 때문), 볼륨 그룹을 디그레이드 모드로 전환해야 합니다:

{{RootCmd|vgchange -ay --partial vg0}}

초기 기록시, LVM에서는 미러가 깨졌음을 알립니다. 기본 정책(''remove'')은 존재하는 부분의 수에 따라 미러를 자동으로 줄이거나 해제합니다. 물리 볼륨이 빠진 3-웨이 미러는 2-웨이 미러로 줄어듭니다. 이런식으로 2-웨이 미러는 일반 선형 볼륨으로 줄어듭니다. 고장이 일시적이고, LVM이 미러를 깨먹고 난 후 물리 볼륨이 돌아온다면, 미러링한 논리 볼륨을 다시 만들어야합니다. 

To recover the mirror, the failed physical volume needs to be removed from the volume group, and a replacement physical volume needs to be added (or if the volume group has a free physical volume, it can be created on that one). Then the mirror can be recreated with <tt>lvconvert</tt> at which point the old physical volume can be removed from the volume group:

{{RootCmd|vgextend vg0 /dev/sdc1
|lvconvert -b -m 1 --mirrorlog disk vg0/lvol1
|vgreduce --removemissing vg0}}

이 명령을 수행하면 한쪽에서 고장이 나면 LVM이 다른 물리 볼륨에 자유 확장 공간으로 미러를 다시 만들게 할 수 있습니다. 동작을 수행하려면 {{Path|lvm.conf}}에 <code>mirror_image_fault_policy</code>값을 ''allocate''로 설정하십시오.

==== 씬 미러 ====

It is not (yet) possible to create a mirrored thin pool or thin volume. It is possible to create a mirrored thin pool by creating a normal mirrored logical volume and then converting the logical volume to a thin pool with <tt>lvconvert</tt>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will merge them into a single logical volume.

{{Warning/ko|제대로 동작하게 하려면 LVM 2.02.98 이상이 필요합니다. 이전 버전은 이 기능을 활용할 수 없을 뿐더러 세그 폴트를 일으키고, 볼륨 그룹을 깨먹을 수 있습니다. 또한 미러를 씬 풀로 변환하는 동작으로 미러상의 모든 데이터를 '''파괴'''합니다!}}

{{RootCmd|lvcreate -m 1 --mirrorlog mirrored -l40%VG -n thin_pool vg0
|lvcreate -m 1 --mirrorlog mirrored -L4MB -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg0/thin_meta}}

==== 스트라이핑(RAID0) ====

선형 볼륨과는 달리, 다중 연속 물리 볼륨이 붙는 경우, 보다 나은 성능을 위해 ''스트라이핑'' 또는 ''RAID 0'' 볼륨을 만들 수 있습니다. 여러 물리 볼륨에 대한 저장 장치 할당 방식을 대체합니다.

세 개의 물리 볼륨에 스트라이프 볼륨을 만들려면:

{{RootCmd|lvcreate -i 3 -l 20%VG -n lvol1_stripe vg0|output=<pre>
Using default stripesize 64.00 KiB
</pre>}}

The <code>-i</code> option indicates over how many physical volumes the striping should be done.

It is possible to mirror a stripe set. The <code>-i</code> and <code>-m</code> options can be combined to create a striped mirror:

{{RootCmd|lvcreate -i 2 -m 1 -l 10%VG vg0}}

This creates a 2 physical volume stripe set and mirrors it on 2 different physical volumes, for a total of 4 physical volumes. An existing stripe set can be mirrored with <tt>lvconvert</tt>.

다른 논리 볼륨처럼 씬 풀을 스트라이핑 할 수 있습니다. 모든 씬 볼륨은 설정에 따라 풀에서 만들 수 있습니다. 씬 볼륨을 만들때 직접 지정하지마십시오.

기존 볼륨을 스트라이핑 할 수 없을 뿐더러 여러 물리 볼륨에 대해 스트라이핑을 다시 재구성 할 수도 없거니와 다른 RAID 레벨, 선형 볼륨으로 변환할 수도 없습니다. 스트라이프 집합은 미러링 할 수 있습니다. 추가 물리 볼륨으로 스트라이프 집합을 확장할 수 있지만, 초기의 다중 스트라이프 셋에 추가해야 합니다(새 스트라이프 집합이 선형으로 붙는 결과를 가져옵니다).

==== 미러링(RAID1) ====

스트라이핑을 하는 RAID 0과는 달리, RAID 1은 미러링 방식이지만, 원래 LVM 미러링과는 다른 방식으로 구현했습니다. RAID 1에서는, 성능 향상을 위해 물리 볼륨 전체를 읽어들입니다. RAID1 미러가 고장나면 장치를 차단하기 위해 I/O 가 일어나지 않는데, LVM 에서는 기록 과정을 멈출 필요가 없기 때문입니다.

LVM 미러를 활용할 수 있는 곳에서는, RAID1 미러를 각 위치에서 사용할 수 있습니다.  {{Path|lvm.conf}}에서 ''mirror_segtype_default'' 값을 ''raid1''으로 설정하면 LVM에서 일반 미러대신 RAID1 미러를 확실히 만들 수 있습니다.

논리 볼륨을 단일 미러로 만들려면:

{{RootCmd|lvcreate -m 1 --type raid1 -l 40%VG --nosync -n lvm_raid1 vg0}}

미러를 만드는 방식과는 차이점이 있습니다: ''mirrorlog''를 지정하지 않는데, RAID1 논리 볼륨에선 확실한 미러 로그를 지니고 있지 않습니다. 논리 볼륨 안에 만들기 때문입니다.

기존 논리 볼륨은 RAID 1으로 변환할 수 있습니다:

{{RootCmd|lvconvert -m 1 --type raid1 -b vg0/lvol1}}

RAID 1 미러를 제거하려면, 미러 갯수를 0으로 설정하십시오:

{{RootCmd|lvconvert -m0 vg0/lvm_raid1}}

RAID1 부분을 사용할 수 없다면(보통 물리 볼륨이 들어간 디스크가 망가졌기 때문), 볼륨 그룹을 디그레이드 모드로 전환해야합니다:

{{RootCmd|vgchange -ay --partial vg0}}

Unlike an LVM mirror, writing does NOT break the mirroring. If the failure is only transient, and the missing physical volume returns, LVM will resync the mirror by copying cover the out-of-date segments instead of the entire logical volume. If the failure is permanent, then the failed physical volume needs to be removed from the volume group, and a replacement physical volume needs to be added (or if the volume group has a free physical volume, it can be created on a different PV). The mirror can then be repaired with <tt>lvconvert</tt>, and the old physical volume can be removed from the volume group:

{{RootCmd|vgextend vg0 /dev/sdc1
|lvconvert --repair -b vg0/lvm_raid1
|vgreduce --removemissing vg0}}

==== 씬 RAID1 ====

It is not (yet) possible to create a RAID1 thin pool or thin volume. It is possible to create a RAID1 thin pool by creating a normal mirrored logical volume and then converting the logical volume to a thin pool with <tt>lvconvert</tt>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will then merge them into a single logical volume.

{{Warning/ko|제대로 동작하려면 LVM 2.02.98 이상이 필요합니다. 이전 버전에서는 기능을 사용할 수 없거나, 세그폴트를 일으키기도 하며 볼륨 그룹을 깨먹습니다. RAID1을 씬 풀로 변환하면 미러상의 모든 데이터를 '''파괴합니다'''!}}

{{RootCmd|lvcreate -m 1 --type raid1 -l40%VG -n thin_pool vg0
|lvcreate -m 1 --type raid1 -L4MB -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg00/thin_meta}}

==== 패리티 스트라이핑(RAID4와 RAID5) ====

{{Note/ko|패리티 스트라이핑 방식은 최소한 3개의 물리 볼륨이 필요합니다.}}

RAID 0에는 내고장성 보증이 없습니다. 물리 볼륨중 하나가 고장나면 논리 볼륨을 사용할 수 없습니다 RAID 0에 패리티 스트라이프를 추가하면 물리 볼륨이 빠져도 논리 볼륨은 제 기능을 할 수 있습니다. 그런 다음에야 새 물리 볼륨을 내고장성 복원을 위해 추가할 수 있습니다.

패리티가 붙은 스트라이프 집합에는 RAID 4와 5 2가지 방식이 있습니다. RAID 4에서는 모든 패리티 스트라이프를 동일한 물리 볼륨에 저장합니다. 이런 구조는 병목 현상을 야기할 수 있는데, 모든 물리 볼륨에 대해 쓰기 동작이 발생하며, 어레이에서 더 많은 물리 볼륨이 잘못될 수 있습니다. RAID 5에서는 패리티 데이터를 각 물리 볼륨에 분산하여 그 어디에서도 병목 현상이 일어나지 않습니다. 이런 이유로 RAID 4는 거의 사용하지 않으며 사장된 기술로 간주합니다. 실제로 패리티가 붙은 모든 스트라이프 셋은 RAID 5 입니다.

{{RootCmd|lvcreate --type raid5 -l 20%VG -i 2 -n lvm_raid5 vg0}}

데이터 물리 볼륨은 -i로만 정의할 수 있으며, LVM은 패리티 목적으로 자동으로 추가합니다. 따라서 3개의 물리 볼륨이 있는 RAID5에서, ''-i 2''을 전달해야지 ''-i 3''을 전달하면 안됩니다.

물리 볼륨이 고장나면, 볼륨 그룹을 디그레이드 모드로 전환해야합니다:

{{RootCmd|vgchange -ay --partial vg0}}

이 시점에서 볼륨은 정상적으로 동작합니다만, 교체 물리 볼륨을 추가하기 전에는 이 명령으로 어레이를 RAID0로 강등합니다. 어레이는 강등되지만 성능에는 전혀 영향이 없습니다. 비록 패리티로 빠진 데이터를 재계산 처리할 필요가 있긴 하지만,  남은 데이터에 대한 패리티 블록은 단순한 XOR 연산만을 필요로 합니다. 부하는 디스크 입출력에 비하면 무시해도 될 정도의 수준입니다.

RAID5를 복원하려면:

{{RootCmd|lvconvert --repair vg0/lvm_raid5
|vgreduce --removemissing vg0}}

마찬가지로 RAID5에서 동작하는 물리 볼륨을 바꿀 수 있습니다:

{{RootCmd|lvconvert --replace /dev/sdb1 vg0/lvm_raid5
|vgreduce vg0 /dev/sdb1}}

마찬가지로 스트라이프 집합에 대한 동일한 제한을 패리티가 붙은 스트라이프 집합에 적용합니다: 기존의 볼륨에서 패리티가 있는 스트라이핑을 활성화 할 수 없으며, 여러 물리 볼륨에 대해 더 혹은 덜 스트라이프를 재배치 할 수도 없거니와, 다른 RAID 레벨, 선형 볼륨으로 변환할 수도 없습니다. 패리티가 있는 스트라이프 집합에 대해 미러링을 수행할 수는 있습니다. 패리티가 붙은 스트라이프 셋을 추가 물리 볼륨으로 확장할 수 있지만, 패리티가 붙은 기존 다중 스트라이프 집합에 먼저 추가해야 합니다(새 스트라이프 집합을 패리티와 함께 기존의 스트라이프 집합에 연속적으로 붙인 효과가 있습니다).

==== 씬 RAID5 논리 볼륨 ====

It is not (yet) possible to create stripe set with parity (RAID5) thin pools or thin logical volumes. It is possible to create a RAID5 thin pool by creating a normal RAID5 logical volume and then converting the logical volume into a thin pool with <tt>lvconvert</tt>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will merge them into a single logical volume.

{{Warning/ko|제대로 동작하려면 LVM 2.02.98 이상이 필요합니다. 이전 버전에서는 기능을 사용할 수 없거나, 세그폴트를 일으키기도 하며 볼륨 그룹을 깨먹습니다. RAID5 논리 볼륨을 씬 풀로 변환하면 논리 볼륨의 모든 데이터를 '''파괴합니다'''!}}

{{RootCmd|lvcreate --type raid5 -i 2  -l20%VG -n thin_pool vg0
|lvcreate --type raid5 -i 2 -L4MB -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg00/thin_meta}}

==== 이중 패리티 스트라이핑(RAID6) ====

{{Note/ko|RAID6에서는 최소한 5개의 물리 볼륨이 필요합니다.}}

RAID6는 RAID5와 유사하지만, RAID6에서는 '''두 개'''의 물리 볼륨 고장을 구할 수 있어, 추가 물리 볼륨에 비용이 들어가는 만큼 RAID5보다 더 우수한 내고장성을 보장합니다. 

{{RootCmd|lvcreate --type raid6 -l 20%VG -i 3 -n lvm_raid6 vg00}}

Like RAID5, the <code>-i</code> option is used to specify the number of physical volumes to stripe, excluding the 2 physical volumes for parity. So for a 5 physical volume RAID6, pass on <code>-i 3</code> and not <code>-i 5</code>.

RAID6 복구는 RAID5와 같습니다.

{{Note/ko|디스크 입출력에 비해 패리티 블록을 처리하는 노력이 적게드는 RAID5와는 달리 RAID6에서는 반만 맞습니다. RAID6에서는 2개의 패리티 스트라이프를 사용합니다. 스트라이프 하나는 RAID5와 동일한 방식으로 처리합니다(단순 XOR). 두번째 패리티 스트라이프는 처리하는데 좀 더 오래걸립니다. 더 많은 내용은 [https://www.kernel.org/pub/linux/kernel/people/hpa/raid6.pdf raid6 (pdf)]를 참고하십시오.}}

==== 씬 RAID6 논리 볼륨 ====

It is not (yet) possible to create a RAID6 thin pool or thin volumes. It is possible to create a RAID6 thin pool by creating a normal RAID6 logical volume and then converting the logical volume into a thin pool with <tt>lvconvert</tt>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will merge them into a single logical volume.

{{Warning/ko|제대로 동작하려면 LVM 2.02.98 이상이 필요합니다. 이전 버전에서는 기능을 사용할 수 없거나, 세그폴트를 일으키기도 하며 볼륨 그룹을 깨먹습니다. RAID6 논리 볼륨을 씬 풀로 변환하면 논리 볼륨의 모든 데이터를 '''파괴합니다'''!}}

{{RootCmd|lvcreate --type raid6 -i 2  -l20%VG -n thin_pool vg0
|lvcreate --type raid6 -i 2 -L4MB -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg0/thin_meta}}

==== LVM RAID10 ====

{{Note/ko|RAID10은 최소한 4개의 물리 볼륨이 필요합니다. 게다가 LVM 문법에서는 RAID10 형식이 이렇지 않지만, 여러 물리 볼륨이 여러개의 스트라이프와 미러가 되어야 합니다.}}

RAID10은 RAID0과 RAID1의 조합입니다. 논리 볼륨 레벨 대신 스트라이프 레벨에서 미러링을 처리하기 때문에 RAID0+RAID1보다 더 강력하기 때문에 물리 배치를 대칭형으로 둘 필요가 없습니다. RAID10 볼륨은 단일 물리 볼륨의 누락이 일어나도 상관 없으며, 가능한 경우 그 이상의 문제가 발생해도 문제 없습니다.

{{Note/ko|LVM에서는 현재 RAID10을 단일 미러로 제한합니다.}}

{{RootCmd|lvcreate --type raid10 -l 1020 -i 2 -m 1 --nosync -n lvm_raid10 vg0}}

Both the <code>-i</code> and <code>-m</code> options are specified: <code>-i</code> is the number of stripes and <code>-m</code> is the number of mirrors. Two stripes and 1 mirror requires 4 physical volumes.

==== 씬 RAID 10 ====

It is not (yet) possible to create a RAID10 thin pool or thin volumes. It is possible to create a RAID10 thin pool by creating a normal RAID10 logical volume and then converting the logical volume into a thin pool with <tt>lvconvert</tt>. 2 logical volumes are required: one for the thin pool and one for the thin metadata; the conversion process will merge them into a single logical volume.
 
{{Warning|Conversion of a RAID10 logical volume into a thin pool '''destroys''' all existing data in the logical volume!}}

{{RootCmd|lvcreate -i 2 -m 1 --type raid10 -l 1012 -n thin_pool vg0
|lvcreate -i 2 -m 1 --type raid10 -l 6 -n thin_meta vg0
|lvconvert --thinpool vg0/thin_pool --poolmetadata vg0/thin_meta}}

== LVM으로 실험하기 ==

실제 저장 장치를 활용하지 않고도 LVM으로 실험 활동을 할 수 있습니다. 이를 수행해보려면, 루프백 장치를 만듭니다.

먼저 루프백 모듈을 불러왔는지 확인하십시오. 

{{RootCmd|modprobe -r loop && modprobe loop max_part{{=}}63}}

{{Note/ko|루프백 지원을 커널에 빌드했다면, <code>loop.max_part{{=}}63</code> 부팅 옵션을 활용하십시오.}}

다음 장치를 검색하는데 LVM에서 [[udev]]를 활용하지 않도록 설정하십시오:

{{FileBox|filename=/etc/lvm/lvm.conf|title=LVM 설정에서 udev 비활성화|lang=ini|1=
obtain_device_list_from_udev = 0
}}

{{Important/ko|시험 목적일 뿐이며, 실제 장치를 다루는데 있어 udev를 사용하는 방식이 빠른지 설정을 되돌려서 확인하십시오!}}

저장 장치가 될 이미지 파일을 만드십시오. 다음 예제에서는 실제 하드 디스크 드라이브 공간에서 전부 ~10GB 가량의 용량에 대해 다섯개의 파일을 사용합니다:

{{RootCmd|mkdir /var/lib/lvm_img
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm0.img bs{{=}}1024 seek{{=}}2097152
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm1.img bs{{=}}1024 seek{{=}}2097152
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm2.img bs{{=}}1024 seek{{=}}2097152
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm3.img bs{{=}}1024 seek{{=}}2097152
|dd if{{=}}/dev/null of{{=}}/var/lib/lvm_img/lvm4.img bs{{=}}1024 seek{{=}}2097152}}

어떤 루프백 장치를 활용할 수 있는지 확인하십시오:

{{RootCmd|losetup -a}}

모든 루프백 장치가 있다면, 장치를 만드십시오:

{{RootCmd|losetup /dev/loop0 /var/lib/lvm_img/lvm0.img
|losetup /dev/loop1 /var/lib/lvm_img/lvm1.img
|losetup /dev/loop2 /var/lib/lvm_img/lvm2.img
|losetup /dev/loop3 /var/lib/lvm_img/lvm3.img
|losetup /dev/loop4 /var/lib/lvm_img/lvm4.img}}

이제 시스템의 다른 하드 디스크 드라이브처럼 {{Path|/dev/loop[0-4]}} 장치를 사용할 수 있습니다(그래서 완벽한 물리 볼륨이 됩니다).

{{Note/ko|다음에 재부팅을 수행할 때 모든 루프백 장치를 활성화해제 할 것이며 {{Path|/var/lib/lvm_img}} 폴더를 삭제할 수 있습니다.}}

== 문제해결 ==

LVM에는 몇가지 레벨의 중복을 지원하는 일부 기능을 지니고 있습니다. 다만, 잃어버린 물리 볼륨 또는 논리 볼륨을 복원할 수 있는 곳에 몇가지 상황이 있긴 합니다.

=== vgcfgrestore 유틸리티 ===

기본적으로 LVM 물리 볼륨, 볼륨 그룹, 논리 볼륨 그 어느 곳에서든 내용이 바뀔 경우, LVM2에서 {{Path|/etc/lvm/archive}}에 메타데이터 백업 파일을 만듭니다. 이 파일은 갑작스럽게 바뀐 상태(잘못된 논리 볼륨 삭제 같은 경우)로부터 복원하는데 활용할 수 있습니다. LVM은 또한 {{Path|/etc/lvm/backup}}에 가장 최근의 메타데이터 백업 사본을 유지합니다. 이런 요소는 교체 디스크에 메타데이터를 복원하는데 활용할 수 있을 뿐만 아니라, 깨진 메타데이터를 복구하는데도 활용할 수 있습니다.

복원하려는 존재 볼륨 그룹의 상태를 보려면(가독성을 개선하기 위해 일부만 출력):

{{RootCmd|vgcfgrestore --list vg00|output=<pre>
  File:		/etc/lvm/archive/vg0_00042-302371184.vg
  VG name:    	vg0
  Description:	Created *before* executing 'lvremove vg0/lvm_raid1'
  Backup Time:	Sat Jul 13 01:41:32 201
</pre>}}

==== 갑작스럽게 삭제된 논리 볼륨 복원 ====

볼륨 그룹 ''vg0''에서 논리 볼륨 ''lvm_raid1''이 갑자기 제거된 상황을 가정하겠습니다. 다음과 같이 복구할 수 있습니다:

{{RootCmd|vgcfgrestore -f /etc/lvm/archive/vg0_00042-302371184.vg vg0}}

{{Important|vgcfgrestore only restores LVM metadata, ''not'' the data inside the logical volume. However pvremove, vgremove, and lvremove only wipe metadata, leaving any data intact. If <code>issue_discards</code> is set in {{Path|/etc/lvm/lvm.conf}} though, then these command ''are'' destructive to data.}}

==== 고장난 물리 볼륨 바꾸기 ====

이 방법을 통해 제대로 ''교체''하고, 이전 물리 볼륨에서와 완전히 똑같은 메타데이터를 새 물리 볼륨에 다시 만들 수 있습니다.

{{RootCmd|vgdisplay --partial --verbose|output=<pre>
  --- Physical volumes ---
  PV Name               /dev/loop0     
  PV UUID               iLdp2U-GX3X-W2PY-aSlX-AVE9-7zVC-Cjr5VU
  PV Status             allocatable
  Total PE / Free PE    511 / 102
  
  PV Name               unknown device     
  PV UUID               T7bUjc-PYoO-bMqI-53vh-uxOV-xHYv-0VejBY
  PV Status             allocatable
  Total PE / Free PE    511 / 102
</pre>}}

여기서 중요한 줄은 UUID ''unknown device'' 입니다. 

{{RootCmd|pvcreate --uuid T7bUjc-PYoO-bMqI-53vh-uxOV-xHYv-0VejBY --restorefile /etc/lvm/backup/vg0 /dev/loop1|output=<pre>
  Couldn't find device with uuid T7bUjc-PYoO-bMqI-53vh-uxOV-xHYv-0VejBY.
  Physical volume "/dev/loop1" successfully created</pre>}}

이 명령으로 물리 볼륨 메타데이터를 다시 만들지만, 물리 볼륨의 빠진 논리 볼륨이나 볼륨 그룹 데이터를 다시 만들진 않습니다.

{{RootCmd|vgcfgrestore -f /etc/lvm/backup/vg0 vg0|output=<pre>
  Restored volume group vg0
</pre>}}

이 명령으로 이제 물리 볼륨, 논리 볼륨, 볼륨 그룹 데이터의 빠진 메타데이터를 다시 만듭니다. 그러나 데이터를 복원하지는 않기 때문에 미러의 동기화는 오래된 상태가 됩니다.

{{RootCmd|vgchange -ay vg0|output=<pre>
  device-mapper: reload ioctl on  failed: Invalid argument
  1 logical volume(s) in volume group "vg0" now active
</pre>}}

{{RootCmd|lvchange --resync vg0/lvm_raid1|output=<pre>
Do you really want to deactivate logical volume lvm_raid1 to resync it? [y/n]: y
</pre>}}

이 명령은 미러를 다시 동기화합니다. RAID 4, 5, 6에서도 동작합니다.

=== 논리 볼륨 비활성화 ===

다음 명령으로 논리 볼륨을 비활성화 할 수 있습니다:

{{RootCmd|umount /dev/vg0/lvol1
|lvchange -a n /dev/vg0/lvol1}}

다시 활성화하기 전에는 논리 볼륨을 어디에서든 다시 마운트 할 수 없습니다:

{{RootCmd|lvchange -a y /dev/vg0/lvol1}}

== 외부 자료 ==

* [http://sourceware.org/lvm2/ LVM2 sourceware.org]
* [http://tldp.org/HOWTO/LVM-HOWTO/ LVM tldp.org]
* [http://sources.redhat.com/lvm2/wiki/ LVM2 Wiki redhat.com]


[[Category:Core system]]
